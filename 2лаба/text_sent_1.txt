springer series in statistics trevor hastie robert tibshirani jerome friedman the elements of statistical learning data mining inference and prediction second edition
this is page printer opaque this to our parents valerie and patrick hastie vera and sami tibshirani florence and harry friedman and to our families samantha timothy and lynda charlie ryan julie and cheryl melanie dora monika and ildiko
this is page vii printer opaque this preface to the second edition in god we trust all others bring data william edwards deming we have been gratified by the popularity of the first edition of the elements of statistical learning
this along with the fast pace of research in the statistical learning field motivated us to update our book with second edition
we have added four new chapters and updated some of the existing chapters
because many readers are familiar with the layout of the first edition we have tried to change it as little as possible
here is summary of the main changes on the web this quote has been widely attributed to both deming and robert
hayden however professor hayden told us that he can claim no credit for this quote and ironically we could find no data confirming that deming actually said this
viii preface to the second edition chapter what's new
overview of supervised learning
linear methods for regression lar algorithm and generalizations of the lasso
linear methods for classification lasso path for logistic regression
basis expansions and regulariza additional illustrations of rkhs tion
kernel smoothing methods
model assessment and selection strengths and pitfalls of crossvalidation
model inference and averaging
additive models trees and related methods
boosting and additive trees new example from ecology some material split off to chapter
neural networks bayesian neural nets and the nips challenge
support vector machines and path algorithm for svm classifier flexible discriminants
prototype methods and nearest neighbors
unsupervised learning spectral clustering kernel pca sparse pca non negative matrix factorization archetypal analysis nonlinear dimension reduction google page rank algorithm direct approach to ica
random forests new
ensemble learning new
undirected graphical models new
we have changed the color palette in this edition to large extent replacing the above with an orange blue contrast
we have fixed this in the new edition
due to lack of space we have specifically omitted coverage of directed graphical models
these problems arise in many areas including genomic and proteomic studies and document classification
we thank the many readers who have found the too numerous errors in the first edition
we apologize for those and have done our best to avoid errors in this new edition
we thank mark segal bala rajaratnam and larry wasserman for comments on some of the new chapters and many stanford graduate and post doctoral students who offered comments in particular mohammed alquraishi john boik holger hoefling arian maleki donal mcmahon saharon rosset babak shababa daniela witten ji zhu and hui zou
we thank john kimmel for his patience in guiding us through this new edition
rt dedicates this edition to the memory of anna mcphee
trevor hastie robert tibshirani jerome friedman stanford california august
preface to the second edition
this is page xi printer opaque this preface to the first edition we are drowning in information and starving for knowledge rutherford
roger the field of statistics is constantly challenged by the problems that science and industry brings to its door
in the early days these problems often came from agricultural and industrial experiments and were relatively small in scope
with the advent of computers and the information age statistical problems have exploded both in size and complexity
challenges in the areas of data storage organization and searching have led to the new field of data mining statistical and computational problems in biology and medicine have created bioinformatics
vast amounts of data are being generated in many fields and the statistician's job is to make sense of it all to extract important patterns and trends and understand what the data says
we call this learning from data
the challenges in learning from data have led to revolution in the statistical sciences
since computation plays such key role it is not surprising that much of this new development has been done by researchers in other fields such as computer science and engineering
the learning problems that we consider can be roughly categorized as either supervised or unsupervised
in supervised learning the goal is to predict the value of an outcome measure based on number of input measures in unsupervised learning there is no outcome measure and the goal is to describe the associations and patterns among set of input measures
xii preface to the first edition this book is our attempt to bring together many of the important new ideas in learning and explain them in statistical framework
while some mathematical details are needed we emphasize the methods and their conceptual underpinnings rather than their theoretical properties
as result we hope that this book will appeal not just to statisticians but also to researchers and practitioners in wide variety of fields
just as we have learned great deal from researchers outside of the field of statistics our statistical viewpoint may help others to better understand different aspects of learning there is no true interpretation of anything interpretation is vehicle in the service of human comprehension
the value of interpretation is in enabling others to fruitfully think about an idea andreas buja we would like to acknowledge the contribution of many people to the conception and completion of this book
david andrews leo breiman andreas buja john chambers bradley efron geoffrey hinton werner stuetzle and john tukey have greatly influenced our careers
balasubramanian narasimhan gave us advice and help on many computational problems and maintained an excellent computing environment
shin ho bang helped in the production of number of the figures
lee wilkinson gave valuable tips on color production
ilana belitskaya eva cantoni maya gupta michael jordan shanti gopatam radford neal jorge picazo bogdan popescu olivier renaud saharon rosset john storey ji zhu mu zhu two reviewers and many students read parts of the manuscript and offered helpful suggestions
john kimmel was supportive patient and helpful at every phase maryann brickner and frank ganz headed superb production team at springer
trevor hastie would like to thank the statistics department at the university of cape town for their hospitality during the final stages of this book
we gratefully acknowledge nsf and nih for their support of this work
finally we would like to thank our families and our parents for their love and support
this is page xiii printer opaque this contents preface to the second edition vii preface to the first edition xi introduction overview of supervised learning introduction
variable types and terminology
two simple approaches to prediction least squares and nearest neighbors
linear models and least squares
nearest neighbor methods
from least squares to nearest neighbors
statistical decision theory
local methods in high dimensions
statistical models supervised learning and function approximation
statistical model for the joint distribution pr
supervised learning
function approximation
structured regression models
difficulty of the problem
xiv contents classes of restricted estimators
roughness penalty and bayesian methods
kernel methods and local regression
basis functions and dictionary methods
model selection and the bias variance tradeoff
bibliographic notes
linear methods for regression introduction
linear regression models and least squares
example prostate cancer
the gauss markov theorem
multiple regression from simple univariate regression
multiple outputs
subset selection
best subset selection
forwardand backward stepwise selection
forward stagewise regression
prostate cancer data example continued
shrinkage methods
ridge regression
the lasso
discussion subset selection ridge regression and the lasso
least angle regression
methods using derived input directions
principal components regression
partial least squares
discussion comparison of the selection and shrinkage methods
multiple outcome shrinkage and selection
more on the lasso and related path algorithms
incremental forward stagewise regression
piecewise linear path algorithms
the dantzig selector
the grouped lasso
further properties of the lasso
pathwise coordinate optimization
computational considerations
bibliographic notes
contents xv linear methods for classification introduction
linear regression of an indicator matrix
linear discriminant analysis
regularized discriminant analysis
computations for lda
reduced rank linear discriminant analysis
logistic regression
fitting logistic regression models
example south african heart disease
quadratic approximations and inference
regularized logistic regression
logistic regression or lda
separating hyperplanes
rosenblatt's perceptron learning algorithm
optimal separating hyperplanes
bibliographic notes
basis expansions and regularization introduction
piecewise polynomials and splines
natural cubic splines
example south african heart disease continued example phoneme recognition
filtering and feature extraction
smoothing splines
degrees of freedom and smoother matrices
automatic selection of the smoothing parameters
fixing the degrees of freedom
the bias variance tradeoff
nonparametric logistic regression
multidimensional splines
regularization and reproducing kernel hilbert spaces
spaces of functions generated by kernels
examples of rkhs
wavelet smoothing
wavelet bases and the wavelet transform
adaptive wavelet filtering
bibliographic notes
appendix computational considerations for splines
appendix splines
appendix computations for smoothing splines
xvi contents kernel smoothing methods one dimensional kernel smoothers
local linear regression
local polynomial regression
selecting the width of the kernel
local regression in irp
structured local regression models in irp
structured kernels
structured regression functions
local likelihood and other models
kernel density estimation and classification
kernel density estimation
kernel density classification
the naive bayes classifier
radial basis functions and kernels
mixture models for density estimation and classification computational considerations
bibliographic notes
model assessment and selection introduction
bias variance and model complexity
the bias variance decomposition
example bias variance tradeoff
optimism of the training error rate
estimates of in sample prediction error
the effective number of parameters
the bayesian approach and bic
minimum description length
vapnik chervonenkis dimension
example continued
cross validation
fold cross validation
the wrong and right way to do cross validation
does cross validation really work
bootstrap methods
example continued
conditional or expected test error
bibliographic notes
model inference and averaging introduction
contents xvii the bootstrap and maximum likelihood methods
smoothing example
maximum likelihood inference
bootstrap versus maximum likelihood
bayesian methods
relationship between the bootstrap and bayesian inference
the em algorithm
two component mixture model
the em algorithm in general
em as maximization maximization procedure mcmc for sampling from the posterior
example trees with simulated data
model averaging and stacking
stochastic search bumping
bibliographic notes
additive models trees and related methods generalized additive models
fitting additive models
example additive logistic regression
tree based methods
regression trees
classification trees
other issues
spam example continued
prim bump hunting
spam example continued
mars multivariate adaptive regression splines
spam example continued
example simulated data
other issues
hierarchical mixtures of experts
missing data
computational considerations
bibliographic notes
boosting and additive trees boosting methods
outline of this chapter
xviii contents boosting fits an additive model
forward stagewise additive modeling
exponential loss and adaboost
why exponential loss
loss functions and robustness
off the shelf procedures for data mining
example spam data
boosting trees
numerical optimization via gradient boosting
steepest descent
gradient boosting
implementations of gradient boosting
right sized trees for boosting
relative importance of predictor variables
partial dependence plots
california housing
new zealand fish
demographics data
bibliographic notes
neural networks introduction
projection pursuit regression
neural networks
fitting neural networks
some issues in training neural networks
starting values
scaling of the inputs
number of hidden units and layers
multiple minima
example simulated data
example zip code data
bayesian neural nets and the nips challenge
bayes boosting and bagging
performance comparisons
computational considerations
bibliographic notes
contents xix exercises
support vector machines and flexible discriminants introduction
the support vector classifier
computing the support vector classifier
mixture example continued
support vector machines and kernels
computing the svm for classification
the svm as penalization method
function estimation and reproducing kernels
svms and the curse of dimensionality
path algorithm for the svm classifier
support vector machines for regression
regression and kernels
generalizing linear discriminant analysis
flexible discriminant analysis
computing the fda estimates
penalized discriminant analysis
mixture discriminant analysis
example waveform data
bibliographic notes
prototype methods and nearest neighbors introduction
prototype methods
means clustering
learning vector quantization
gaussian mixtures
nearest neighbor classifiers
example comparative study
example nearest neighbors and image scene classification
invariant metrics and tangent distance
adaptive nearest neighbor methods
global dimension reduction for nearest neighbors
computational considerations
bibliographic notes
xx contents unsupervised learning introduction
association rules
market basket analysis
the apriori algorithm
example market basket analysis
unsupervised as supervised learning
generalized association rules
choice of supervised learning method
example market basket analysis continued
cluster analysis
proximity matrices
dissimilarities based on attributes
object dissimilarity
clustering algorithms
combinatorial algorithms
means
gaussian mixtures as soft means clustering
example human tumor microarray data
vector quantization
medoids
practical issues
hierarchical clustering
self organizing maps
principal components curves and surfaces
principal components
principal curves and surfaces
spectral clustering
kernel principal components
sparse principal components
non negative matrix factorization
archetypal analysis
independent component analysis and exploratory projection pursuit
latent variables and factor analysis
independent component analysis
exploratory projection pursuit
direct approach to ica
multidimensional scaling
nonlinear dimension reduction and local multidimensional scaling
the google pagerank algorithm
bibliographic notes
contents xxi random forests introduction
definition of random forests
details of random forests
out of bag samples
variable importance
proximity plots
random forests and overfitting
analysis of random forests
variance and the de correlation effect
adaptive nearest neighbors
bibliographic notes
ensemble learning introduction
boosting and regularization paths
penalized regression
the bet on sparsity principle
regularization paths over fitting and margins
learning ensembles
learning good ensemble
rule ensembles
bibliographic notes
undirected graphical models introduction
markov graphs and their properties
undirected graphical models for continuous variables
estimation of the parameters when the graph structure is known
estimation of the graph structure
undirected graphical models for discrete variables
estimation of the parameters when the graph structure is known
hidden nodes
estimation of the graph structure
restricted boltzmann machines
high dimensional problems when is much bigger than
xxii contents diagonal linear discriminant analysis and nearest shrunken centroids
linear classifiers with quadratic regularization
regularized discriminant analysis
logistic regression with quadratic regularization
the support vector classifier
feature selection
computational shortcuts when
linear classifiers with regularization
application of lasso to protein mass spectroscopy
the fused lasso for functional data
classification when features are unavailable
example string kernels and protein classification
classification and other models using inner product kernels and pairwise distances
example abstracts classification
high dimensional regression supervised principal components
connection to latent variable modeling
relationship with partial least squares
pre conditioning for feature selection
feature assessment and the multiple testing problem
the false discovery rate
asymmetric cutpoints and the sam procedure bayesian interpretation of the fdr
bibliographic notes
references author index index
this is page printer opaque this introduction statistical learning plays key role in many areas of science finance and industry
the prediction is to be based on demographic diet and clinical measurements for that patient
the science of learning plays key role in the fields of statistics data mining and artificial intelligence intersecting with areas of engineering and other disciplines
this book is about learning from data
in typical scenario we have an outcome measurement usually quantitative such as stock price or categorical such as heart attack no heart attack that we wish to predict based on set of features such as diet and clinical measurements
we have training set of data in which we observe the outcome and feature
introduction table
average percentage of words or characters in an email message equal to the indicated word or character
we have chosen the words and characters showing the largest difference between spam and email george you your hp free hpl
our re edu remove spam email measurements for set of objects such as people
using this data we build prediction model or learner which will enable us to predict the outcome for new unseen objects
good learner is one that accurately predicts such an outcome
the examples above describe what is called the supervised learning problem
it is called supervised because of the presence of the outcome variable to guide the learning process
in the unsupervised learning problem we observe only the features and have no measurements of the outcome
our task is rather to describe how the data are organized or clustered
we devote most of this book to supervised learning the unsupervised problem is less developed in the literature and is the focus of chapter
here are some examples of real learning problems that are discussed in this book
example email spam the data for this example consists of information from email messages in study to try to predict whether the email was junk email or spam
the objective was to design an automatic spam detector that could filter out spam before clogging the users mailboxes
for all email messages the true outcome email type email or spam is available along with the relative frequencies of of the most commonly occurring words and punctuation marks in the email message
this is supervised learning problem with the outcome the class variable email spam
it is also called classification problem
table lists the words and characters showing the largest average difference between spam and email
our learning method has to decide which features to use and how for example we might use rule such as if george you then spam else email
another form of rule might be if you george then spam else email
scatterplot matrix of the prostate cancer data
the first row shows the response against each of the predictors in turn
two of the predictors svi and gleason are categorical
for this problem not all errors are equal we want to avoid filtering out good email while letting spam get through is not desirable but less serious in its consequences
we discuss number of different methods for tackling this learning problem in the book
example prostate cancer the data for this example displayed in figure come from study by stamey et al that examined the correlation between the level of there was an error in these data in the first edition of this book
subject had value of for lweight which translates to gm prostate
the correct value is gm
we are grateful to prof
stephen
link for alerting us to this error
examples of handwritten digits from postal envelopes prostate specific antigen psa and number of clinical measures in men who were about to receive radical prostatectomy
the goal is to predict the log of psa lpsa from number of measurements including log cancer volume lcavol log prostate weight lweight age log of benign prostatic hyperplasia amount lbph seminal vesicle invasion svi log of capsular penetration lcp gleason score gleason and percent of gleason scores or pgg
figure is scatterplot matrix of the variables
some correlations with lpsa are evident but good predictive model is difficult to construct by eye
this is supervised learning problem known as regression problem because the outcome measurement is quantitative
example handwritten digit recognition the data from this example come from the handwritten zip codes on envelopes from postal mail
each image is segment from five digit zip code isolating single digit
the images are eight bit grayscale maps with each pixel ranging in intensity from to
some sample images are shown in figure
the images have been normalized to have approximately the same size and orientation
the task is to predict from the matrix of pixel intensities the identity of each image quickly and accurately
if it is accurate enough the resulting algorithm would be used as part of an automatic sorting procedure for envelopes
this is classification problem for which the error rate needs to be kept very low to avoid misdirection of
introduction mail
in order to achieve this low error rate some objects can be assigned to don't know category and sorted instead by hand
example dna expression microarrays dna stands for deoxyribonucleic acid and is the basic material that makes up human chromosomes
dna microarrays measure the expression of gene in cell by measuring the amount of mrna messenger ribonucleic acid present for that gene
microarrays are considered breakthrough technology in biology facilitating the quantitative study of thousands of genes simultaneously from single sample of cells
here is how dna microarray works
the nucleotide sequences for few thousand genes are printed on glass slide
target sample and reference sample are labeled with red and green dyes and each are hybridized with the dna on the slide
through fluoroscopy the log red green intensities of rna hybridizing at each site is measured
the result is few thousand numbers typically ranging from say to measuring the expression level of each gene in the target relative to the reference sample
positive values indicate higher expression in the target versus the reference and vice versa for negative values
gene expression dataset collects together the expression values from series of dna microarray experiments with each column representing an experiment
there are therefore several thousand rows representing individual genes and tens of columns representing samples in the particular example of figure there are genes rows and samples columns although for clarity only random sample of rows are shown
the figure displays the data set as heat map ranging from green negative to red positive
the samples are cancer tumors from different patients
the challenge here is to understand how the genes and samples are organized
typical questions include the following which samples are most similar to each other in terms of their expression profiles across genes
which genes are most similar to each other in terms of their expression profiles across samples
do certain genes show very high or low expression for certain cancer samples
we could view this task as regression problem with two categorical predictor variables genes and samples with the response variable being the level of expression
however it is probably more useful to view it as unsupervised learning problem
for example for question above we think of the samples as points in dimensional space which we want to cluster together in some way
dna microarray data expression matrix of genes rows and samples columns for the human tumor data
only random sample of rows are shown
the display is heat map ranging from bright green negative under expressed to bright red positive over expressed
missing values are gray
the rows and columns are displayed in randomly chosen order
introduction who should read this book this book is designed for researchers and students in broad variety of fields statistics artificial intelligence engineering finance and others
we expect that the reader will have had at least one elementary course in statistics covering basic topics including linear regression
we have not attempted to write comprehensive catalog of learning methods but rather to describe some of the most important techniques
equally notable we describe the underlying concepts and considerations by which researcher can judge learning method
we have tried to write this book in an intuitive fashion emphasizing concepts rather than mathematical details
as statisticians our exposition will naturally reflect our backgrounds and areas of expertise
however in the past eight years we have been attending conferences in neural networks data mining and machine learning and our thinking has been heavily influenced by these exciting fields
this influence is evident in our current research and in this book
how this book is organized our view is that one must understand simple methods before trying to grasp more complex ones
hence after giving an overview of the supervising learning problem in chapter we discuss linear methods for regression and classification in chapters and
in chapter we describe splines wavelets and regularization penalization methods for single predictor while chapter covers kernel methods and local regression
both of these sets of methods are important building blocks for high dimensional learning techniques
model assessment and selection is the topic of chapter covering the concepts of bias and variance overfitting and methods such as cross validation for choosing models
chapter discusses model inference and averaging including an overview of maximum likelihood bayesian inference and the bootstrap the em algorithm gibbs sampling and bagging related procedure called boosting is the focus of chapter
in chapters we describe series of structured methods for supervised learning with chapters and covering regression and chapters and focusing on classification
chapter describes methods for unsupervised learning
two recently proposed techniques random forests and ensemble learning are discussed in chapters and
we describe undirected graphical models in chapter and finally we study highdimensional problems in chapter
at the end of each chapter we discuss computational considerations important for data mining applications including how the computations scale with the number of observations and predictors
each chapter ends with bibliographic notes giving background references for the material
introduction we recommend that chapters be first read in sequence
chapter should also be considered mandatory as it covers central concepts that pertain to all learning methods
with this in mind the rest of the book can be read sequentially or sampled depending on the reader's interest
the symbol indicates technically difficult section one that can be skipped without interrupting the flow of the discussion
book website the website for this book is located at http www stat stanford edu elemstatlearn it contains number of resources including many of the datasets used in this book
note for instructors we have successively used the first edition of this book as the basis for two quarter course and with the additional materials in this second edition it could even be used for three quarter sequence
exercises are provided at the end of each chapter
it is important for students to have access to good software tools for these topics
we used the and plus programming languages in our courses
this is page printer opaque this overview of supervised learning introduction the first three examples described in chapter have several components in common
for each there is set of variables that might be denoted as inputs which are measured or preset
these have some influence on one or more outputs
for each example the goal is to use the inputs to predict the values of the outputs
this exercise is called supervised learning
we have used the more modern language of machine learning
in the statistical literature the inputs are often called the predictors term we will use interchangeably with inputs and more classically the independent variables
in the pattern recognition literature the term features is preferred which we use as well
the outputs are called the responses or classically the dependent variables
variable types and terminology the outputs vary in nature among the examples
in the glucose prediction example the output is quantitative measurement where some measurements are bigger than others and measurements close in value are close in nature
in the famous iris discrimination example due to
fisher the output is qualitative species of iris and assumes values in finite set virginica setosa and versicolor
in the handwritten digit example the output is one of different digit classes
in both of
overview of supervised learning these there is no explicit ordering in the classes and in fact often descriptive labels rather than numbers are used to denote the classes
qualitative variables are also referred to as categorical or discrete variables as well as factors
for both types of outputs it makes sense to think of using the inputs to predict the output
given some specific atmospheric measurements today and yesterday we want to predict the ozone level tomorrow
given the grayscale values for the pixels of the digitized image of the handwritten digit we want to predict its class label
this distinction in output type has led to naming convention for the prediction tasks regression when we predict quantitative outputs and classification when we predict qualitative outputs
we will see that these two tasks have lot in common and in particular both can be viewed as task in function approximation
inputs also vary in measurement type we can have some of each of qualitative and quantitative input variables
these have also led to distinctions in the types of methods that are used for prediction some methods are defined most naturally for quantitative inputs some most naturally for qualitative and some for both
third variable type is ordered categorical such as small medium and large where there is an ordering between the values but no metric notion is appropriate the difference between medium and small need not be the same as that between large and medium
these are discussed further in chapter
qualitative variables are typically represented numerically by codes
the easiest case is when there are only two classes or categories such as success or failure survived or died
these are often represented by single binary digit or bit as or or else by and
for reasons that will become apparent such numeric codes are sometimes referred to as targets
when there are more than two categories several alternatives are available
the most useful and commonly used coding is via dummy variables
here level qualitative variable is represented by vector of binary variables or bits only one of which is on at time
although more compact coding schemes are possible dummy variables are symmetric in the levels of the factor
we will typically denote an input variable by the symbol
if is vector its components can be accessed by subscripts xj
quantitative outputs will be denoted by and qualitative outputs by for group
we use uppercase letters such as or when referring to the generic aspects of variable
observed values are written in lowercase hence the ith observed value of is written as xi where xi is again scalar or vector
matrices are represented by bold uppercase letters for example set of input vectors xi would be represented by the matrix
in general vectors will not be bold except when they have components this convention distinguishes vector of inputs xi for the
least squares and nearest neighbors ith observation from the vector xj consisting of all the observations on variable xj
since all vectors are assumed to be column vectors the ith row of is xti the vector transpose of xi
for the moment we can loosely state the learning task as follows given the value of an input vector make good prediction of the output denoted by pronounced hat
if takes values in ir then so should likewise for categorical outputs should take values in the same set associated with
for two class one approach is to denote the binary coded target as and then treat it as quantitative output
the predictions will typically lie in and we can assign to the class label according to whether
this approach generalizes to level qualitative outputs as well
we need data to construct prediction rules often lot of it
we thus suppose we have available set of measurements xi yi or xi gi known as the training data with which to construct our prediction rule
two simple approaches to prediction least squares and nearest neighbors in this section we develop two simple but powerful prediction methods the linear model fit by least squares and the nearest neighbor prediction rule
the linear model makes huge assumptions about structure and yields stable but possibly inaccurate predictions
the method of nearest neighbors makes very mild structural assumptions its predictions are often accurate but can be unstable
linear models and least squares the linear model has been mainstay of statistics for the past years and remains one of our most important tools
given vector of inputs xp we predict the output via the model xj
the term is the intercept also known as the bias in machine learning
often it is convenient to include the constant variable in include in the vector of coefficients and then write the linear model in vector form as an inner product
overview of supervised learning where denotes vector or matrix transpose being column vector
here we are modeling single output so is scalar in general can be vector in which case would be matrix of coefficients
in the dimensional input output space represents hyperplane
if the constant is included in then the hyperplane includes the origin and is subspace if not it is an affine set cutting the axis at the point
from now on we assume that the intercept is included in
viewed as function over the dimensional input space is linear and the gradient is vector in input space that points in the steepest uphill direction
how do we fit the linear model to set of training data
there are many different methods but by far the most popular is the method of least squares
in this approach we pick the coefficients to minimize the residual sum of squares rss yi xti
rss is quadratic function of the parameters and hence its minimum always exists but may not be unique
the solution is easiest to characterize in matrix notation
we can write rss where is an matrix with each row an input vector and is an vector of the outputs in the training set
differentiating
we get the normal equations xt
if xt is nonsingular then the unique solution is given by xt xt and the fitted value at the ith input xi is xi xti
at an arbitrary input the prediction is xt
the entire fitted surface is characterized by the parameters
intuitively it seems that we do not need very large data set to fit such model
let's look at an example of the linear model in classification context
figure shows scatterplot of training data on pair of inputs and
the data are simulated and for the present the simulation model is not important
the output class variable has the values blue or orange and is represented as such in the scatterplot
there are points in each of the two classes
the linear regression model was fit to these data with the response coded as for blue and for orange
the fitted values are converted to fitted class variable according to the rule orange if blue if
classification example in two dimensions
the classes are coded as binary variable blue orange and then fit by linear regression
the line is the decision boundary defined by xt
the orange shaded region denotes that part of input space classified as orange while the blue region is classified as blue
the set of points in ir classified as orange corresponds to xt indicated in figure and the two predicted classes are separated by the decision boundary xt which is linear in this case
we see that for these data there are several misclassifications on both sides of the decision boundary
perhaps our linear model is too rigidor are such errors unavoidable
remember that these are errors on the training data itself and we have not said where the constructed data came from
consider the two possible scenarios scenario the training data in each class were generated from bivariate gaussian distributions with uncorrelated components and different means
scenario the training data in each class came from mixture of lowvariance gaussian distributions with individual means themselves distributed as gaussian
mixture of gaussians is best described in terms of the generative model
one first generates discrete variable that determines which of
overview of supervised learning the component gaussians to use and then generates an observation from the chosen density
in the case of one gaussian per class we will see in chapter that linear decision boundary is the best one can do and that our estimate is almost optimal
the region of overlap is inevitable and future data to be predicted will be plagued by this overlap as well
in the case of mixtures of tightly clustered gaussians the story is different
linear decision boundary is unlikely to be optimal and in fact is not
the optimal decision boundary is nonlinear and disjoint and as such will be much more difficult to obtain
we now look at another classification and regression procedure that is in some sense at the opposite end of the spectrum to the linear model and far better suited to the second scenario
nearest neighbor methods nearest neighbor methods use those observations in the training set closest in input space to to form
specifically the nearest neighbor fit for is defined as follows yi xi nk where nk is the neighborhood of defined by the closest points xi in the training sample
closeness implies metric which for the moment we assume is euclidean distance
so in words we find the observations with xi closest to in input space and average their responses
in figure we use the same training data as in figure and use nearest neighbor averaging of the binary coded response as the method of fitting
thus is the proportion of orange's in the neighborhood and so assigning class orange to if amounts to majority vote in the neighborhood
the colored regions indicate all those points in input space classified as blue or orange by such rule in this case found by evaluating the procedure on fine grid in input space
we see that the decision boundaries that separate the blue from the orange regions are far more irregular and respond to local clusters where one class dominates
figure shows the results for nearest neighbor classification is assigned the value of the closest point to in the training data
in this case the regions of classification can be computed relatively easily and correspond to voronoi tessellation of the training data
each point xi has an associated tile bounding the region for which it is the closest input point
for all points in the tile gi
the decision boundary is even more irregular than before
the method of nearest neighbor averaging is defined in exactly the same way for regression of quantitative output although would be an unlikely choice
the same classification example in two dimensions as in figure
the classes are coded as binary variable blue orange and then fit by nearest neighbor averaging as in
the predicted class is hence chosen by majority vote amongst the nearest neighbors
in figure we see that far fewer training observations are misclassified than in figure
this should not give us too much comfort though since in figure none of the training data are misclassified
little thought suggests that for nearest neighbor fits the error on the training data should be approximately an increasing function of and will always be for
an independent test set would give us more satisfactory means for comparing the different methods
it appears that nearest neighbor fits have single parameter the number of neighbors compared to the parameters in least squares fits
although this is the case we will see that the effective number of parameters of nearest neighbors is and is generally bigger than and decreases with increasing
to get an idea of why note that if the neighborhoods were nonoverlapping there would be neighborhoods and we would fit one parameter mean in each neighborhood
it is also clear that we cannot use sum of squared errors on the training set as criterion for picking since we would always pick
it would seem that nearest neighbor methods would be more appropriate for the mixture scenario described above while for gaussian data the decision boundaries of nearest neighbors would be unnecessarily noisy
the same classification example in two dimensions as in figure
the classes are coded as binary variable blue orange and then predicted by nearest neighbor classification
from least squares to nearest neighbors the linear decision boundary from least squares is very smooth and apparently stable to fit
it does appear to rely heavily on the assumption that linear decision boundary is appropriate
in language we will develop shortly it has low variance and potentially high bias
on the other hand the nearest neighbor procedures do not appear to rely on any stringent assumptions about the underlying data and can adapt to any situation
however any particular subregion of the decision boundary depends on handful of input points and their particular positions and is thus wiggly and unstable high variance and low bias
each method has its own situations for which it works best in particular linear regression is more appropriate for scenario above while nearest neighbors are more suitable for scenario
the time has come to expose the oracle
the data in fact were simulated from model somewhere between the two but closer to scenario
first we generated means mk from bivariate gaussian distribution and labeled this class blue
similarly more were drawn from and labeled class orange
then for each class we generated observations as follows for each observation we picked an mk at random with probability and
misclassification curves for the simulation example used in figures and
single training sample of size was used and test sample of size
the orange curves are test and the blue are training error for nearest neighbor classification
the results for linear regression are the bigger orange and blue squares at three degrees of freedom
the purple line is the optimal bayes error rate then generated mk thus leading to mixture of gaussian clusters for each class
figure shows the results of classifying new observations generated from the model
we compare the results for least squares and those for nearest neighbors for range of values of
large subset of the most popular techniques in use today are variants of these two simple procedures
in fact nearest neighbor the simplest of all captures large percentage of the market for low dimensional problems
statistical decision theory in this section we develop small amount of theory that provides framework for developing models such as those discussed informally so far
we first consider the case of quantitative output and place ourselves in the world of random variables and probability spaces
let irp denote real valued random input vector and ir real valued random output variable with joint distribution pr
we seek function for predicting given values of the input
this theory requires loss function for penalizing errors in prediction and by far the most common and convenient is squared error loss
this leads us to criterion for choosing epe pr dx dy the expected squared prediction error
by conditioning on we can write epe as epe ex ey and we see that it suffices to minimize epe pointwise argminc ey
the solution is the conditional expectation also known as the regression function
thus the best prediction of at any point is the conditional mean when best is measured by average squared error
the nearest neighbor methods attempt to directly implement this recipe using the training data
at each point we might ask for the average of all conditioning here amounts to factoring the joint density pr pr pr where pr pr pr and splitting up the bivariate integral accordingly
statistical decision theory those yi with input xi
since there is typically at most one observation at any point we settle for ave yi xi nk where ave denotes average and nk is the neighborhood containing the points in closest to
for large training sample size the points in the neighborhood are likely to be close to and as gets large the average will get more stable
in fact under mild regularity conditions on the joint probability distribution pr one can show that as such that
in light of this why look further since it seems we have universal approximator
we often do not have very large samples
if the linear or some more structured model is appropriate then we can usually get more stable estimate than nearest neighbors although such knowledge has to be learned from the data as well
there are other problems though sometimes disastrous
in section we see that as the dimension gets large so does the metric size of the nearest neighborhood
so settling for nearest neighborhood as surrogate for conditioning will fail us miserably
the convergence above still holds but the rate of convergence decreases as the dimension increases
how does linear regression fit into this framework
the simplest explanation is that one assumes that the regression function is approximately linear in its arguments xt
this is model based approach we specify model for the regression function
plugging this linear model for into epe and differentiating we can solve for theoretically xx xy
note we have not conditioned on rather we have used our knowledge of the functional relationship to pool over values of
the least squares solution amounts to replacing the expectation in by averages over the training data
so both nearest neighbors and least squares end up approximating conditional expectations by averages
although the latter seems more palatable we have already seen that we may pay price for this flexibility
many of the more modern techniques described in this book are model based although far more flexible than the rigid linear model
for example additive models assume that fj xj
this retains the additivity of the linear model but each coordinate function fj is arbitrary
it turns out that the optimal estimate for the additive model uses techniques such as nearest neighbors to approximate univariate conditional expectations simultaneously for each of the coordinate functions
thus the problems of estimating conditional expectation in high dimensions are swept away in this case by imposing some often unrealistic model assumptions in this case additivity
are we happy with the criterion
what happens if we replace the loss function with the
the solution in this case is the conditional median median which is different measure of location and its estimates are more robust than those for the conditional mean
criteria have discontinuities in their derivatives which have hindered their widespread use
other more resistant loss functions will be mentioned in later chapters but squared error is analytically convenient and the most popular
what do we do when the output is categorical variable
the same paradigm works here except we need different loss function for penalizing prediction errors
an estimate will assume values in the set of possible classes
our loss function can be represented by matrix where card
will be zero on the diagonal and nonnegative elsewhere where is the price paid for classifying an observation belonging to class gk as
most often we use the zero one loss function where all misclassifications are charged single unit
the expected prediction error is epe where again the expectation is taken with respect to the joint distribution pr
again we condition and can write epe as epe ex gk pr gk
the optimal bayes decision boundary for the simulation example of figures and
since the generating density is known for each class this boundary can be calculated exactly exercise and again it suffices to minimize epe pointwise argming gk pr gk
with the loss function this simplifies to argming pr or simply gk if pr gk max pr
this reasonable solution is known as the bayes classifier and says that we classify to the most probable class using the conditional discrete distribution pr
figure shows the bayes optimal decision boundary for our simulation example
the error rate of the bayes classifier is called the bayes rate
overview of supervised learning again we see that the nearest neighbor classifier directly approximates this solution majority vote in nearest neighborhood amounts to exactly this except that conditional probability at point is relaxed to conditional probability within neighborhood of point and probabilities are estimated by training sample proportions
suppose for two class problem we had taken the dummy variable approach and coded via binary followed by squared error loss estimation
then pr if corresponded to
likewise for class problem yk pr gk
this shows that our dummy variable regression procedure followed by classification to the largest fitted value is another way of representing the bayes classifier
although this theory is exact in practice problems can occur depending on the regression model used
for example when linear regression is used need not be positive and we might be suspicious about using it as an estimate of probability
we will discuss variety of approaches to modeling pr in chapter
local methods in high dimensions we have examined two learning techniques for prediction so far the stable but biased linear model and the less stable but apparently less biased class of nearest neighbor estimates
it would seem that with reasonably large set of training data we could always approximate the theoretically optimal conditional expectation by nearest neighbor averaging since we should be able to find fairly large neighborhood of observations close to any and average them
this approach and our intuition breaks down in high dimensions and the phenomenon is commonly referred to as the curse of dimensionality bellman
there are many manifestations of this problem and we will examine few here
consider the nearest neighbor procedure for inputs uniformly distributed in dimensional unit hypercube as in figure
suppose we send out hypercubical neighborhood about target point to capture fraction of the observations
since this corresponds to fraction of the unit volume the expected edge length will be ep
in ten dimensions and while the entire range for each input is only
so to capture or of the data to form local average we must cover or of the range of each input variable
such neighborhoods are no longer local
reducing dramatically does not help much either since the fewer observations we average the higher is the variance of our fit
another consequence of the sparse sampling in high dimensions is that all sample points are close to an edge of the sample
consider data points uniformly distributed in dimensional unit ball centered at the origin
suppose we consider nearest neighbor estimate at the origin
the median
the curse of dimensionality is well illustrated by subcubical neighborhood for uniform data in unit cube
the figure on the right shows the side length of the subcube needed to capture fraction of the volume of the data for different dimensions
in ten dimensions we need to cover of the range of each coordinate to capture of the data distance from the origin to the closest data point is given by the expression exercise
more complicated expression exists for the mean distance to the closest point
for more than halfway to the boundary
hence most data points are closer to the boundary of the sample space than to any other data point
the reason that this presents problem is that prediction is much more difficult near the edges of the training sample
one must extrapolate from neighboring sample points rather than interpolate between them
another manifestation of the curse is that the sampling density is proportional to where is the dimension of the input space and is the sample size
thus if represents dense sample for single input problem then is the sample size required for the same sampling density with inputs
thus in high dimensions all feasible training samples sparsely populate the input space
let us construct another uniform example
suppose we have training examples xi generated uniformly on
assume that the true relationship between and is without any measurement error
we use the nearest neighbor rule to predict at the test point
denote the training set by
we can
overview of supervised learning compute the expected prediction error at for our procedure averaging over all such samples of size
since the problem is deterministic this is the mean squared error mse for estimating mse et et et et vart bias
figure illustrates the setup
we have broken down the mse into two components that will become familiar as we proceed variance and squared bias
such decomposition is always possible and often useful and is known as the bias variance decomposition
unless the nearest neighbor is at will be smaller than in this example and so the average estimate will be biased downward
the variance is due to the sampling variance of the nearest neighbor
in low dimensions and with the nearest neighbor is very close to and so both the bias and variance are small
as the dimension increases the nearest neighbor tends to stray further from the target point and both bias and variance are incurred
by for more than of the samples the nearest neighbor is distance greater than from the origin
thus as increases the estimate tends to be more often than not and hence the mse levels off at as does the bias and the variance starts dropping an artifact of this example
although this is highly contrived example similar phenomena occur more generally
the complexity of functions of many variables can grow exponentially with the dimension and if we wish to be able to estimate such functions with the same accuracy as function in low dimensions then we need the size of our training set to grow exponentially as well
in this example the function is complex interaction of all variables involved
the dependence of the bias term on distance depends on the truth and it need not always dominate with nearest neighbor
for example if the function always involves only few dimensions as in figure then the variance can dominate instead
suppose on the other hand that we know that the relationship between and is linear where and we fit the model by least squares to the training data
for an arbitrary test point we have xt which can pn be written as xt where is the ith element of xt
since under this model the least squares estimates are
local methods in high dimensions nn in one dimension nn in one vs
dimension mse vs
simulation example demonstrating the curse of dimensionality and its effect on mse bias and variance
the input features are uniformly distributed in for the top left panel shows the target func tion no noise in ir and demonstrates the error that nearest neighbor makes in estimating
the training point is indicated by the blue tick mark
the top right panel illustrates why the radius of the nearest neighborhood increases with dimension
the lower left panel shows the average radius of the nearest neighborhoods
the lower right panel shows the mse squared bias and variance curves as function of dimension
overview of supervised learning nn in one dimension mse vs
simulation example with the same setup as in figure
here the function is constant in all but one dimension
the variance dominates unbiased we find that epe ey et var et et et xt var vart bias et xt xt
here we have incurred an additional variance in the prediction error since our target is not deterministic
there is no bias and the variance depends on
if is large and were selected at random and assuming then xt cov and ex epe ex xt cov trace cov cov
here we see that the expected epe increases linearly as function of with slope
if is large and or is small this growth in variance is negligible in the deterministic case
by imposing some heavy restrictions on the class of models being fitted we have avoided the curse of dimensionality
some of the technical details in and are derived in exercise
figure compares nearest neighbor vs least squares in two situations both of which have the form uniform as before and
the sample size is
for the orange curve
local methods in high dimensions expected prediction error of nn vs
the curves show the expected prediction error at for nearest neighbor relative to least squares for the model
for the orange curve while for the blue curve is linear in the first coordinate for the blue curve cubic as in figure
shown is the relative epe of nearest neighbor to least squares which appears to start at around for the linear case
least squares is unbiased in this case and as discussed above the epe is slightly above
the epe for nearest neighbor is always above since the variance of in this case is at least and the ratio increases with dimension as the nearest neighbor strays from the target point
for the cubic case least squares is biased which moderates the ratio
clearly we could manufacture examples where the bias of least squares would dominate the variance and the nearest neighbor would come out the winner
by relying on rigid assumptions the linear model has no bias at all and negligible variance while the error in nearest neighbor is substantially larger
however if the assumptions are wrong all bets are off and the nearest neighbor may dominate
we will see that there is whole spectrum of models between the rigid linear models and the extremely flexible nearest neighbor models each with their own assumptions and biases which have been proposed specifically to avoid the exponential growth in complexity of functions in high dimensions by drawing heavily on these assumptions
before we delve more deeply let us elaborate bit on the concept of statistical models and see how they fit into the prediction framework
overview of supervised learning statistical models supervised learning and function approximation our goal is to find useful approximation to the function that underlies the predictive relationship between the inputs and outputs
in the theoretical setting of section we saw that squared error loss lead us to the regression function for quantitative response
we anticipate using other classes of models for in many cases specifically designed to overcome the dimensionality problems and here we discuss framework for incorporating them into the prediction problem
statistical model for the joint distribution pr suppose in fact that our data arose from statistical model where the random error has and is independent of
note that for this model and in fact the conditional distribution pr depends on only through the conditional mean
the additive error model is useful approximation to the truth
for most systems the input output pairs will not have deterministic relationship
generally there will be other unmeasured variables that also contribute to including measurement error
the additive model assumes that we can capture all these departures from deterministic relationship via the error
for some problems deterministic relationship does hold
many of the classification problems studied in machine learning are of this form where the response surface can be thought of as colored map defined in irp
the training data consist of colored examples from the map xi gi and the goal is to be able to color any point
here the function is deterministic and the randomness enters through the location of the training points
for the moment we will not pursue such problems but will see that they can be handled by techniques appropriate for the error based models
the assumption in that the errors are independent and identically distributed is not strictly necessary but seems to be at the back of our mind
statistical models supervised learning and function approximation when we average squared errors uniformly in our epe criterion
with such model it becomes natural to use least squares as data criterion for model estimation as in
simple modifications can be made to avoid the independence assumption for example we can have var and now both the mean and variance depend on
in general the conditional distribution pr can depend on in complicated ways but the additive error model precludes these
so far we have concentrated on the quantitative response
additive error models are typically not used for qualitative outputs in this case the target function is the conditional density pr and this is modeled directly
for example for two class data it is often reasonable to assume that the data arise from independent binary trials with the probability of one particular outcome being and the other
thus if is the coded version of then but the variance depends on as well var
supervised learning before we launch into more statistically oriented jargon we present the function fitting paradigm from machine learning point of view
suppose for simplicity that the errors are additive and that the model is reasonable assumption
supervised learning attempts to learn by example through teacher
one observes the system under study both the inputs and outputs and assembles training set of observations xi yi
the observed input values to the system xi are also fed into an artificial system known as learning algorithm usually computer program which also produces outputs xi in response to the inputs
the learning algorithm has the property that it can modify its input output relationship in response to differences yi xi between the original and generated outputs
this process is known as learning by example
upon completion of the learning process the hope is that the artificial and real outputs will be close enough to be useful for all sets of inputs likely to be encountered in practice
function approximation the learning paradigm of the previous section has been the motivation for research into the supervised learning problem in the fields of machine learning with analogies to human reasoning and neural networks with biological analogies to the brain
the approach taken in applied mathematics and statistics has been from the perspective of function approximation and estimation
here the data pairs xi yi are viewed as points in dimensional euclidean space
the function has domain equal to the dimensional input subspace and is related to the data via model
overview of supervised learning such as yi xi
for convenience in this chapter we will assume the domain is irp dimensional euclidean space although in general the inputs can be of mixed type
the goal is to obtain useful approximation to for all in some region of irp given the representations in
although somewhat less glamorous than the learning paradigm treating supervised learning as problem in function approximation encourages the geometrical concepts of euclidean spaces and mathematical concepts of probabilistic inference to be applied to the problem
this is the approach taken in this book
many of the approximations we will encounter have associated set of parameters that can be modified to suit the data at hand
for example the linear model xt has
another class of useful approximators can be expressed as linear basis expansions hk where the hk are suitable set of functions or transformations of the input vector
traditional examples are polynomial and trigonometric expansions where for example hk might be cos and so on
we also encounter nonlinear expansions such as the sigmoid transformation common to neural network models hk
exp xt we can use least squares to estimate the parameters in as we did for the linear model by minimizing the residual sum of squares rss yi xi as function of
this seems reasonable criterion for an additive error model
in terms of function approximation we imagine our parameterized function as surface in space and what we observe are noisy realizations from it
this is easy to visualize when and the vertical coordinate is the output as in figure
the noise is in the output coordinate so we find the set of parameters such that the fitted surface gets as close to the observed points as possible where close is measured by the sum of squared vertical errors in rss
for the linear model we get simple closed form solution to the minimization problem
this is also true for the basis function methods if the basis functions themselves do not have any hidden parameters
otherwise the solution requires either iterative methods or numerical optimization
while least squares is generally very convenient it is not the only criterion used and in some cases would not make much sense
more general
least squares fitting of function of two inputs
the parameters of are chosen so as to minimize the sum of squared vertical errors principle for estimation is maximum likelihood estimation
suppose we have random sample yi from density pr indexed by some parameters
the log probability of the observed sample is log pr yi
the principle of maximum likelihood assumes that the most reasonable values for are those for which the probability of the observed sample is largest
least squares for the additive error model with is equivalent to maximum likelihood using the conditional likelihood pr
so although the additional assumption of normality seems more restrictive the results are the same
the log likelihood of the data is log log yi xi and the only term involving is the last which is rss up to scalar negative multiplier
more interesting example is the multinomial likelihood for the regression function pr for qualitative output
suppose we have model pr gk pk for the conditional probability of each class given indexed by the parameter vector
then the
overview of supervised learning log likelihood also referred to as the cross entropy is log pgi xi and when maximized it delivers values of that best conform with the data in this likelihood sense
structured regression models we have seen that although nearest neighbor and other local methods focus directly on estimating the function at point they face problems in high dimensions
they may also be inappropriate even in low dimensions in cases where more structured approaches can make more efficient use of the data
this section introduces classes of such structured approaches
before we proceed though we discuss further the need for such classes
difficulty of the problem consider the rss criterion for an arbitrary function rss yi xi
minimizing leads to infinitely many solutions any function passing through the training points xi yi is solution
any particular solution chosen might be poor predictor at test points different from the training points
if there are multiple observation pairs xi yi ni at each value of xi the risk is limited
in this case the solutions pass through the average values of the yi at each xi see exercise
the situation is similar to the one we have already visited in section indeed is the finite sample version of on page
if the sample size were sufficiently large such that repeats were guaranteed and densely arranged it would seem that these solutions might all tend to the limiting conditional expectation
in order to obtain useful results for finite we must restrict the eligible solutions to to smaller set of functions
how to decide on the nature of the restrictions is based on considerations outside of the data
these restrictions are sometimes encoded via the parametric representation of or may be built into the learning method itself either implicitly or explicitly
these restricted classes of solutions are the major topic of this book
one thing should be clear though
any restrictions imposed on that lead to unique solution to do not really remove the ambiguity
classes of restricted estimators caused by the multiplicity of solutions
there are infinitely many possible restrictions each leading to unique solution so the ambiguity has simply been transferred to the choice of constraint
in general the constraints imposed by most learning methods can be described as complexity restrictions of one kind or another
this usually means some kind of regular behavior in small neighborhoods of the input space
that is for all input points sufficiently close to each other in some metric exhibits some special structure such as nearly constant linear or low order polynomial behavior
the estimator is then obtained by averaging or polynomial fitting in that neighborhood
the strength of the constraint is dictated by the neighborhood size
the larger the size of the neighborhood the stronger the constraint and the more sensitive the solution is to the particular choice of constraint
for example local constant fits in infinitesimally small neighborhoods is no constraint at all local linear fits in very large neighborhoods is almost globally linear model and is very restrictive
the nature of the constraint depends on the metric used
some methods such as kernel and local regression and tree based methods directly specify the metric and size of the neighborhood
the nearest neighbor methods discussed so far are based on the assumption that locally the function is constant close to target input the function does not change much and so close outputs can be averaged to produce
other methods such as splines neural networks and basis function methods implicitly define neighborhoods of local behavior
in section we discuss the concept of an equivalent kernel see figure on page which describes this local dependence for any method linear in the outputs
these equivalent kernels in many cases look just like the explicitly defined weighting kernels discussed above peaked at the target point and falling away smoothly away from it
one fact should be clear by now
any method that attempts to produce locally varying functions in small isotropic neighborhoods will run into problems in high dimensions again the curse of dimensionality
and conversely all methods that overcome the dimensionality problems have an associated and often implicit or adaptive metric for measuring neighborhoods which basically does not allow the neighborhood to be simultaneously small in all directions
classes of restricted estimators the variety of nonparametric regression techniques or learning methods fall into number of different classes depending on the nature of the restrictions imposed
these classes are not distinct and indeed some methods fall in several classes
here we give brief summary since detailed descriptions
overview of supervised learning are given in later chapters
each of the classes has associated with it one or more parameters sometimes appropriately called smoothing parameters that control the effective size of the local neighborhood
here we describe three broad classes
roughness penalty and bayesian methods here the class of functions is controlled by explicitly penalizing rss with roughness penalty prss bb rss bb
the user selected functional will be large for functions that vary too rapidly over small regions of input space
for example the popular cubic smoothing spline for one dimensional inputs is the solution to the penalized least squares criterion prss bb yi xi bb dx
the roughness penalty here controls large values of the second derivative of and the amount of penalty is dictated by bb
for bb no penalty is imposed and any interpolating function will do while for bb only functions linear in are permitted
penalty functionals can be constructed for functions in any dimension and special versions can be created pp to impose special structure
for example additive penalties fj are used in conjunction with pp additive functions fj xj to create additive models with smooth coordinate pmfunctions
similarly projection pursuit regression modt els have gm for adaptively chosen directions and the functions gm can each have an associated roughness penalty
penalty function or regularization methods express our prior belief that the type of functions we seek exhibit certain type of smooth behavior and indeed can usually be cast in bayesian framework
the penalty corresponds to log prior and prss bb the log posterior distribution and minimizing prss bb amounts to finding the posterior mode
we discuss roughness penalty approaches in chapter and the bayesian paradigm in chapter
kernel methods and local regression these methods can be thought of as explicitly providing estimates of the regression function or conditional expectation by specifying the nature of the local neighborhood and of the class of regular functions fitted locally
the local neighborhood is specified by kernel function bb which assigns
classes of restricted estimators weights to points in region around see figure on page
for example the gaussian kernel has weight function based on the gaussian density function bb exp bb bb and assigns weights to points that die exponentially with their squared euclidean distance from
the parameter bb corresponds to the variance of the gaussian density and controls the width of the neighborhood
the simplest form of kernel estimate is the nadaraya watson weighted average pn bb xi yi pi
bb xi in general we can define local regression estimate of as where minimizes rss bb xi yi xi and is some parameterized function such as low order polynomial
nearest neighbor methods can be thought of as kernel methods having more data dependent metric
indeed the metric for nearest neighbors is kk where is the training observation ranked kth in distance from and is the indicator of the set
these methods of course need to be modified in high dimensions to avoid the curse of dimensionality
various adaptations are discussed in chapter
basis functions and dictionary methods this class of methods includes the familiar linear and polynomial expansions but more importantly wide variety of more flexible models
the model for is linear expansion of basis functions hm
overview of supervised learning where each of the hm is function of the input and the term linear here refers to the action of the parameters
this class covers wide variety of methods
in some cases the sequence of basis functions is prescribed such as basis for polynomials in of total degree
for one dimensional polynomial splines of degree can be represented by an appropriate sequence of spline basis functions determined in turn by knots
these produce functions that are piecewise polynomials of degree between the knots and joined up with continuity of degree at the knots
as an example consider linear splines or piecewise linear functions
one intuitively satisfying basis consists of the functions and bm tm where tm is the mth knot and denotes positive part
tensor products of spline bases can be used for inputs with dimensions larger than one see section and the cart and mars models in chapter
the parameter can be the total degree of the polynomial or the number of knots in the case of splines
radial basis functions are symmetric dimensional kernels located at particular centroids bb m for example the gaussian kernel bb bb is popular
radial basis functions have centroids m and scales bb that have to be determined
the spline basis functions have knots
in general we would like the data to dictate them as well
including these as parameters changes the regression problem from straightforward linear problem to combinatorially hard nonlinear problem
in practice shortcuts such as greedy algorithms or two stage processes are used
section describes some such approaches
single layer feed forward neural network model with linear output weights can be thought of as an adaptive basis function method
the model has the form bm where is known as the activation function
here as in the projection pursuit model the directions and the bias terms bm have to be determined and their estimation is the meat of the computation
details are give in chapter
these adaptively chosen basis function methods are also known as dictionary methods where one has available possibly infinite set or dictionary of candidate basis functions from which to choose and models are built up by employing some kind of search mechanism
in the case of the smoothing spline the parameter bb indexes models ranging from straight line fit to the interpolating model
similarly local degreem polynomial model ranges between degree global polynomial when the window size is infinitely large to an interpolating fit when the window size shrinks to zero
this means that we cannot use residual sum of squares on the training data to determine these parameters as well since we would always pick those that gave interpolating fits and hence zero residuals
such model is unlikely to predict future data well at all
the nearest neighbor regression fit usefully illustrates the competing forces that effect the predictive ability of such approximations
suppose the data arise from model with and var
for simplicity here we assume that the values of xi in the sample are fixed in advance nonrandom
the expected prediction error at also known as test or generalization error can be decomposed epek bias vart
the subscripts in parentheses indicate the sequence of nearest neighbors to
there are three terms in this expression
the first term is the irreducible error the variance of the new test target and is beyond our control even if we know the true
the second and third terms are under our control and make up the mean squared error of in estimating which is broken down into bias component and variance component
the bias term is the squared difference between the true mean and the expected value of the estimate et where the expectation averages the randomness in the training data
this term will most likely increase with if the true function is reasonably smooth
for small the few closest neighbors will have values close to so their average should
test and training error as function of model complexity be close to
as grows the neighbors are further away and then anything can happen
the variance term is simply the variance of an average here and decreases as the inverse of
so as varies there is bias variance tradeoff
more generally as the model complexity of our procedure is increased the variance tends to increase and the squared bias tends to decreases
the opposite behavior occurs as the model complexity is decreased
for nearest neighbors the model complexity is controlled by
typically we would like to choose our model complexity to trade bias off with variance in such way as to minimize pthe test error
an obvious estimate of test error is the training error yi
unfortunately training error is not good estimate of test error as it does not properly account for model complexity
figure shows the typical behavior of the test and training error as model complexity is varied
the training error tends to decrease whenever we increase the model complexity that is whenever we fit the data harder
however with too much fitting the model adapts itself too closely to the training data and will not generalize well have large test error
in that case the predictions will have large variance as reflected in the last term of expression
in contrast if the model is not complex enough it will underfit and may have large bias again resulting in poor generalization
in chapter we discuss methods for estimating the test error of prediction method and hence estimating the optimal amount of model complexity for given prediction method and training set
exercises bibliographic notes some good general books on the learning problem are duda et al
bishop bishop ripley cherkassky and mulier and vapnik
parts of this chapter are based on friedman
exercises ex
suppose each of classes has an associated target tk which is vector of all zeros except one in the kth position
show that classifying to the largest element of amounts to choosing the closest target mink tk if the elements of sum to one
show how to compute the bayes decision boundary for the simulation example in figure
derive equation
the edge effect problem discussed on page is not peculiar to uniform sampling from bounded domains
consider inputs drawn from spherical multinormal distribution ip
the squared distance from any sample point to the origin has distribution with mean
consider prediction point drawn from this distribution and let be an associated unit vector
let zi at xi be the projection of each of the training points on this direction
show that the zi are distributed with expected squared distance from the origin while the target point has expected squared distance from the origin
hence for randomly drawn test point is about standard deviations from the origin while all the training points are on average one standard deviation along direction
so most prediction points see themselves as lying on the edge of the training set
derive equation
the last line makes use of through conditioning argument derive equation making use of the cyclic property of the trace operator trace ab trace ba and its linearity which allows us to interchange the order of trace and expectation
consider regression problem with inputs xi and outputs yi and parameterized model to be fit by least squares
show that if there are observations with tied or identical values of then the fit can be obtained from reduced weighted least squares problem
overview of supervised learning ex
suppose we have sample of pairs xi yi drawn from the distribution characterized as follows xi the design density yi xi is the regression function mean zero variance we construct an estimator for linear in the yi yi where the weights do not depend on the yi but do depend on the entire training sequence of xi denoted here by show that linear regression and nearest neighbor regression are members of this class of estimators
describe explicitly the weights in each of these cases decompose the conditional mean squared error ey into conditional squared bias and conditional variance component
like represents the entire training sequence of yi decompose the unconditional mean squared error ey into squared bias and variance component establish relationship between the squared biases and variances in the above two cases
compare the classification performance of linear regression and nearest neighbor classification on the zipcode data
in particular consider only the and and and
show both the training and test error for each choice
the zipcode data are available from the book website www stat stanford edu elemstatlearn
consider linear regression model with parameters fit by least squares to set of training data xn yn drawn at random from population
let be the least squares estimate
suppose we have some test data drawn at random from the same poppn ulation as the training data
if rtr yi xi and rte prove that rtr rte
exercises where the expectations are over all that is random in each expression
this exercise was brought to our attention by ryan tibshirani from homework assignment given by andrew ng
overview of supervised learning
this is page printer opaque this linear methods for regression introduction linear regression model assumes that the regression function is linear in the inputs xp
linear models were largely developed in the precomputer age of statistics but even in today's computer era there are still good reasons to study and use them
they are simple and often provide an adequate and interpretable description of how the inputs affect the output
for prediction purposes they can sometimes outperform fancier nonlinear models especially in situations with small numbers of training cases low signal to noise ratio or sparse data
finally linear methods can be applied to transformations of the inputs and this considerably expands their scope
these generalizations are sometimes called basis function methods and are discussed in chapter
in this chapter we describe linear methods for regression while in the next chapter we discuss linear methods for classification
on some topics we go into considerable detail as it is our firm belief that an understanding of linear methods is essential for understanding nonlinear ones
in fact many nonlinear techniques are direct generalizations of the linear methods discussed here
linear methods for regression linear regression models and least squares as introduced in chapter we have an input vector xp and want to predict real valued output
the linear regression model has the form xj
the linear model either assumes that the regression function is linear or that the linear model is reasonable approximation
for example if is five level factor input we might create xj such that xj
no matter the source of the xj the model is linear in the parameters
typically we have set of training data xn yn from which to estimate the parameters
each xi xi xi xip is vector of feature measurements for the ith case
the most popular estimation method is least squares in which we pick the coefficients to minimize the residual sum of squares rss yi xi yi xij
from statistical point of view this criterion is reasonable if the training observations xi yi represent independent random draws from their population
even if the xi were not drawn randomly the criterion is still valid if the yi are conditionally independent given the inputs xi
figure illustrates the geometry of least squares fitting in the irp dimensional
linear least squares fitting with ir
we seek the linear function of that minimizes the sum of squared residuals from space occupied by the pairs
note that makes no assumptions about the validity of model it simply finds the best linear fit to the data
least squares fitting is intuitively satisfying no matter how the data arise the criterion measures the average lack of fit
how do we minimize
denote by the matrix with each row an input vector with in the first position and similarly let be the vector of outputs in the training set
then we can write the residual sum of squares as rss
this is quadratic function in the parameters
differentiating with respect to we obtain rss xt rss xt
assuming for the moment that has full column rank and hence xt is positive definite we set the first derivative to zero xt to obtain the unique solution xt xt
the dimensional geometry of least squares regression with two predictors
the outcome vector is orthogonally projected onto the hyperplane spanned by the input vectors and
the projection represents the vector of the least squares predictions the predicted values at an input vector are given by the fitted values at the training inputs are xt xt where xi
the matrix xt xt appearing in equation is sometimes called the hat matrix because it puts the hat on
figure shows different geometrical representation of the least squares estimate this time in irn
we denote the column vectors of by xp with
for much of what follows this first column is treated like any other
these vectors span subspace of irn also referred to as the column space of
we minimize rss ky by choosing so that the residual vector is orthogonal to this subspace
this orthogonality is expressed in and the resulting estimate is hence the orthogonal projection of onto this subspace
the hat matrix computes the orthogonal projection and hence it is also known as projection matrix
it might happen that the columns of are not linearly independent so that is not of full rank
this would occur for example if two of the inputs were perfectly correlated
then xt is singular and the least squares coefficients are not uniquely defined
however the fitted values are still the projection of onto the column space of there is just more than one way to express that projection in terms of the column vectors of
the non full rank case occurs most often when one or more qualitative inputs are coded in redundant fashion
there is usually natural way to resolve the non unique representation by recoding and or dropping redundant columns in
most regression software packages detect these redundancies and automatically implement
linear regression models and least squares some strategy for removing them
rank deficiencies can also occur in signal and image analysis where the number of inputs can exceed the number of training cases
in this case the features are typically reduced by filtering or else the fitting is controlled by regularization section and chapter
up to now we have made minimal assumptions about the true distribution of the data
in order to pin down the sampling properties of we now assume that the observations yi are uncorrelated and have constant variance and that the xi are fixed non random
the variance covariance matrix of the least squares parameter estimates is easily derived from and is given by var xt
typically one estimates the variance by xn yi
the rather than in the denominator makes an unbiased estimate of
to draw inferences about the parameters and the model additional assumptions are needed
we now assume that is the correct model for the mean that is the conditional expectation of is linear in xp
we also assume that the deviations of around its expectation are additive and gaussian
hence xp xp xj where the error is gaussian random variable with expectation zero and variance written
under it is easy to show that xt
this is multivariate normal distribution with mean vector and variance covariance matrix as shown
also chi squared distribution with degrees of freedom
in addition and are statistically independent
we use these distributional properties to form tests of hypothesis and confidence intervals for the parameters
the tail probabilities pr for three distributions and standard normal
shown are the appropriate quantiles for testing significance at the and levels
the difference between and the standard normal becomes negligible for bigger than about
to test the hypothesis that particular coefficient we form the standardized coefficient or score zj vj where vj is the jth diagonal element of xt
under the null hypothesis that zj is distributed as tn distribution with degrees of freedom and hence large absolute value of zj will lead to rejection of this null hypothesis
if is replaced by known value then zj would have standard normal distribution
the difference between the tail quantiles of distribution and standard normal become negligible as the sample size increases and so we typically use the normal quantiles see figure
often we need to test for the significance of groups of coefficients simultaneously
for example to test if categorical variable with levels can be excluded from model we need to test whether the coefficients of the dummy variables used to represent the levels can all be set to zero
here we use the statistic rss rss rss where rss is the residual sum of squares for the least squares fit of the bigger model with parameters and rss the same for the nested smaller model with parameters having parameters constrained to be
linear regression models and least squares zero
the statistic measures the change in residual sum of squares per additional parameter in the bigger model and it is normalized by an estimate of
under the gaussian assumptions and the null hypothesis that the smaller model is correct the statistic will have fp distribution
it can be shown exercise that the zj in are equivalent to the statistic for dropping the single coefficient from the model
for large the quantiles of the fp approach those of the
similarly we can isolate in to obtain confidence interval for vj vj
here is the percentile of the normal distribution etc
hence the standard practice of reporting se amounts to an approximate confidence interval
even if the gaussian error assumption does not hold this interval will be approximately correct with its coverage approaching as the sample size
in similar fashion we can obtain an approximate confidence set for the entire parameter vector namely xt where is the percentile of the chi squared distribution on degrees of freedom for example
this confidence set for generates corresponding confidence set for the true function xt namely xt exercise see also figure in section for examples of confidence bands for functions
example prostate cancer the data for this example come from study by stamey et al
they examined the correlation between the level of prostate specific antigen and number of clinical measures in men who were about to receive radical prostatectomy
the variables are log cancer volume lcavol log prostate weight lweight age log of the amount of benign prostatic hyperplasia lbph seminal vesicle invasion svi log of capsular penetration lcp gleason score gleason and percent of gleason scores or pgg
the correlation matrix of the predictors given in table shows many strong correlations
figure page of chapter is scatterplot matrix showing every pairwise plot between the variables
we see that svi is binary variable and gleason is an ordered categorical variable
we see for
linear methods for regression table
correlations of predictors in the prostate cancer data lcavol lweight age lbph svi lcp gleason lweight age lbph svi lcp gleason pgg table
linear model fit to the prostate cancer data
the score is the coefficient divided by its standard error
roughly score larger than two in absolute value is significantly nonzero at the level
term coefficient std
error score intercept lcavol lweight age lbph svi lcp gleason pgg example that both lcavol and lcp show strong relationship with the response lpsa and with each other
we need to fit the effects jointly to untangle the relationships between the predictors and the response
we fit linear model to the log of prostate specific antigen lpsa after first standardizing the predictors to have unit variance
we randomly split the dataset into training set of size and test set of size
we applied least squares estimation to the training set producing the estimates standard errors and scores shown in table
the scores are defined in and measure the effect of dropping that variable from the model
score greater than in absolute value is approximately significant at the level
for our example we have nine parameters and the tail quantiles of the distribution are
the predictor lcavol shows the strongest effect with lweight and svi also strong
notice that lcp is not significant once lcavol is in the model when used in model without lcavol lcp is strongly significant
we can also test for the exclusion of number of terms at once using the statistic
for example we consider dropping all the non significant terms in table namely age
linear regression models and least squares lcp gleason and pgg
we get which has value of pr and hence is not significant
the mean prediction error on the test data is
in contrast prediction using the mean training value of lpsa has test error of which is called the base error rate
hence the linear model reduces the base error rate by about
we will return to this example later to compare various selection and shrinkage methods
the gauss markov theorem one of the most famous results in statistics asserts that the least squares estimates of the parameters have the smallest variance among all linear unbiased estimates
we will make this precise here and also make clear that the restriction to unbiased estimates is not necessarily wise one
this observation will lead us to consider biased estimates such as ridge regression later in the chapter
we focus on estimation of any linear combination of the parameters at for example predictions xt are of this form
the least squares estimate of at is at at xt xt
considering to be fixed this is linear function ct of the response vector
if we assume that the linear model is correct at is unbiased since at at xt xt at xt xt at
the gauss markov theorem states that if we have any other linear estimator ct that is unbiased for at that is ct at then var at var ct
the proof exercise uses the triangle inequality
for simplicity we have stated the result in terms of estimation of single parameter at but with few more definitions one can state it in terms of the entire parameter vector exercise
consider the mean squared error of an estimator in estimating mse var
linear methods for regression the first term is the variance while the second term is the squared bias
the gauss markov theorem implies that the least squares estimator has the smallest mean squared error of all linear estimators with no bias
however there may well exist biased estimator with smaller mean squared error
such an estimator would trade little bias for larger reduction in variance
biased estimates are commonly used
any method that shrinks or sets to zero some of the least squares coefficients may result in biased estimate
we discuss many examples including variable subset selection and ridge regression later in this chapter
from more pragmatic point of view most models are distortions of the truth and hence are biased picking the right model amounts to creating the right balance between bias and variance
we go into these issues in more detail in chapter
mean squared error is intimately related to prediction accuracy as discussed in chapter
consider the prediction of the new response at input
then the expected prediction error of an estimate dc xt is dc xt mse dc
therefore expected prediction error and mean squared error differ only by the constant representing the variance of the new observation
multiple regression from simple univariate regression the linear model with inputs is called the multiple linear regression model
the least squares estimates for this model are best understood in terms of the estimates for the univariate linear model as we indicate in this section
suppose first that we have univariate model with no intercept that is
the least squares estimate and residuals are pn xi yi xi ri yi xi
in convenient vector notation we let yn xn and define hx yi xi yi
linear regression models and least squares the inner product between and
then we can write hx yi hx xi
as we will see this simple univariate regression provides the building block for multiple linear regression
suppose next that the inputs xp the columns of the data matrix are orthogonal that is hxj xk for all
then it is easy to check that the multiple least squares estimates are equal to hxj yi hxj xj the univariate estimates
in other words when the inputs are orthogonal they have no effect on each other's parameter estimates in the model
orthogonal inputs occur most often with balanced designed experiments where orthogonality is enforced but almost never with observational data
hence we will have to orthogonalize them in order to carry this idea further
suppose next that we have an intercept and single input
then the least squares coefficient of has the form hx yi hx where xi and the vector of ones
we can view the estimate as the result of two applications of the simple regression
the steps are regress on to produce the residual regress on the residual to give the coefficient
in this procedure regress on means simple univariate regression of on with no intercept producing coefficient ha bi ha ai and residual vector
we say that is adjusted for or is orthogonalized with respect to
step orthogonalizes with respect to
step is just simple univariate regression using the orthogonal predictors and
figure shows this process for two general inputs and
the orthogonalization does not change the subspace spanned by and it simply produces an orthogonal basis for representing it
this recipe generalizes to the case of inputs as shown in algorithm
note that the inputs zj in step are orthogonal hence the simple regression coefficients computed there are in fact also the multiple regression coefficients
the inner product notation is suggestive of generalizations of linear regression to different metric spaces as well as to probability spaces
least squares regression by orthogonalization of the inputs
the vector is regressed on the vector leaving the residual vector
the regression of on gives the multiple regression coefficient of
adding together the projections of on each of and gives the least squares fit
algorithm regression by successive orthogonalization
initialize
for regress xj on zj to produce coefficients hz xj hz and residual vector zj pj xj kj zk
regress on the residual zp to give the estimate
the result of this algorithm is hzp yi
hzp zp re arranging the residual in step we can see that each of the xj is linear combination of the zk
since the zj are all orthogonal they form basis for the column space of and hence the least squares projection onto this subspace is
since zp alone involves xp with coefficient we see that the coefficient is indeed the multiple regression coefficient of on xp
this key result exposes the effect of correlated inputs in multiple regression
note also that by rearranging the xj any one of them could be in the last position and similar results holds
hence stated more generally we have shown that the jth multiple regression coefficient is the univariate regression coefficient of on xj the residual after regressing xj on xj xj xp
linear regression models and least squares the multiple regression coefficient represents the additional contribution of xj on after xj has been adjusted for xj xj xp
if xp is highly correlated with some of the other xk the residual vector zp will be close to zero and from the coefficient will be very unstable
this will be true for all the variables in the correlated set
in such situations we might have all the scores as in table be smallany one of the set can be deleted yet we cannot delete them all
from we also obtain an alternate formula for the variance estimates var
hzp zp kzp in other words the precision with which we can estimate depends on the length of the residual vector zp this represents how much of xp is unexplained by the other xk
algorithm is known as the gram schmidt procedure for multiple regression and is also useful numerical strategy for computing the estimates
we can obtain from it not just but also the entire multiple least squares fit as shown in exercise
we can represent step of algorithm in matrix form where has as columns the zj in order and is the upper triangular matrix with entries kj
introducing the diagonal matrix with jth diagonal entry djj kzj we get zd qr the so called qr decomposition of
here is an orthogonal matrix qt and is upper triangular matrix
the qr decomposition represents convenient orthogonal basis for the column space of
it is easy to see for example that the least squares solution is given by qt qq
equation is easy to solve because is upper triangular exercise
linear methods for regression multiple outputs suppose we have multiple outputs yk that we wish to predict from our inputs xp
we assume linear model for each output yk xj jk fk
with training cases we can write the model in matrix notation xb
here is the response matrix with ik entry yik is the input matrix is the matrix of parameters and is the matrix of errors
straightforward generalization of the univariate loss function is rss yik fk xi tr xb xb
the least squares estimates have exactly the same form as before xt xt
hence the coefficients for the kth outcome are just the least squares estimates in the regression of yk on xp
multiple outputs do not affect one another's least squares estimates
if the errors in are correlated then it might seem appropriate to modify in favor of multivariate version
specifically suppose cov then the multivariate weighted criterion rss yi xi yi xi arises naturally from multivariate gaussian theory
here is the vector function fk and yi the vector of responses for observation
however it can be shown that again the solution is given by separate regressions that ignore the correlations exercise
if the vary among observations then this is no longer the case and the solution for no longer decouples
in section we pursue the multiple outcome problem and consider situations where it does pay to combine the regressions
subset selection subset selection there are two reasons why we are often not satisfied with the least squares estimates
prediction accuracy can sometimes be improved by shrinking or setting some coefficients to zero
by doing so we sacrifice little bit of bias to reduce the variance of the predicted values and hence may improve the overall prediction accuracy
with large number of predictors we often would like to determine smaller subset that exhibit the strongest effects
in order to get the big picture we are willing to sacrifice some of the small details
in this section we describe number of approaches to variable subset selection with linear regression
in later sections we discuss shrinkage and hybrid approaches for controlling variance as well as other dimension reduction strategies
these all fall under the general heading model selection
model selection is not restricted to linear models chapter covers this topic in some detail
with subset selection we retain only subset of the variables and eliminate the rest from the model
least squares regression is used to estimate the coefficients of the inputs that are retained
there are number of different strategies for choosing the subset
best subset selection best subset regression finds for each the subset of size that gives smallest residual sum of squares
an efficient algorithmthe leaps and bounds procedure furnival and wilson makes this feasible for as large as or
figure shows all the subset models for the prostate cancer example
the lower boundary represents the models that are eligible for selection by the best subsets approach
note that the best subset of size for example need not include the variable that was in the best subset of size for this example all the subsets are nested
the best subset curve red lower boundary in figure is necessarily decreasing so cannot be used to select the subset size
the question of how to choose involves the tradeoff between bias and variance along with the more subjective desire for parsimony
there are number of criteria that one may use typically we choose the smallest model that minimizes an estimate of the expected prediction error
many of the other approaches that we discuss in this chapter are similar in that they use the training data to produce sequence of models varying in complexity and indexed by single parameter
in the next section we use
all possible subset models for the prostate cancer example
at each subset size is shown the residual sum of squares for each model of that size cross validation to estimate prediction error and select the aic criterion is popular alternative
we defer more detailed discussion of these and other approaches to chapter
forwardand backward stepwise selection rather than search through all possible subsets which becomes infeasible for much larger than we can seek good path through them
forwardstepwise selection starts with the intercept and then sequentially adds into the model the predictor that most improves the fit
with many candidate predictors this might seem like lot of computation however clever updating algorithms can exploit the qr decomposition for the current fit to rapidly establish the next candidate exercise
like best subset regression forward stepwise produces sequence of models indexed by the subset size which must be determined
forward stepwise selection is greedy algorithm producing nested sequence of models
in this sense it might seem sub optimal compared to best subset selection
however there are several reasons why it might be preferred
comparison of four subset selection techniques on simulated linear regression problem
there are observations on standard gaussian variables with pairwise correlations all equal to
for of the variables the coefficients are drawn at random from distribution the rest are zero
the noise resulting in signal to noise ratio of
results are averaged over simulations
shown is the mean squared error of the estimated coefficient at each step from the true
backward stepwise selection starts with the full model and sequentially deletes the predictor that has the least impact on the fit
the candidate for dropping is the variable with the smallest score exercise
backward selection can only be used when while forward stepwise can always be used
figure shows the results of small simulation study to compare best subset regression with the simpler alternatives forward and backward selection
their performance is very similar as is often the case
included in the figure is forward stagewise regression next section which takes longer to reach minimum error
linear methods for regression on the prostate cancer example best subset forward and backward selection all gave exactly the same sequence of terms
some software packages implement hybrid stepwise selection strategies that consider both forward and backward moves at each step and select the best of the two
for example in the package the step function uses the aic criterion for weighing the choices which takes proper account of the number of parameters fit at each step an add or drop will be performed that minimizes the aic score
other more traditional packages base the selection on statistics adding significant terms and dropping non significant terms
these are out of fashion since they do not take proper account of the multiple testing issues
it is also tempting after model search to print out summary of the chosen model such as in table however the standard errors are not valid since they do not account for the search process
the bootstrap section can be useful in such settings
finally we note that often variables come in groups such as the dummy variables that code multi level categorical predictor
smart stepwise procedures such as step in will add or drop whole groups at time taking proper account of their degrees of freedom
forward stagewise regression forward stagewise regression fs is even more constrained than forwardstepwise regression
it starts like forward stepwise regression with an intercept equal to and centered predictors with coefficients initially all
at each step the algorithm identifies the variable most correlated with the current residual
it then computes the simple linear regression coefficient of the residual on this chosen variable and then adds it to the current coefficient for that variable
this is continued till none of the variables have correlation with the residuals the least squares fit when
unlike forward stepwise regression none of the other variables are adjusted when term is added to the model
as consequence forward stagewise can take many more than steps to reach the least squares fit and historically has been dismissed as being inefficient
it turns out that this slow fitting can pay dividends in high dimensional problems
we see in section that both forward stagewise and variant which is slowed down even further are quite competitive especially in very highdimensional problems
forward stagewise regression is included in figure
in this example it takes over steps to get all the correlations below
for subset size we plotted the error for the last step for which there where nonzero coefficients
although it catches up with the best fit it takes longer to do so
shrinkage methods prostate cancer data example continued table shows the coefficients from number of different selection and shrinkage methods
they are best subset selection using an all subsets search ridge regression the lasso principal components regression and partial least squares
each method has complexity parameter and this was chosen to minimize an estimate of prediction error based on tenfold cross validation full details are given in section
briefly cross validation works by dividing the training data randomly into ten equal parts
the learning method is fit for range of values of the complexity parameter to nine tenths of the data and the prediction error is computed on the remaining one tenth
this is done in turn for each one tenth of the data and the ten prediction error estimates are averaged
from this we obtain an estimated prediction error curve as function of the complexity parameter
note that we have already divided these data into training set of size and test set of size
cross validation is applied to the training set since selecting the shrinkage parameter is part of the training process
the test set is there to judge the performance of the selected model
the estimated prediction error curves are shown in figure
many of the curves are very flat over large ranges near their minimum
included are estimated standard error bands for each estimated error rate based on the ten error estimates computed by cross validation
we have used the one standard error rule we pick the most parsimonious model within one standard error of the minimum section page
such rule acknowledges the fact that the tradeoff curve is estimated with error and hence takes conservative approach
best subset selection chose to use the two predictors lcvol and lweight
the last two lines of the table give the average prediction error and its estimated standard error over the test set
shrinkage methods by retaining subset of the predictors and discarding the rest subset selection produces model that is interpretable and has possibly lower prediction error than the full model
however because it is discrete processvariables are either retained or discarded it often exhibits high variance and so doesn't reduce the prediction error of the full model
shrinkage methods are more continuous and don't suffer as much from high variability
ridge regression ridge regression shrinks the regression coefficients by imposing penalty on their size
the ridge coefficients minimize penalized residual sum of
estimated prediction error curves and their standard errors for the various selection and shrinkage methods
each curve is plotted as function of the corresponding complexity parameter for that method
the horizontal axis has been chosen so that the model complexity increases as we move from left to right
the estimates of prediction error and their standard errors were obtained by tenfold cross validation full details are given in section
the least complex model within one standard error of the best is chosen indicated by the purple vertical broken lines
shrinkage methods table
estimated coefficients and test error results for different subset and shrinkage methods applied to the prostate data
the blank entries correspond to variables omitted
term ls best subset ridge lasso pcr pls intercept lcavol lweight age lbph svi lcp gleason pgg test error std error squares bd be ridge argmin yi xij bb
here bb is complexity parameter that controls the amount of shrinkage the larger the value of bb the greater the amount of shrinkage
the coefficients are shrunk toward zero and each other
the idea of penalizing by the sum of squares of the parameters is also used in neural networks where it is known as weight decay chapter
an equivalent way to write the ridge problem is ridge argmin yi xij subject to which makes explicit the size constraint on the parameters
there is oneto one correspondence between the parameters bb in and in
when there are many correlated variables in linear regression model their coefficients can become poorly determined and exhibit high variance
wildly large positive coefficient on one variable can be canceled by similarly large negative coefficient on its correlated cousin
by imposing size constraint on the coefficients as in this problem is alleviated
the ridge solutions are not equivariant under scaling of the inputs and so one normally standardizes the inputs before solving
in addition
linear methods for regression notice that the intercept has been left out of the penalty term
penalization of the intercept would make the procedure depend on the origin chosen for that is adding constant to each of the targets yi would not simply result in shift of the predictions by the same amount
it can be shown exercise that the solution to can be separated into two parts after reparametrization using centered inputs each xij gets pn replaced by xij
we estimate by yi
the remaining coefficients get estimated by ridge regression without intercept using the centered xij
henceforth we assume that this centering has been done so that the input matrix has rather than columns
writing the criterion in in matrix form rss bb bb the ridge regression solutions are easily seen to be ridge xt bb xt where is the identity matrix
notice that with the choice of quadratic penalty the ridge regression solution is again linear function of
the solution adds positive constant to the diagonal of xt before inversion
this makes the problem nonsingular even if xt is not of full rank and was the main motivation for ridge regression when it was first introduced in statistics hoerl and kennard
traditional descriptions of ridge regression start with definition
we choose to motivate it via and as these provide insight into how it works
figure shows the ridge coefficient estimates for the prostate cancer example plotted as functions of df bb the effective degrees of freedom implied by the penalty bb defined in on page
in the case of orthonormal inputs the ridge estimates are just scaled version of the least squares estimates that is ridge bb
ridge regression can also be derived as the mean or mode of posterior distribution with suitably chosen prior distribution
in detail suppose yi xti and the parameters are each distributed as independently of one another
then the negative log posterior density of with and assumed known is equal to the expression in curly braces in with bb exercise
thus the ridge estimate is the mode of the posterior distribution since the distribution is gaussian it is also the posterior mean
the singular value decomposition svd of the centered input matrix gives us some additional insight into the nature of ridge regression
this decomposition is extremely useful in the analysis of many statistical methods
the svd of the matrix has the form udvt
profiles of ridge coefficients for the prostate cancer example as the tuning parameter bb is varied
coefficients are plotted versus df bb the effective degrees of freedom
vertical line is drawn at df the value chosen by cross validation
linear methods for regression here and are and orthogonal matrices with the columns of spanning the column space of and the columns of spanning the row space
is diagonal matrix with diagonal entries dp called the singular values of
if one or more values dj is singular
using the singular value decomposition we can write the least squares fitted vector as ls xt xt uut after some simplification
note that ut are the coordinates of with respect to the orthonormal basis
note also the similarity with and are generally different orthogonal bases for the column space of exercise
now the ridge solutions are ridge xt bb xt bb ut xp uj utj bb where the uj are the columns of
note that since bb we have bb
like linear regression ridge regression computes the coordinates of with respect to the orthonormal basis
it then shrinks these coordinates by the factors bb
this means that greater amount of shrinkage is applied to the coordinates of basis vectors with smaller
what does small value of mean
the svd of the centered matrix is another way of expressing the principal components of the variables in
the sample covariance matrix is given by xt and from we have xt vd vt which is the eigen decomposition of xt and of up to factor
the eigenvectors vj columns of are also called the principal components or karhunen loeve directions of
the first principal component direction has the property that xv has the largest sample variance amongst all normalized linear combinations of the columns of
this sample variance is easily seen to be var var xv and in fact xv
the derived variable is called the first principal component of and hence is the normalized first principal
principal components of some input data points
the largest principal component is the direction that maximizes the variance of the projected data and the smallest principal component minimizes that variance
ridge regression projects onto these components and then shrinks the coefficients of the low variance components more than the high variance components component
subsequent principal components zj have maximum variance subject to being orthogonal to the earlier ones
conversely the last principal component has minimum variance
hence the small singular values dj correspond to directions in the column space of having small variance and ridge regression shrinks these directions the most
figure illustrates the principal components of some data points in two dimensions
if we consider fitting linear surface over this domain the axis is sticking out of the page the configuration of the data allow us to determine its gradient more accurately in the long direction than the short
ridge regression protects against the potentially high variance of gradients estimated in the short directions
the implicit assumption is that the response will tend to vary most in the directions of high variance of the inputs
this is often reasonable assumption since predictors are often chosen for study because they vary with the response variable but need not hold in general
linear methods for regression in figure we have plotted the estimated prediction error versus the quantity df bb tr xt bb xt tr bb xp
bb this monotone decreasing function of bb is the effective degrees of freedom of the ridge regression fit
usually in linear regression fit with variables the degrees of freedom of the fit is the number of free parameters
the idea is that although all coefficients in ridge fit will be non zero they are fit in restricted fashion controlled by bb
note that df bb when bb no regularization and df bb as bb
of course there is always an additional one degree of freedom for the intercept which was removed apriori
this definition is motivated in more detail in section and sections
in figure the minimum occurs at df bb
table shows that ridge regression reduces the test error of the full least squares estimates by small amount
the lasso the lasso is shrinkage method like ridge with subtle but important differences
the lasso estimate is defined by lasso argmin yi xij subject to
just as in ridge regression we can re parametrize the constant by standardizing the predictors the solution for is and thereafter we fit model without an intercept exercise
in the signal processing literature the lasso is also known as basis pursuit chen et al
we can also write the lasso problem in the equivalent lagrangian form bd xp xp be lasso argmin yi xij bb
pp to the ridge regression problem or notice the similarity pp the ridge penalty is replaced by the lasso penalty
this latter constraint makes the solutions nonlinear in the yi and there is no closed form expression as in ridge regression
computing the lasso solution
shrinkage methods is quadratic programming problem although we see in section that efficient algorithms are available for computing the entire path of solutions as bb is varied with the same computational cost as for ridge regression
because of the nature of the constraint making sufficiently small will cause some of the coefficients to be exactly zero
thus the lasso pp does kind of continuous subset selection
if is chosen larger than where jls the least squares estimates then the lasso estimates are the
on the other hand for say then the least squares coefficients are shrunk by about on average
however the nature of the shrinkage is not obvious and we investigate it further in section below
like the subset size in variable subset selection or the penalty parameter in ridge regression should be adaptively chosen to minimize an estimate of expected prediction error
in figure for ease of interpretation we have plotted the lasso pp prediction error estimates versus the standardized parameter
value was chosen by fold cross validation this caused four coefficients to be set to zero fifth column of table
the resulting model has the second lowest test error slightly lower than the full least squares model but the standard errors of the test error estimates last line of table are fairly large
figure shows pp the lasso coefficients as the standardized tuning parameter is varied
at these are the least squares estimates they decrease to as
this decrease is not always strictly monotonic although it is in this example
vertical line is drawn at the value chosen by cross validation
discussion subset selection ridge regression and the lasso in this section we discuss and compare the three approaches discussed so far for restricting the linear regression model subset selection ridge regression and the lasso
in the case of an orthonormal input matrix the three procedures have explicit solutions
each method applies simple transformation to the least squares estimate as detailed in table
ridge regression does proportional shrinkage
lasso translates each coefficient by constant factor bb truncating at zero
this is called soft thresholding and is used in the context of wavelet based smoothing in section
best subset selection drops all variables with coefficients smaller than the th largest this is form of hard thresholding
back to the nonorthogonal case some pictures help understand their relationship
figure depicts the lasso left and ridge regression right when there are only two parameters
the residual sum of squares has elliptical contours centered at the full least squares estimate
the constraint
profiles of lasso coefficients as the tuning parameter is varied
coefficients are plotted versus
vertical line is drawn at the value chosen by cross validation
compare figure on page the lasso profiles hit zero while those for ridge do not
the profiles are piece wise linear and so are computed only at the points displayed see section for details
shrinkage methods table
estimators of in the case of orthonormal columns of
and bb are constants chosen by the corresponding techniques sign denotes the sign of its argument and denotes positive part of
below the table estimators are shown by broken red lines
the line in gray shows the unrestricted estimate for reference
estimator formula best subset size ridge bb lasso sign bb best subset ridge lasso bb

estimation picture for the lasso left and ridge regression right
shown are contours of the error and constraint functions
the solid blue areas are the constraint regions and respectively while the red ellipses are the contours of the least squares error function
linear methods for regression region for ridge regression is the disk while that for lasso is the diamond
both methods find the first point where the elliptical contours hit the constraint region
unlike the disk the diamond has corners if the solution occurs at corner then it has one parameter equal to zero
when the diamond becomes rhomboid and has many corners flat edges and faces there are many more opportunities for the estimated parameters to be zero
we can generalize ridge regression and the lasso and view them as bayes estimates
consider the criterion argmin yi xij bb for
the contours of constant value of are shown in figure for the case of two inputs
thinking of as the log prior density for these are also the equicontours of the prior distribution of the parameters
the value corresponds to variable subset selection as the penalty simply counts the number of nonzero parameters corresponds to the lasso while to ridge regression
notice that for the prior is not uniform in direction but concentrates more mass in the coordinate directions
the prior corresponding to the case is an independent double exponential or laplace distribution for each input with density exp and bb
the case lasso is the smallest such that the constraint region is convex non convex constraint regions make the optimization problem more difficult
in this view the lasso ridge regression and best subset selection are bayes estimates with different priors
note however that they are derived as posterior modes that is maximizers of the posterior
it is more common to use the mean of the posterior as the bayes estimate
ridge regression is also the posterior mean but the lasso and best subset selection are not
looking again at the criterion we might try using other values of besides or
although one might consider estimating from the data our experience is that it is not worth the effort for the extra variance incurred
values of suggest compromise between the lasso and ridge regression
contours of constant value of for given values of
contours of constant value of for left plot and the elastic net penalty for right plot
although visually very similar the elastic net has sharp non differentiable corners while the penalty does not setting coefficients exactly to zero
partly for this reason as well as for computational tractability zou and hastie introduced the elasticnet penalty bb different compromise between ridge and lasso
figure compares the lq penalty with and the elastic net penalty with it is hard to detect the difference by eye
the elastic net selects variables like the lasso and shrinks together the coefficients of correlated predictors like ridge
it also has considerable computational advantages over the lq penalties
we discuss the elastic net further in section
least angle regression least angle regression lar is relative newcomer efron et al and can be viewed as kind of democratic version of forward stepwise regression section
as we will see lar is intimately connected with the lasso and in fact provides an extremely efficient algorithm for computing the entire lasso path as in figure
forward stepwise regression builds model sequentially adding one variable at time
at each step it identifies the best variable to include in the active set and then updates the least squares fit to include all the active variables
least angle regression uses similar strategy but only enters as much of predictor as it deserves
at the first step it identifies the variable most correlated with the response
rather than fit this variable completely lar moves the coefficient of this variable continuously toward its leastsquares value causing its correlation with the evolving residual to decrease in absolute value
as soon as another variable catches up in terms of correlation with the residual the process is paused
the second variable then joins the active set and their coefficients are moved together in way that keeps their correlations tied and decreasing
this process is continued
linear methods for regression until all the variables are in the model and ends at the full least squares fit
algorithm provides the details
the termination condition in step requires some explanation
if the lar algorithm reaches zero residual solution after steps the is because we have centered the data
algorithm least angle regression
standardize the predictors to have mean zero and unit norm
start with the residual
find the predictor xj most correlated with
move from towards its least squares coefficient hxj ri until some other competitor xk has as much correlation with the current residual as does xj
move and in the direction defined by their joint least squares coefficient of the current residual on xj xk until some other competitor xl has as much correlation with the current residual
continue in this way until all predictors have been entered
after min steps we arrive at the full least squares solution
suppose ak is the active set of variables at the beginning of the kth step and let ak be the coefficient vector for these variables at this step there will be nonzero values and the one just entered will be zero
if rk xak ak is the current residual then the direction for this step is xtak xak xtak rk
the coefficient profile then evolves as ak ak
exercise verifies that the directions chosen in this fashion do what is claimed keep the correlations tied and decreasing
if the fit vector at the beginning of this step is then it evolves as uk where uk xak is the new fit direction
the name least angle arises from geometrical interpretation of this process uk makes the smallest and equal angle with each of the predictors in ak exercise
figure shows the absolute correlations decreasing and joining ranks with each step of the lar algorithm using simulated data
by construction the coefficients in lar change in piecewise linear fashion
figure left panel shows the lar coefficient profile evolving as function of their arc length
note that we do not need to take small the arc length of differentiable curve for is given by tv rs ds where
for the piecewise linear lar coefficient profile this amounts to summing the norms of the changes in coefficients from step to step
progression of the absolute correlations during each step of the lar procedure using simulated data set with six predictors
the labels at the top of the plot indicate which variables enter the active set at each step
the step length are measured in units of arc length
left panel shows the lar coefficient profiles on the simulated data as function of the arc length
the right panel shows the lasso profile
they are identical until the dark blue coefficient crosses zero at an arc length of about
linear methods for regression steps and recheck the correlations in step using knowledge of the covariance of the predictors and the piecewise linearity of the algorithm we can work out the exact step length at the beginning of each step exercise
the right panel of figure shows the lasso coefficient profiles on the same data
they are almost identical to those in the left panel and differ for the first time when the blue coefficient passes back through zero
for the prostate data the lar coefficient profile turns out to be identical to the lasso profile in figure which never crosses zero
these observations lead to simple modification of the lar algorithm that gives the entire lasso path which is also piecewise linear
algorithm least angle regression lasso modification
if non zero coefficient hits zero drop its variable from the active set of variables and recompute the current joint least squares direction
the lar lasso algorithm is extremely efficient requiring the same order of computation as that of single least squares fit using the predictors
least angle regression always takes steps to get to the full least squares estimates
the lasso path can have more than steps although the two are often quite similar
algorithm with the lasso modification is an efficient way of computing the solution to any lasso problem especially when
osborne et al also discovered piecewise linear path for computing the lasso which they called homotopy algorithm
we now give heuristic argument for why these procedures are so similar
although the lar algorithm is stated in terms of correlations if the input features are standardized it is equivalent and easier to work with innerproducts
suppose is the active set of variables at some stage in the algorithm tied in their absolute inner product with the current residuals
we can express this as xtj sj where sj indicates the sign of the inner product and is the common value
also xtk
now consider the lasso criterion which we write in vector form bb
let be the active set of variables in the solution for given value of bb
for these variables is differentiable and the stationarity conditions give xtj bb sign comparing with we see that they are identical only if the sign of matches the sign of the inner product
that is why the lar
shrinkage methods algorithm and lasso start to differ when an active coefficient passes through zero condition is violated for that variable and it is kicked out of the active set
exercise shows that these equations imply piecewiselinear coefficient profile as bb decreases
the stationarity conditions for the non active variables require that xtk bb which again agrees with the lar algorithm
figure compares lar and lasso to forward stepwise and stagewise regression
the setup is the same as in figure on page except here here rather than so the problem is more difficult
we see that the more aggressive forward stepwise starts to overfit quite early well before the true variables can enter the model and ultimately performs worse than the slower forward stagewise regression
the behavior of lar and lasso is similar to that of forward stagewise regression
incremental forward stagewise is similar to lar and lasso and is described in section
degrees of freedom formula for lar and lasso suppose that we fit linear model via the least angle regression procedure stopping at some number of steps or equivalently using lasso bound that produces constrained version of the full least squares fit
how many parameters or degrees of freedom have we used
consider first linear regression using subset of features
if this subset is prespecified in advance without reference to the training data then the degrees of freedom used in the fitted model is defined to be
indeed in classical statistics the number of linearly independent parameters is what is meant by degrees of freedom
alternatively suppose that we carry out best subset selection to determine the optimal set of predictors
then the resulting model has parameters but in some sense we have used up more than degrees of freedom
we need more general definition for the effective degrees of freedom of an adaptively fitted model
we define the degrees of freedom of the fitted vector as df cov yi
here cov yi refers to the sampling covariance between the predicted value and its corresponding outcome value yi
this makes intuitive sense the harder that we fit to the data the larger this covariance and hence df
expression is useful notion of degrees of freedom one that can be applied to any model prediction
this includes models that are
comparison of lar and lasso with forward stepwise forward stagewise fs and incremental forward stagewise fs regression
the setup is the same as in figure except here rather than
here the slower fs regression ultimately outperforms forward stepwise
lar and lasso show similar behavior to fs and fs
since the procedures take different numbers of steps across simulation replicates and methods we plot the mse as function of the fraction of total arc length toward the least squares fit adaptively fitted to the training data
this definition is motivated and discussed further in sections
now for linear regression with fixed predictors it is easy to show that df
likewise for ridge regression this definition leads to the closed form expression on page df tr bb
in both these cases is simple to evaluate because the fit bb is linear in
if we think about definition in the context of best subset selection of size it seems clear that df will be larger than and this can be verified by estimating cov yi directly by simulation
however there is no closed form method for estimating df for best subset selection
for lar and lasso something magical happens
these techniques are adaptive in smoother way than best subset selection and hence estimation of degrees of freedom is more tractable
specifically it can be shown that after the kth step of the lar procedure the effective degrees of freedom of the fit vector is exactly
now for the lasso the modified lar procedure
methods using derived input directions often takes more than steps since predictors can drop out
hence the definition is little different for the lasso at any stage df approximately equals the number of predictors in the model
while this approximation works reasonably well anywhere in the lasso path for each it works best at the last model in the sequence that contains predictors
detailed study of the degrees of freedom for the lasso may be found in zou et al
methods using derived input directions in many situations we have large number of inputs often very correlated
the methods in this section produce small number of linear combinations zm of the original inputs xj and the zm are then used in place of the xj as inputs in the regression
the methods differ in how the linear combinations are constructed
principal components regression in this approach the linear combinations zm used are the principal components as defined in section above
principal component regression forms the derived input columns zm xvm and then regresses on zm for some
since the zm are orthogonal this regression is just sum of univariate regressions pcr zm where hzm yi hzm zm
since the zm are each linear combinations of the original xj we can express the solution in terms of coefficients of the xj exercise pcr vm
as with ridge regression principal components depend on the scaling of the inputs so typically we first standardize them
note that if we would just get back the usual least squares estimates since the columns of ud span the column space of
for we get reduced regression
we see that principal components regression is very similar to ridge regression both operate via the principal components of the input matrix
ridge regression shrinks the coefficients of the principal components figure shrinking more depending on the size of the corresponding eigenvalue principal components regression discards the smallest eigenvalue components
figure illustrates this
ridge regression shrinks the regression coefficients of the principal components using shrinkage factors bb as in
principal component regression truncates them
shown are the shrinkage and truncation patterns corresponding to figure as function of the principal component index
in figure we see that cross validation suggests seven terms the resulting model has the lowest test error in table
partial least squares this technique also constructs set of linear combinations of the inputs for regression but unlike principal components regression it uses in addition to for this construction
like principal component regression partial least squares pls is not scale invariant so we assume that each xj is standardized to have mean and variance
pls begins by computingp hxj yi for each
from this we construct the derived input xj which is the first partial least squares direction
hence in the construction of each zm the inputs are weighted by the strength of their univariate effect on
the outcome is regressed on giving coefficient and then we orthogonalize xp with respect to
we continue this process until directions have been obtained
in this manner partial least squares produces sequence of derived orthogonal inputs or directions zm
as with principal component regression if we were to construct all directions we would get back solution equivalent to the usual least squares estimates using directions produces reduced regression
the procedure is described fully in algorithm
since the are standardized the first directions are the univariate regression coefficients up to an irrelevant constant this is not the case for subsequent directions
methods using derived input directions algorithm partial least squares
standardize each xj to have mean zero and variance one
set and xj xj
for pp zm mj xj where mj hxj yi hzm yi hzm zm zm orthogonalize each xj with respect to zm xj xj hzm xj hzm zm zm
output the sequence of fitted vectors
since the are pls linear in the original xj so is
these linear coefficients can be recovered from the sequence of pls transformations
in the prostate cancer example cross validation chose pls directions in figure
this produced the model given in the rightmost column of table
what optimization problem is partial least squares solving
since it uses the response to construct its directions its solution path is nonlinear function of
it can be shown exercise that partial least squares seeks directions that have high variance and have high correlation with the response in contrast to principal components regression which keys only on high variance stone and brooks frank and friedman
in particular the mth principal component direction vm solves max var subject to sv where is the sample covariance matrix of the xj
the conditions sv ensures that zm is uncorrelated with all the previous linear combinations xv
the mth pls direction solves max corr var subject to
further analysis reveals that the variance aspect tends to dominate and so partial least squares behaves much like ridge regression and principal components regression
we discuss this further in the next section
if the input matrix is orthogonal then partial least squares finds the least squares estimates after steps
subsequent steps have no effect
linear methods for regression since the mj are zero for exercise
it can also be shown that the sequence of pls coefficients for represents the conjugate gradient sequence for computing the least squares solutions exercise
discussion comparison of the selection and shrinkage methods there are some simple settings where we can understand better the relationship between the different methods described above
consider an example with two correlated inputs and with correlation
we assume that the true regression coefficients are and
figure shows the coefficient profiles for the different methods as their tuning parameters are varied
the top panel has the bottom panel
the tuning parameters for ridge and lasso vary over continuous range while best subset pls and pcr take just two discrete steps to the least squares solution
in the top panel starting at the origin ridge regression shrinks the coefficients together until it finally converges to least squares
pls and pcr show similar behavior to ridge although are discrete and more extreme
best subset overshoots the solution and then backtracks
the behavior of the lasso is intermediate to the other methods
when the correlation is negative lower panel again pls and pcr roughly track the ridge path while all of the methods are more similar to one another
it is interesting to compare the shrinkage behavior of these different methods
recall that ridge regression shrinks all directions but shrinks low variance directions more
principal components regression leaves high variance directions alone and discards the rest
interestingly it can be shown that partial least squares also tends to shrink the low variance directions but can actually inflate some of the higher variance directions
this can make pls little unstable and cause it to have slightly higher prediction error compared to ridge regression
full study is given in frank and friedman
these authors conclude that for minimizing prediction error ridge regression is generally preferable to variable subset selection principal components regression and partial least squares
however the improvement over the latter two methods was only slight
to summarize pls pcr and ridge regression tend to behave similarly
ridge regression may be preferred because it shrinks smoothly rather than in discrete steps
lasso falls somewhere between ridge regression and best subset regression and enjoys some of the properties of each
coefficient profiles from different methods for simple problem two inputs with correlation and the true regression coefficients
linear methods for regression multiple outcome shrinkage and selection as noted in section the least squares estimates in multiple output linear model are simply the individual least squares estimates for each of the outputs
to apply selection and shrinkage methods in the multiple output case one could apply univariate technique individually to each outcome or simultaneously to all outcomes
with ridge regression for example we could apply formula to each of the columns of the outcome matrix using possibly different parameters bb or apply it to all columns using the same value of bb
the former strategy would allow different amounts of regularization to be applied to different outcomes but require estimation of separate regularization parameters bb bb while the latter would permit all outputs to be used in estimating the sole regularization parameter bb
other more sophisticated shrinkage and selection strategies that exploit correlations in the different responses can be helpful in the multiple output case
suppose for example that among the outputs we have yk and share the same structural part in their models
it is clear in this case that we should pool our observations on yk and yl to estimate the common
combining responses is at the heart of canonical correlation analysis cca data reduction technique developed for the multiple output case
similar to pca cca finds sequence of uncorrelated linear combinations xvm of the xj and corresponding sequence of uncorrelated linear combinations yum of the responses yk such that the correlations corr yum xvm are successively maximized
note that at most min directions can be found
the leading canonical response variates are those linear combinations derived responses best predicted by the xj in contrast the trailing canonical variates can be poorly predicted by the xj and are candidates for being dropped
the cca solution is computed using generalized svd of the sample cross covariance matrix yt assuming and are centered exercise
reduced rank regression izenman van der merwe and zidek formalizes this approach in terms of regression model that explicitly pools information
given an error covariance cov we solve the following
multiple outcome shrinkage and selection restricted multivariate regression problem rr argmin yi bt xi yi bt xi
rank with replaced by the estimate yt one can show exercise that the solution is given by cca of and rr um where um is the sub matrix of consisting of the first columns and is the matrix of left canonical vectors um
is its generalized inverse
writing the solution as rr xt xt yum we see that reduced rank regression performs linear regression on the pooled response matrix yum and then maps the coefficients and hence the fits as well back to the original response space
the reduced rank fits are given by rr xt xt yum hypm where is the usual linear regression projection operator and pm is the rank cca response projection operator
although better estimate of would be xb xb pk one can show that the solution remains the same exercise
reduced rank regression borrows strength among responses by truncating the cca
breiman and friedman explored with some success shrinkage of the canonical variates between and smooth version of reduced rank regression
their proposal has the form compare where is diagonal shrinkage matrix the stands for curds and whey the name they gave to their procedure
based on optimal prediction in the population setting they show that has diagonal entries bb cm where cm is the mth canonical correlation coefficient
note that as the ratio of the number of input variables to sample size gets small the shrinkage factors approach
breiman and friedman proposed modified versions of based on training data and cross validation but the general form is the same
here the fitted response has the form hysc
linear methods for regression where sc is the response shrinkage operator
breiman and friedman also suggested shrinking in both the space and space
this leads to hybrid shrinkage models of the form ridge bb ysc where bb xt bb xt is the ridge regression shrinkage operator as in on page
their paper and the discussions thereof contain many more details
more on the lasso and related path algorithms since the publication of the lar algorithm efron et al there has been lot of activity in developing algorithms for fitting regularization paths for variety of different problems
in addition regularization has taken on life of its own leading to the development of the field compressed sensing in the signal processing literature
donoho candes
in this section we discuss some related proposals and other path algorithms starting off with precursor to the lar algorithm
incremental forward stagewise regression here we present another lar like algorithm this time focused on forward stagewise regression
interestingly efforts to understand flexible nonlinear regression procedure boosting led to new algorithm for linear models lar
in reading the first edition of this book and the forward stagewise algorithm incremental forward stagewise regression fso
start with the residual equal to and
all the predictors are standardized to have mean zero and unit norm
find the predictor xj most correlated with
update where sign hxj ri and is small step size and set xj
repeat steps and many times until the residuals are uncorrelated with all the predictors
algorithm of chapter our colleague brad efron realized that with in the first edition this was algorithm in chapter
coefficient profiles for the prostate data
the left panel shows incremental forward stagewise regression with step size
the right panel shows the infinitesimal version fs obtained letting
this profile was fit by the modification to the lar algorithm
in this example the fs profiles are monotone and hence identical to those of lasso and lar linear models one could explicitly construct the piecewise linear lasso paths of figure
this led him to propose the lar procedure of section as well as the incremental version of forward stagewise regression presented here
consider the linear regression version of the forward stagewise boosting algorithm proposed in section page
it generates coefficient profile by repeatedly updating by small amount the coefficient of the variable most correlated with the current residuals
algorithm gives the details
figure left panel shows the progress of the algorithm on the prostate data with step size
if hxj ri the least squares coefficient of the residual on jth predictor then this is exactly the usual forward stagewise procedure fs outlined in section
here we are mainly interested in small values of
letting gives the right panel of figure which in this case is identical to the lasso path in figure
we call this limiting procedure infinitesimal forward stagewise regression or fs
this procedure plays an important role in non linear adaptive methods like boosting chapters and and is the version of incremental forward stagewise regression that is most amenable to theoretical analysis
bu hlmann and hothorn refer to the same procedure as boost because of its connections to boosting
linear methods for regression efron originally thought that the lar algorithm was an implementation of fs allowing each tied predictor chance to update their coefficients in balanced way while remaining tied in correlation
however he then realized that the lar least squares fit amongst the tied predictors can result in coefficients moving in the opposite direction to their correlation which cannot happen in algorithm
the following modification of the lar algorithm implements fs algorithm least angle regression fs modification
find the new direction by solving the constrained least squares problem min xa subject to bj sj where sj is the sign of hxj ri
the modification amounts to non negative least squares fit keeping the signs of the coefficients the same as those of the correlations
one can show that this achieves the optimal balancing of infinitesimal update turns for the variables tied for maximal correlation hastie et al
like lasso the entire fs path can be computed very efficiently via the lar algorithm
as consequence of these results if the lar profiles are monotone nonincreasing or non decreasing as they are in figure then all three methods lar lasso and fs give identical profiles
if the profiles are not monotone but do not cross the zero axis then lar and lasso are identical
since fs is different from the lasso it is natural to ask if it optimizes criterion
the answer is more complex than for lasso the fs coefficient profile is the solution to differential equation
while the lasso makes optimal progress in terms of reducing the residual sum of squares per unit increase in norm of the coefficient vector fs is optimal per unit increase in arc length traveled along the coefficient path
hence its coefficient path is discouraged from changing directions too often
fs is more constrained than lasso and in fact can be viewed as monotone version of the lasso see figure on page for dramatic example
fs may be useful in situations where its coefficient profiles are much smoother and hence have less variance than those of lasso
more details on fs are given in section and hastie et al
figure includes fs where its performance is very similar to that of the lasso
more on the lasso and related path algorithms piecewise linear path algorithms the least angle regression procedure exploits the piecewise linear nature of the lasso solution paths
it has led to similar path algorithms for other regularized problems
suppose we solve bb argmin bb with yi xij where both the loss function and the penalty function are convex
then the following are sufficient conditions for the solution path bb to be piecewise linear rosset and zhu
is quadratic or piecewise quadratic as function of and
is piecewise linear in
this also implies in principle that the solution path can be efficiently computed
examples include squaredand absolute error loss huberized losses and the penalties on
another example is the hinge loss function used in the support vector machine
there the loss is piecewise linear and the penalty is quadratic
interestingly this leads to piecewiselinear path algorithm in the dual space more details are given in section
the dantzig selector candes and tao proposed the following criterion min subject to xt
they call the solution the dantzig selector ds
it can be written equivalently as min xt subject to
here denotes the norm the maximum absolute value of the components of the vector
in this form it resembles the lasso replacing squared error loss by the maximum absolute value of its gradient
note that as gets large both procedures yield the least squares solution if
if they both yield the least squares solution with minimum norm
however for smaller values of the ds procedure produces different path of solutions than the lasso
candes and tao show that the solution to ds is linear programming problem hence the name dantzig selector in honor of the late
linear methods for regression george dantzig the inventor of the simplex method for linear programming
they also prove number of interesting mathematical properties for the method related to its ability to recover an underlying sparse coefficient vector
these same properties also hold for the lasso as shown later by bickel et al
unfortunately the operating properties of the ds method are somewhat unsatisfactory
the method seems similar in spirit to the lasso especially when we look at the lasso's stationary conditions
like the lar algorithm the lasso maintains the same inner product and correlation with the current residual for all variables in the active set and moves their coefficients to optimally decrease the residual sum of squares
in the process this common correlation is decreased monotonically exercise and at all times this correlation is larger than that for non active variables
the dantzig selector instead tries to minimize the maximum inner product of the current residual with all the predictors
hence it can achieve smaller maximum than the lasso but in the process curious phenomenon can occur
if the size of the active set is there will be variables tied with maximum correlation
however these need not coincide with the active set
hence it can include variable in the model that has smaller correlation with the current residual than some of the excluded variables efron et al
this seems unreasonable and may be responsible for its sometimes inferior prediction accuracy
efron et al also show that ds can yield extremely erratic coefficient paths as the regularization parameter is varied
the grouped lasso in some problems the predictors belong to pre defined groups for example genes that belong to the same biological pathway or collections of indicator dummy variables for representing the levels of categorical predictor
in this situation it may be desirable to shrink and select the members of group together
the grouped lasso is one way to achieve this
suppose that the predictors are divided into groups with the number in group
for ease of notation we use matrix to represent the predictors corresponding to the th group with corresponding coefficient vector
the grouped lasso minimizes the convex criterion
minp bb ir where the terms accounts for the varying group sizes and is the euclidean norm not squared
since the euclidean norm of vector is zero only if all of its components are zero this procedure encourages sparsity at both the group and individual levels
that is for some values of bb an entire group of predictors may drop out of the model
this procedure
more on the lasso and related path algorithms was proposed by bakin and lin and zhang and studied and generalized by yuan and lin
generalizations include more general norms as well as allowing overlapping groups of predictors zhao et al
there are also connections to methods for fitting sparse additive models lin and zhang ravikumar et al
further properties of the lasso number of authors have studied the ability of the lasso and related procedures to recover the correct model as and grow
examples of this work include knight and fu greenshtein and ritov tropp donoho meinshausen meinshausen and bu hlmann tropp zhao and yu wainwright and bunea et al
for example donoho focuses on the case and considers the lasso solution as the bound gets large
in the limit this gives the solution with minimum norm among all models with zero training error
he shows that under certain assumptions on the model matrix if the true model is sparse this solution identifies the correct predictors with high probability
many of the results in this area assume condition on the model matrix of the form xs xs xs xs for some
here indexes the subset of features with non zero coefficients in the true underlying model and xs are the columns of corresponding to those features
similarly are the features with true coefficients equal to zero and xs the corresponding columns
this says that the least squares coefficients for the columns of xs on xs are not too large that is the good variables are not too highly correlated with the nuisance variables
regarding the coefficients themselves the lasso shrinkage causes the estimates of the non zero coefficients to be biased towards zero and in general they are not consistent
one approach for reducing this bias is to run the lasso to identify the set of non zero coefficients and then fit an unrestricted linear model to the selected set of features
this is not always feasible if the selected set is large
alternatively one can use the lasso to select the set of non zero predictors and then apply the lasso again but using only the selected predictors from the first step
this is known as the relaxed lasso meinshausen
the idea is to use cross validation to estimate the initial penalty parameter for the lasso and then again for second penalty parameter applied to the selected set of predictors
since statistical consistency means as the sample size grows the estimates converge to the true values
linear methods for regression the variables in the second step have less competition from noise variables cross validation will tend to pick smaller value for bb and hence their coefficients will be shrunken less than those in the initial estimate
alternatively one can modify the lasso penalty function so that larger coefficients are shrunken less severely the smoothly clipped absolute deviation scad penalty of fan and li replaces bb by ja bb where dja bb bb bb sign bb bb bb for some
the second term in square braces reduces the amount of shrinkage in the lasso for larger values of with ultimately no shrinkage as
the lasso and two alternative non convex penalties designed to penalize large coefficients less
for scad we use bb and and bd in the last panel
bd
however this criterion is non convex which is drawback since it makes the computation much more difficult
the adaptive lasso zou pp uses weighted penalty of the form wj where wj bd is the ordinary least squares estimate and bd
this is practical approximation to the penalties bd here discussed in section
the adaptive lasso yields consistent estimates of the parameters while retaining the attractive convexity property of the lasso
pathwise coordinate optimization an alternate approach to the lars algorithm for computing the lasso solution is simple coordinate descent
this idea was proposed by fu and daubechies et al and later studied and generalized by friedman et al
wu and lange and others
the idea is to fix the penalty parameter bb in the lagrangian form and optimize successively over each parameter holding the other parameters fixed at their current values
suppose the predictors are all standardized to have mean zero and unit norm
denote by bb the current estimate for at penalty parameter
computational considerations bb
we can rearrange to isolate
bb yi xik bb xij bb bb bb where we have suppressed the intercept and introduced factor for convenience
this can be viewed as univariate lasso problem with response variable the partial residual yi yi xik bb
this has an explicit solution resulting in the update an
bb xij yi bb
here bb sign bb is the soft thresholding operator in table on page
the first argument to is the simple least squares coefficient of the partial residual on the standardized variable xij
repeated iteration of cycling through each variable in turn until convergence yields the lasso estimate bb
we can also use this simple algorithm to efficiently compute the lasso solutions at grid of values of bb
we start with the smallest value bb max for which bb max decrease it little and cycle through the variables until convergence
then bb is decreased again and the process is repeated using the previous solution as warm start for the new value of bb
this can be faster than the lars algorithm especially in large problems
key to its speed is the fact that the quantities in can be updated quickly as varies and often the update is to leave
on the other hand it delivers solutions over grid of bb values rather than the entire solution path
the same kind of algorithm can be applied to the elastic net the grouped lasso and many other models in which the penalty is sum of functions of the individual parameters friedman et al
it can also be applied with some substantial modifications to the fused lasso section details are in friedman et al
computational considerations least squares fitting is usually done via the cholesky decomposition of the matrix xt or qr decomposition of
with observations and features the cholesky decomposition requires operations while the qr decomposition requires operations
depending on the relative size of and the cholesky can sometimes be faster on the other hand it can be less numerically stable lawson and hansen
computation of the lasso via the lar algorithm has the same order of computation as least squares fit
linear methods for regression bibliographic notes linear regression is discussed in many statistics books for example seber weisberg and mardia et al
ridge regression was introduced by hoerl and kennard while the lasso was proposed by tibshirani
around the same time lasso type penalties were proposed in the basis pursuit method for signal processing chen et al
the least angle regression procedure was proposed in efron et al related to this is the earlier homotopy procedure of osborne et al and osborne et al
their algorithm also exploits the piecewise linearity used in the lar lasso algorithm but lacks its transparency
the criterion for the forward stagewise criterion is discussed in hastie et al
park and hastie develop path algorithm similar to least angle regression for generalized regression models
partial least squares was introduced by wold
comparisons of shrinkage methods may be found in copas and frank and friedman
exercises ex
show that the statistic for dropping single coefficient from model is equal to the square of the corresponding score
given data on two variables and consider fitting cubic polynomial regression model
in addition to plotting the fitted curve you would like confidence band about the curve
consider the following two approaches form confidence interval for the linear func
at each point tion at xj
form confidence set for as in which in turn generates confidence intervals for
how do these approaches differ
which band is likely to be wider
conduct small simulation experiment to compare the two methods
gauss markov theorem prove the gauss markov theorem the least squares estimate of parameter at has variance no bigger than that of any other linear unbiased estimate of at section the matrix inequality holds if is positive semidefinite
show that if is the variance covariance matrix of the least squares estimate of and is the variance covariance matrix of any other linear unbiased estimate then
exercises ex
show how the vector of least squares coefficients can be obtained from single pass of the gram schmidt procedure algorithm
represent your solution in terms of the qr decomposition of
consider the ridge regression problem
show that this problem is equivalent to the problem argmin yi xij bb
give the correspondence between and the original in
characterize the solution to this modified criterion
show that similar result holds for the lasso
show that the ridge regression estimate is the mean and mode of the posterior distribution under gaussian prior and gaussian sampling model
find the relationship between the regularization parameter bb in the ridge formula and the variances and
assume yi xti and the parameters are each distributed as independently of one another
assuming and are known show that the minus log posterior density of is pn pp proportional to yi xij bb where bb
consider the qr decomposition of the uncentered matrix whose first column is all ones and the svd of the centered matrix
show that and span the same subspace where is the sub matrix of with the first column removed
under what circumstances will they be the same up to sign flips
forward stepwise regression
suppose we have the qr decomposition for the matrix in multiple regression problem with response and we have an additional predictors in the matrix
denote the current residual by
we wish to establish which one of these additional variables will reduce the residual sum of squares the most when included with those in
describe an efficient procedure for doing this
backward stepwise regression
suppose we have the multiple regression fit of on along with the standard errors and scores as in table
we wish to establish which variable when dropped will increase the residual sum of squares the least
how would you do this
show that the solution to the multivariate linear regression problem is given by
what happens if the covariance matrices are different for each observation
linear methods for regression ex
show that the ridge regression estimates can be obtained by ordinary least squares regression on an augmentedv data set
we augment the centered matrix with additional rows bb and augment with zeros
by introducing artificial data having response value zero the fitting procedure is forced to shrink the coefficients toward zero
this is related to the idea of hints due to abu mostafa where model constraints are implemented by adding artificial data examples that satisfy them
derive the expression and show that pcr ls
show that in the orthogonal case pls stops after steps because subsequent mj in step in algorithm are zero
verify expression and hence show that the partial least squares directions are compromise between the ordinary regression coefficient and the principal component directions
derive the entries in table the explicit forms for estimators in the orthogonal case
repeat the analysis of table on the spam data discussed in chapter
read about conjugate gradient algorithms murray et al for example and establish connection between these algorithms and partial least squares
show that ridge increases as its tuning parameter bb
does the same property hold for the lasso and partial least squares estimates
for the latter consider the tuning parameter to be the successive steps in the algorithm
consider the canonical correlation problem
show that the leading pair of canonical variates and solve the problem max ut yt ut vt xt generalized svd problem
show that the solution is given by yt and xt where and are the leading left and right singular vectors in yt yt xt
show that the entire sequence um vm min is also given by
show that the solution to the reduced rank regression problem with estimated by yt is given by
hint transform
exercises to and solved in terms of the canonical vectors
show that um and generalized inverse is um
show that the solution in exercise does not change if is estimated by the more natural quantity xb xb pk
consider regression problem with all variables and response having mean zero and standard deviation one
suppose also that each variable has identical absolute correlation with the response hxj yi bb
let be the least squares coefficient of on and let for be the vector that moves fraction toward the least squares fit
let rss be the residual sum of squares from the full least squares fit show that hxj bb and hence the correlations of each xj with the residuals remain equal in magnitude as we progress toward show that these correlations are all equal to bb bb rss and hence they decrease monotonically to zero use these results to show that the lar algorithm in section keeps the correlations tied and monotonically decreasing as claimed in
lar directions
using the notation around equation on page show that the lar direction makes an equal angle with each of the predictors in ak
lar look ahead efron et al sec
starting at the beginning of the kth step of the lar algorithm derive expressions to identify the next variable to enter the active set at step and the value of at which this occurs using the notation around equation on page
forward stepwise regression enters the variable at each step that most reduces the residual sum of squares
lar adjusts variables that have the most absolute correlation with the current residuals
show that these two entry criteria are not necessarily the same
hint let xj be the jth
linear methods for regression variable linearly adjusted for all the variables currently in the model
show that the first criterion amounts to identifying the for which cor xj is largest in magnitude
consider ex
lasso and lar the lasso problem in lagrange multiplier form with yi xij we minimize bb for fixed bb setting with expression becomes bb
show that the lagrange dual function is bb bb bb and the karush kuhn tucker optimality conditions are bb bb bb bb bb bb along with the non negativity constraints on the parameters and all the lagrange multipliers show that bb and that the kkt conditions imply one of the following three scenarios bb bb bb bb bb bb bb
hence show that for any active predictor having we must have bb if and bb if
assuming the predictors are standardized relate bb to the correlation between the jth predictor and the current residuals suppose that the set of active predictors is unchanged for bb bb bb
show that there is vector such that bb bb bb bb thus the lasso solution path is linear as bb ranges from bb to bb efron et al rosset and zhu
exercises ex
suppose for given in the fitted lasso coefficient for variable xj is
suppose we augment our set of variables with an identical copy xj xj
characterize the effect of this exact collinearity by describing the set of solutions for and using the same value of
suppose we run ridge regression with parameter bb on single variable and get coefficient
we now include an exact copy and refit our ridge regression
show that both coefficients are identical and derive their value
show in general that if copies of variable xj are included in ridge regression their coefficients are all the same
consider the elastic net optimization problem min bb
show how one can turn this into lasso problem using an augmented version of and
linear methods for regression
this is page printer opaque this linear methods for classification introduction in this chapter we revisit the classification problem and focus on linear methods for classification
since our predictor takes values in discrete set we can always divide the input space into collection of regions labeled according to the classification
we saw in chapter that the boundaries of these regions can be rough or smooth depending on the prediction function
for an important class of procedures these decision boundaries are linear this is what we will mean by linear methods for classification
there are several different ways in which linear decision boundaries can be found
in chapter we fit linear regression models to the class indicator variables and classify to the largest fit
suppose there are classes for convenience labeled and the fitted linear model for the kth indicator response variable is kt
the decision boundary between class and is that set of points for which that is the set an affine set or hyperplane since the same is true for any pair of classes the input space is divided into regions of constant classification with piecewise hyperplanar decision boundaries
this regression approach is member of class of methods that model discriminant functions for each class and then classify to the class with the largest value for its discriminant function
methods strictly speaking hyperplane passes through the origin while an affine set need not
we sometimes ignore the distinction and refer in general to hyperplanes
linear methods for classification that model the posterior probabilities pr are also in this class
clearly if either the or pr are linear in then the decision boundaries will be linear
actually all we require is that some monotone transformation of or pr be linear for the decision boundaries to be linear
for example if there are two classes popular model for the posterior probabilities is exp pr exp pr exp here the monotone transformation is the logit transformation log and in fact we see that pr log
pr the decision boundary is the set of points for which aa the log odds are zero and this is hyperplane defined by
we discuss two very popular but different methods that result in linear log odds or logits linear discriminant analysis and linear logistic regression
although they differ in their derivation the essential difference between them is in the way the linear function is fit to the training data
more direct approach is to explicitly model the boundaries between the classes as linear
for two class problem in dimensional input space this amounts to modeling the decision boundary as hyperplane in other words normal vector and cut point
we will look at two methods that explicitly look for separating hyperplanes
the first is the wellknown perceptron model of rosenblatt with an algorithm that finds separating hyperplane in the training data if one exists
the second method due to vapnik finds an optimally separating hyperplane if one exists else finds hyperplane that minimizes some measure of overlap in the training data
we treat the separable case here and defer treatment of the nonseparable case to chapter
while this entire chapter is devoted to linear decision boundaries there is considerable scope for generalization
for example we can expand our variable set xp by including their squares and cross products thereby adding additional variables
linear functions in the augmented space map down to quadratic functions in the original space hence linear decision boundaries to quadratic decision boundaries
figure illustrates the idea
the data are the same the left plot uses linear decision boundaries in the two dimensional space shown while the right plot uses linear decision boundaries in the augmented five dimensional space described above
this approach can be used with any basis transfor
the left plot shows some data from three classes with linear decision boundaries found by linear discriminant analysis
the right plot shows quadratic decision boundaries
these were obtained by finding linear boundaries in the five dimensional space
linear inequalities in this space are quadratic inequalities in the original space mation where irp irq with and will be explored in later chapters
linear regression of an indicator matrix here each of the response categories are coded via an indicator variable
thus if has classes there will be such indicators yk with yk if else
these are collected together in vector yk and the training instances of these form an indicator response matrix
is matrix of and with each row having single
we fit linear regression model to each of the columns of simultaneously and the fit is given by xt xt
chapter has more details on linear regression
note that we have coefficient vector for each response column yk and hence coefficient matrix xt xt
here is the model matrix with columns corresponding to the inputs and leading column of for the intercept
linear methods for classification what is the rationale for this approach
one rather formal justification is to view the regression as an estimate of conditional expectation
for the random variable yk yk pr so conditional expectation of each of the yk seems sensible goal
the real issue is how good an approximation to conditional expectation is the rather rigid linear regression model
alternatively are the reasonable estimates of the posterior probabilities pr and more importantly does this matter
it is quite straightforward to verify that for any as long as there is an intercept in the model column of in
however the can be negative or greater than and typically some are
this is consequence of the rigid nature of linear regression especially if we make predictions outside the hull of the training data
these violations in themselves do not guarantee that this approach will not work and in fact on many problems it gives similar results to more standard linear methods for classification
if we allow linear regression onto basis expansions of the inputs this approach can lead to consistent estimates of the probabilities
as the size of the training set grows bigger we adaptively include more basis elements so that linear regression onto these basis functions approaches conditional expectation
we discuss such approaches in chapter
more simplistic viewpoint is to construct targets tk for each class where tk is the kth column of the identity matrix
our prediction problem is to try and reproduce the appropriate target for an observation
with the same coding as before the response vector yi ith row of for observation has the value yi tk if gi
we might then fit the linear model by least squares min yi xti
the criterion is sum of squared euclidean distances of the fitted vectors from their targets
new observation is classified by computing its fitted vector and classifying to the closest target argmin tk
since squared norm is itself sum of squares the components decouple and can be rearranged as separate linear model for each element
note that this is only possible because there is nothing in the model that binds the different responses together
the data come from three classes in ir and are easily separated by linear decision boundaries
the right plot shows the boundaries found by linear discriminant analysis
the left plot shows the boundaries found by linear regression of the indicator response variables
the middle class is completely masked never dominates
there is serious problem with the regression approach when the number of classes especially prevalent when is large
because of the rigid nature of the regression model classes can be masked by others
figure illustrates an extreme situation when
the three classes are perfectly separated by linear decision boundaries yet linear regression misses the middle class completely
in figure we have projected the data onto the line joining the three centroids there is no information in the orthogonal direction in this case and we have included and coded the three response variables and
the three regression lines left panel are included and we see that the line corresponding to the middle class is horizontal and its fitted values are never dominant
thus observations from class are classified either as class or class
the right panel uses quadratic regression rather than linear regression
for this simple example quadratic rather than linear fit for the middle class at least would solve the problem
however it can be seen that if there were four rather than three classes lined up like this quadratic would not come down fast enough and cubic would be needed as well
loose but general rule is that if classes are lined up polynomial terms up to degree might be needed to resolve them
note also that these are polynomials along the derived direction
the effects of masking on linear regression in ir for three class problem
the rug plot at the base indicates the positions and class membership of each observation
the three curves in each panel are the fitted regressions to the three class indicator variables for example for the blue class yblue is for the blue observations and for the green and orange
the fits are linear and quadratic polynomials
above each plot is the training error rate
the bayes error rate is for this problem as is the lda error rate passing through the centroids which can have arbitrary orientation
so in dimensional input space one would need general polynomial terms and cross products of total degree pk terms in all to resolve such worst case scenarios
the example is extreme but for large and small such maskings naturally occur
as more realistic illustration figure is projection of the training data for vowel recognition problem onto an informative two dimensional subspace
there are classes in dimensions
this is difficult classification problem and the best methods achieve around errors on the test data
the main point here is summarized in table linear regression has an error rate of while close relative linear discriminant analysis has an error rate of
it seems that masking has hurt in this case
while all the other methods in this chapter are based on linear functions of as well they use them in such way that avoids this masking problem
linear discriminant analysis decision theory for classification section tells us that we need to know the class posteriors pr for optimal classification
suppose fk is the class conditional density of in class and let be the prior probability of class with
simple application of bayes
two dimensional plot of the vowel training data
there are eleven classes with ir and this is the best view in terms of lda model section
the heavy circles are the projected mean vectors for each class
the class overlap is considerable
training and test error rates using variety of linear techniques on the vowel data
there are eleven classes in ten dimensions of which three account for of the variance via principal components analysis
we see that linear regression is hurt by masking increasing the test and training error by over
technique error rates training test linear regression linear discriminant analysis quadratic discriminant analysis logistic regression
linear methods for classification theorem gives us fk pr pk
we see that in terms of ability to classify having the fk is almost equivalent to having the quantity pr
suppose that we model each class density as multivariate gaussian fk k k
linear discriminant analysis lda arises in the special case when we assume that the classes have common covariance matrix
in comparing two classes and it is sufficient to look at the log ratio and we see that pr fk log log log pr log k k xt k an equation linear in
the equal covariance matrices cause the normalization factors to cancel as well as the quadratic part in the exponents
this linear log odds function implies that the decision boundary between classes and the set where pr pr is linear in in dimensions hyperplane
this is of course true for any pair of classes so all the decision boundaries are linear
if we divide irp into regions that are classified as class class etc these regions will be separated by hyperplanes
figure left panel shows an idealized example with three classes and
here the data do arise from three gaussian distributions with common covariance matrix
we have included in
the left panel shows three gaussian distributions with the same covariance and different means
included are the contours of constant density enclosing of the probability in each case
the bayes decision boundaries between each pair of classes are shown broken straight lines and the bayes decision boundaries separating all three classes are the thicker solid lines subset of the former
on the right we see sample of drawn from each gaussian distribution and the fitted lda decision boundaries the figure the contours corresponding to highest probability density as well as the class centroids
notice that the decision boundaries are not the perpendicular bisectors of the line segments joining the centroids
this would be the case if the covariance were spherical and the class priors were equal
from we see that the linear discriminant functions xt k tk k log are an equivalent description of the decision rule with argmaxk
figure right panel shows the estimated decision boundaries based on sample of size each from three gaussian distributions
figure on page is another example but here the classes are not gaussian
with two classes there is simple correspondence between linear discriminant analysis and classification by linear least squares as in
the lda rule classifies to class if xt log log
linear methods for classification and class otherwise
suppose we code the targets in the two classes as and respectively
it is easy to show that the coefficient vector from least squares is proportional to the lda direction given in exercise
in fact this correspondence occurs for any distinct coding of the targets see exercise
however unless the intercepts are different and hence the resulting decision rules are different
since this derivation of the lda direction via least squares does not use gaussian assumption for the features its applicability extends beyond the realm of gaussian data
however the derivation of the particular intercept or cut point given in does require gaussian data
thus it makes sense to instead choose the cut point that empirically minimizes training error for given dataset
this is something we have found to work well in practice but have not seen it mentioned in the literature
with more than two classes lda is not the same as linear regression of the class indicator matrix and it avoids the masking problems associated with that approach hastie et al
correspondence between regression and lda can be established through the notion of optimal scoring discussed in section
getting back to the general discriminant problem if the are not assumed to be equal then the convenient cancellations in do not occur in particular the pieces quadratic in remain
we then get quadratic discriminant functions qda log k k log
the decision boundary between each pair of classes and is described by quadratic equation
figure shows an example from figure on page where the three classes are gaussian mixtures section and the decision boundaries are approximated by quadratic equations in
here we illustrate two popular ways of fitting these quadratic boundaries
the right plot uses qda as described here while the left plot uses lda in the enlarged five dimensional quadratic polynomial space
the differences are generally small qda is the preferred approach with the lda method convenient substitute
the estimates for qda are similar to those for lda except that separate covariance matrices must be estimated for each class
when is large this can mean dramatic increase in parameters
since the decision boundaries are functions of the parameters of the densities counting the number of parameters must be done with care
for lda it seems there are parameters since we only need the differences for this figure and many similar figures in the book we compute the decision bound aries by an exhaustive contouring method
we compute the decision rule on fine lattice of points and then use contouring algorithms to compute the boundaries
two methods for fitting quadratic boundaries
the left plot shows the quadratic decision boundaries for the data in figure obtained using lda in the five dimensional space
the right plot shows the quadratic decision boundaries found by qda
the differences are small as is usually the case between the discriminant functions where is some pre chosen class here we have chosen the last and each difference requires parameters
likewise for qda there will be parameters
both lda and qda perform well on an amazingly large and diverse set of classification tasks
for example in the statlog project michie et al lda was among the top three classifiers for of the datasets qda among the top three for four datasets and one of the pair were in the top three for datasets
both techniques are widely used and entire books are devoted to lda
it seems that whatever exotic tools are the rage of the day we should always have available these two simple tools
the question arises why lda and qda have such good track record
the reason is not likely to be that the data are approximately gaussian and in addition for lda that the covariances are approximately equal
more likely reason is that the data can only support simple decision boundaries such as linear or quadratic and the estimates provided via the gaussian models are stable
this is bias variance tradeoff we can put up with the bias of linear decision boundary because it can be estimated with much lower variance than more exotic alternatives
this argument is less believable for qda since it can have many parameters itself although perhaps fewer than the non parametric alternatives
although we fit the covariance matrix to compute the lda discriminant functions much reduced function of it is all that is required to estimate the parameters needed to compute the decision boundaries
test and training errors for the vowel data using regularized discriminant analysis with series of values of
the optimum for the test data occurs around close to quadratic discriminant analysis
regularized discriminant analysis friedman proposed compromise between lda and qda which allows one to shrink the separate covariances of qda toward common covariance as in lda
these methods are very similar in flavor to ridge regression
the regularized covariance matrices have the form where is the pooled covariance matrix as used in lda
here allows continuum of models between lda and qda and needs to be specified
in practice can be chosen based on the performance of the model on validation data or by cross validation
figure shows the results of rda applied to the vowel data
both the training and test error improve with increasing although the test error increases sharply after
the large discrepancy between the training and test error is partly due to the fact that there are many repeat measurements on small number of individuals different in the training and test set
similar modifications allow itself to be shrunk toward the scalar covariance for
replacing in by leads to more general family of covariances indexed by pair of parameters
in chapter we discuss other regularized versions of lda which are more suitable when the data arise from digitized analog signals and images
linear discriminant analysis in these situations the features are high dimensional and correlated and the lda coefficients can be regularized to be smooth or sparse in the original domain of the signal
this leads to better generalization and allows for easier interpretation of the coefficients
in chapter we also deal with very high dimensional problems where for example the features are geneexpression measurements in microarray studies
there the methods focus on the case in and other severely regularized versions of lda
computations for lda as lead in to the next topic we briefly digress on the computations required for lda and especially qda
their computations are simplified by diagonalizing or
for the latter suppose we compute the eigendecomposition for each uk dk utk where uk is orthonormal and dk diagonal matrix of positive eigenvalues dk
the common covariance estimate of will now be the identity
reduced rank linear discriminant analysis so far we have discussed lda as restricted gaussian classifier
part of its popularity is due to an additional restriction that allows us to view informative low dimensional projections of the data
the centroids in dimensional input space lie in an affine subspace of dimension and if is much larger than this will be considerable drop in dimension
moreover in locating the closest centroid we can ignore distances orthogonal to this subspace since they will contribute equally to each class
thus we might just as well project the onto this centroid spanning subspace hk and make distance comparisons there
thus there is fundamental dimension reduction in lda namely that we need only consider the data in subspace of dimension at most
linear methods for classification if for instance this could allow us to view the data in twodimensional plot color coding the classes
in doing so we would not have relinquished any of the information needed for lda classification
what if
we might then ask for dimensional subspace hl hk optimal for lda in some sense
fisher defined optimal to mean that the projected centroids were spread out as much as possible in terms of variance
this amounts to finding principal component subspaces of the centroids themselves principal components are described briefly in section and in more detail in section
figure shows such an optimal two dimensional subspace for the vowel data
here there are eleven classes each different vowel sound in ten dimensional input space
the centroids require the full space in this case since but we have shown an optimal two dimensional subspace
the dimensions are ordered so we can compute additional dimensions in sequence
figure shows four additional pairs of coordinates also known as canonical or discriminant variables
the columns of in sequence from first to last define the coordinates of the optimal subspaces
combining all these operations the th discriminant variable is given by with
fisher arrived at this decomposition via different route without referring to gaussian distributions at all
he posed the problem find the linear combination at such that the betweenclass variance is maximized relative to the within class variance
again the between class variance is the variance of the class means of and the within class variance is the pooled variance about the means
figure shows why this criterion makes sense
although the direction joining the centroids separates the means as much as possible maximizes the between class variance there is considerable overlap between the projected classes due to the nature of the covariances
by taking the covariance into account as well direction with minimum overlap can be found
the between class variance of is at ba and the within class variance at wa where is defined earlier and is the covariance matrix of the class centroid matrix
note that where is the total covariance matrix of ignoring class information
four projections onto pairs of canonical variates
notice that as the rank of the canonical variates increases the centroids become less spread out
in the lower right panel they appear to be superimposed and the classes most confused
although the line joining the centroids defines the direction of greatest centroid spread the projected data overlap because of the covariance left panel
the discriminant direction minimizes this overlap for gaussian data right panel
fisher's problem therefore amounts to maximizing the rayleigh quotient at ba max at wa or equivalently max at ba subject to at wa
this is generalized eigenvalue problem with given by the largest eigenvalue of
it is not hard to show exercise that the optimal is identical to defined above
similarly one can find the next direction orthogonal in to such that at ba at wa is maximized the solution is and so on
the are referred to as discriminant coordinates not to be confused with discriminant functions
they are also referred to as canonical variates since an alternative derivation of these results is through canonical correlation analysis of the indicator response matrix on the predictor matrix
this line is pursued in section
classification can be achieved by sphering the data with respect to and classifying to the closest centroid modulo log in the sphered space
this decomposition is identical to the decomposition due to fisher
training and test error rates for the vowel data as function of the dimension of the discriminant subspace
in this case the best error rate is for dimension
figure shows the decision boundaries in this space
the reduced subspaces have been motivated as data reduction for viewing tool
can they also be used for classification and what is the rationale
clearly they can as in our original derivation we simply limit the distance to centroid calculations to the chosen subspace
one can show that this is gaussian classification rule with the additional restriction that the centroids of the gaussians lie in dimensional subspace of irp
fitting such model by maximum likelihood and then constructing the posterior probabilities using bayes theorem amounts to the classification rule described above exercise
gaussian classification dictates the log correction factor in the distance calculation
the reason for this correction can be seen in figure
the misclassification rate is based on the area of overlap between the two densities
if the are equal implicit in that figure then the optimal cut point is midway between the projected means
if the are not equal moving the cut point toward the smaller class will improve the error rate
as mentioned earlier for two classes one can derive the linear rule using lda or any other method and then choose the cut point to minimize misclassification error over the training data
as an example of the benefit of the reduced rank restriction we return to the vowel data
there are classes and variables and hence possible dimensions for the classifier
we can compute the training and test error in each of these hierarchical subspaces figure shows the results
figure shows the decision boundaries for the classifier based on the two dimensional lda solution
there is close connection between fisher's reduced rank discriminant analysis and regression of an indicator response matrix
it turns out that
decision boundaries for the vowel training data in the two dimensional subspace spanned by the first two canonical variates
note that in any higher dimensional subspace the decision boundaries are higher dimensional affine planes and could not be represented as lines
logistic regression lda amounts to the regression followed by an eigen decomposition of
in the case of two classes there is single discriminant variable that is identical up to scalar multiplication to either of the columns of
these connections are developed in chapter
related fact is that if one transforms the original predictors to then lda using is identical to lda in the original space exercise
logistic regression the logistic regression model arises from the desire to model the posterior probabilities of the classes via linear functions in while at the same time ensuring that they sum to one and remain in
the model has the form pr log pr pr log pr
pr log
pr the model is specified in terms of log odds or logit transformations reflecting the constraint that the probabilities sum to one
although the model uses the last class as the denominator in the odds ratios the choice of denominator is arbitrary in that the estimates are equivariant under this choice
simple calculation shows that exp kt pr pk exp pr pk exp and they clearly sum to one
to emphasize the dependence on the entire pat rameter set we denote the probabilities pr pk
when this model is especially simple since there is only single linear function
it is widely used in biostatistical applications where binary responses two classes occur quite frequently
for example patients survive or die have heart disease or not or condition is present or absent
linear methods for classification fitting logistic regression models logistic regression models are usually fit by maximum likelihood using the conditional likelihood of given
since pr completely specifies the conditional distribution the multinomial distribution is appropriate
the log likelihood for observations is log pgi xi where pk xi pr xi
we discuss in detail the two class case since the algorithms simplify considerably
it is convenient to code the two class gi via response yi where yi when gi and yi when gi
let and
the log likelihood can be written yi log xi yi log xi yi xi log xi
here and we assume that the vector of inputs xi includes the constant term to accommodate the intercept
to maximize the log likelihood we set its derivatives to zero
these score equations are xi yi xi which are equations nonlinear in
notice that first composince thep pn nent of xi is the first score equation specifies that yi xi the expected number of class ones matches the observed number and hence also class twos
to solve the score equations we use the newton raphson algorithm which requires the second derivative or hessian matrix xn xi xi xi xi
starting with old single newton update is new old where the derivatives are evaluated at old
logistic regression it is convenient to write the score and hessian in matrix notation
let denote the vector of yi values the matrix of xi values the vector of fitted probabilities with ith element xi old and diagonal matrix of weights with ith diagonal element xi old xi old
then we have xt xt wx the newton step is thus new old xt wx xt xt wx xt old xt wx xt wz
in the second and third line we have re expressed the newton step as weighted least squares step with the response old sometimes known as the adjusted response
these equations get solved repeatedly since at each iteration changes and hence so does and
this algorithm is referred to as iteratively reweighted least squares or irls since each iteration solves the weighted least squares problem new arg min
it seems that is good starting value for the iterative procedure although convergence is never guaranteed
typically the algorithm does converge since the log likelihood is concave but overshooting can occur
in the rare cases that the log likelihood decreases step size halving will guarantee convergence
for the multiclass case the newton algorithm can also be expressed as an iteratively reweighted least squares algorithm but with vector of responses and nondiagonal weight matrix per observation
the latter precludes any simplified algorithms and in this case it is numerically more convenient to work with the expanded vector directly exercise
alternatively coordinate descent methods section can be used to maximize the log likelihood efficiently
the package glmnet friedman et al can fit very large logistic regression problems efficiently both in and
although designed to fit regularized models options allow for unregularized fits
logistic regression models are used mostly as data analysis and inference tool where the goal is to understand the role of the input variables
linear methods for classification table
results from logistic regression fit to the south african heart disease data
coefficient std
error score intercept sbp tobacco ldl famhist obesity alcohol age in explaining the outcome
typically many models are fit in search for parsimonious model involving subset of the variables possibly with some interactions terms
the following example illustrates some of the issues involved
example south african heart disease here we present an analysis of binary data to illustrate the traditional statistical use of the logistic regression model
the data in figure are subset of the coronary risk factor study coris baseline survey carried out in three rural areas of the western cape south africa rousseauw et al
the aim of the study was to establish the intensity of ischemic heart disease risk factors in that high incidence region
the data represent white males between and and the response variable is the presence or absence of myocardial infarction mi at the time of the survey the overall prevalence of mi was in this region
there are cases in our data set and sample of controls
these data are described in more detail in hastie and tibshirani
we fit logistic regression model by maximum likelihood giving the results shown in table
this summary includes scores for each of the coefficients in the model coefficients divided by their standard errors nonsignificant score suggests coefficient can be dropped from the model
each of these correspond formally to test of the null hypothesis that the coefficient in question is zero while all the others are not also known as the wald test
score greater than approximately in absolute value is significant at the level
there are some surprises in this table of coefficients which must be interpreted with caution
systolic blood pressure sbp is not significant
nor is obesity and its sign is negative
this confusion is result of the correlation between the set of predictors
on their own both sbp and obesity are significant and with positive sign
however in the presence of many
scatterplot matrix of the south african heart disease data
each plot shows pair of risk factors and the cases and controls are color coded red is case
the variable family history of heart disease famhist is binary yes or no
linear methods for classification table
results from stepwise logistic regression fit to south african heart disease data
coefficient std
error score intercept tobacco ldl famhist age other correlated variables they are no longer needed and can even get negative sign
at this stage the analyst might do some model selection find subset of the variables that are sufficient for explaining their joint effect on the prevalence of chd
one way to proceed by is to drop the least significant coefficient and refit the model
this is done repeatedly until no further terms can be dropped from the model
this gave the model shown in table
better but more time consuming strategy is to refit each of the models with one variable removed and then perform an analysis of deviance to decide which variable to exclude
the residual deviance of fitted model is minus twice its log likelihood and the deviance between two models is the difference of their individual residual deviances in analogy to sums ofsquares
this strategy gave the same final model as above
how does one interpret coefficient of std
error for tobacco for example
tobacco is measured in total lifetime usage in kilograms with median of kg for the controls and kg for the cases
thus an increase of kg in lifetime tobacco usage accounts for an increase in the odds of coronary heart disease of exp or
incorporating the standard error we get an approximate confidence interval of exp
we return to these data in chapter where we see that some of the variables have nonlinear effects and when modeled appropriately are not excluded from the model
quadratic approximations and inference the maximum likelihood parameter estimates satisfy self consistency relationship they are the coefficients of weighted least squares fit where the responses are yi zi xti
logistic regression and the weights are wi both depending on itself
this and other asymptotics can be derived directly from the weighted least squares fit by mimicking normal theory inference
popular shortcuts are the rao score test which tests for inclusion of term and the wald test which can be used to test for exclusion of term
neither of these require iterative fitting and are based on the maximum likelihood fit of the current model
it turns out that both of these amount to adding or dropping term from the weighted least squares fit using the same weights
such computations can be done efficiently without recomputing the entire weighted least squares fit
software implementations can take advantage of these connections
for example the generalized linear modeling software in which includes logistic regression as part of the binomial family of models exploits them fully
glm generalized linear model objects can be treated as linear model objects and all the tools available for linear models can be applied automatically
regularized logistic regression the penalty used in the lasso section can be used for variable selection and shrinkage with any linear regression model
for logistic regression we would maximize penalized version of fc xn fd max yi xi log xi bb
fe as with the lasso we typically do not penalize the intercept term and standardize the predictors for the penalty to be meaningful
criterion is
linear methods for classification concave and solution can be found using nonlinear programming methods koh et al for example
alternatively using the same quadratic approximations that were used in the newton algorithm in section we can solve by repeated application of weighted lasso algorithm
interestingly the score equations see for the variables with non zero coefficients have the form xtj bb sign which generalizes in section the active variables are tied in their generalized correlation with the residuals
path algorithms such as lar for lasso are more difficult because the coefficient profiles are piecewise smooth rather than linear
regularized logistic regression coefficients for the south african heart disease data plotted as function of the norm
the variables were all standardized to have unit variance
the profiles are computed exactly at each of the plotted points
figure shows the regularization path for the south african heart disease data of section
this was produced using the package glmpath park and hastie which uses predictor corrector methods of convex optimization to identify the exact values of bb at which the active set of non zero coefficients changes vertical lines in the figure
here the profiles look almost linear in other examples the curvature will be more visible
coordinate descent methods section are very efficient for computing the coefficient profiles on grid of values for bb
the package glmnet
logistic regression friedman et al can fit coefficient paths for very large logistic regression problems efficiently large in or
their algorithms can exploit sparsity in the predictor matrix which allows for even larger problems
see section for more details and discussion of regularized multinomial models
logistic regression or lda
in section we find that the log posterior odds between class and are linear functions of pr log log k k k k pr xt k k kt
this linearity is consequence of the gaussian assumption for the class densities as well as the assumption of common covariance matrix
the linear logistic model by construction has linear logits pr log kt
pr it seems that the models are the same
although they have exactly the same form the difference lies in the way the linear coefficients are estimated
the logistic regression model is more general in that it makes less assumptions
we can write the joint density of and as pr pr pr where pr denotes the marginal density of the inputs
for both lda and logistic regression the second term on the right has the logit linear form pr pk where we have again arbitrarily chosen the last class as the reference
the logistic regression model leaves the marginal density of as an arbitrary density function pr and fits the parameters of pr by maximizing the conditional likelihood the multinomial likelihood with probabilities the pr
although pr is totally ignored we can think of this marginal density as being estimated in fully nonparametric and unrestricted fashion using the empirical distribution function which places mass at each observation
with lda we fit the parameters by maximizing the full log likelihood based on the joint density pr k
linear methods for classification where is the gaussian density function
standard normal theory leads easily to the estimates and given in section
since the linear parameters of the logistic form are functions of the gaussian parameters we get their maximum likelihood estimates by plugging in the corresponding estimates
however unlike in the conditional case the marginal density pr does play role here
it is mixture density pr k which also involves the parameters
what role can this additional component restriction play
by relying on the additional model assumptions we have more information about the parameters and hence can estimate them more efficiently lower variance
if in fact the true fk are gaussian then in the worst case ignoring this marginal part of the likelihood constitutes loss of efficiency of about asymptotically in the error rate efron
paraphrasing with more data the conditional likelihood will do as well
for example observations far from the decision boundary which are down weighted by logistic regression play role in estimating the common covariance matrix
this is not all good news because it also means that lda is not robust to gross outliers
from the mixture formulation it is clear that even observations without class labels have information about the parameters
often it is expensive to generate class labels but unclassified observations come cheaply
by relying on strong model assumptions such as here we can use both types of information
the marginal likelihood can be thought of as regularizer requiring in some sense that class densities be visible from this marginal view
for example if the data in two class logistic regression model can be perfectly separated by hyperplane the maximum likelihood estimates of the parameters are undefined infinite see exercise
the lda coefficients for the same data will be well defined since the marginal likelihood will not permit these degeneracies
in practice these assumptions are never correct and often some of the components of are qualitative variables
it is generally felt that logistic regression is safer more robust bet than the lda model relying on fewer assumptions
it is our experience that the models give very similar results even when lda is used inappropriately such as with qualitative predictors
toy example with two classes separable by hyperplane
the orange line is the least squares solution which misclassifies one of the training points
also shown are two blue separating hyperplanes found by the perceptron learning algorithm with different random starts
separating hyperplanes we have seen that linear discriminant analysis and logistic regression both estimate linear decision boundaries in similar but slightly different ways
for the rest of this chapter we describe separating hyperplane classifiers
these procedures construct linear decision boundaries that explicitly try to separate the data into different classes as well as possible
they provide the basis for support vector classifiers discussed in chapter
the mathematical level of this section is somewhat higher than that of the previous sections
figure shows data points in two classes in ir
these data can be separated by linear boundary
included in the figure blue lines are two of the infinitely many possible separating hyperplanes
the orange line is the least squares solution to the problem obtained by regressing the response on with intercept the line is given by
this least squares solution does not do perfect job in separating the points and makes one error
this is the same boundary found by lda in light of its equivalence with linear regression in the two class case section and exercise
classifiers such as that compute linear combination of the input features and return the sign were called perceptrons in the engineering liter
the linear algebra of hyperplane affine set ature in the late rosenblatt
perceptrons set the foundations for the neural network models of the and
before we continue let us digress slightly and review some vector algebra
figure depicts hyperplane or affine set defined by the equation since we are in ir this is line
here we list some properties
for any two points and lying in and hence is the vector normal to the surface of
for any point in
the signed distance of any point to is given by
hence is proportional to the signed distance from to the hyperplane defined by
rosenblatt's perceptron learning algorithm the perceptron learning algorithm tries to find separating hyperplane by minimizing the distance of misclassified points to the decision boundary
separating hyperplanes response yi is misclassified then xti and the opposite for misclassified response with yi
the goal is to minimize yi xti where indexes the set of misclassified points
the quantity is nonnegative and proportional to the distance of the misclassified points to the decision boundary defined by
the gradient assuming is fixed is given by yi xi yi
the algorithm in fact uses stochastic gradient descent to minimize this piecewise linear criterion
this means that rather than computing the sum of the gradient contributions of each observation followed by step in the negative gradient direction step is taken after each observation is visited
hence the misclassified observations are visited in some sequence and the parameters are updated via yi xi
yi here is the learning rate which in this case can be taken to be without loss in generality
if the classes are linearly separable it can be shown that the algorithm converges to separating hyperplane in finite number of steps exercise
figure shows two solutions to toy problem each started at different random guess
the smaller the gap the longer the time to find it
the cycles can be long and therefore hard to detect
the second problem can often be eliminated by seeking hyperplane not in the original space but in much enlarged space obtained by creating
linear methods for classification many basis function transformations of the original variables
this is analogous to driving the residuals in polynomial regression problem down to zero by making the degree sufficiently large
perfect separation cannot always be achieved for example if observations from two different classes share the same input
it may not be desirable either since the resulting model is likely to be overfit and will not generalize well
we return to this point at the end of the next section
rather elegant solution to the first problem is to add additional constraints to the separating hyperplane
optimal separating hyperplanes the optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class vapnik
not only does this provide unique solution to the separating hyperplane problem but by maximizing the margin between the two classes on the training data this leads to better classification performance on test data
we need to generalize criterion
consider the optimization problem max subject to yi xti
the set of conditions ensure that all the points are at least signed distance from the decision boundary defined by and and we seek the largest such and associated parameters
we can get rid of the constraint by replacing the conditions with yi xti which redefines or equivalently yi xti
since for any and satisfying these inequalities any positively scaled multiple satisfies them too we can arbitrarily set
thus is equivalent to min subject to yi xti
in light of the constraints define an empty slab or margin around the linear decision boundary of thickness
hence we choose and to maximize its thickness
this is convex optimization problem quadratic
separating hyperplanes criterion with linear inequality constraints
the lagrange primal function to be minimized
and is xn lp yi xti
setting the derivatives to zero we obtain yi xi yi and substituting these in we obtain the so called wolfe dual xx ld yi yk xti xk subject to
the solution is obtained by maximizing ld in the positive orthant simpler convex optimization problem for which standard software can be used
in addition the solution must satisfy the karush kuhn tucker conditions which include and yi xti
from we see that the solution vector is defined in terms of linear combination of the support points xi those points defined to be on the boundary of the slab via
figure shows the optimal separating hyperplane for our toy example there are three support points
likewise is obtained by solving for any of the support points
the optimal separating hyperplane produces function xt for classifying new observations signf
although none of the training observations fall in the margin by construction this will not necessarily be the case for test observations
the same data as in figure
the shaded region delineates the maximum margin separating the two classes
there are three support points indicated which lie on the boundary of the margin and the optimal separating hyperplane blue line bisects the slab
included in the figure is the boundary found using logistic regression red line which is very close to the optimal separating hyperplane see section intuition is that large margin on the training data will lead to good separation on the test data
the description of the solution in terms of support points seems to suggest that the optimal hyperplane focuses more on the points that count and is more robust to model misspecification
the lda solution on the other hand depends on all of the data even points far away from the decision boundary
note however that the identification of these support points required the use of all the data
of course if the classes are really gaussian then lda is optimal and separating hyperplanes will pay price for focusing on the noisier data at the boundaries of the classes
included in figure is the logistic regression solution to this problem fit by maximum likelihood
both solutions are similar in this case
when separating hyperplane exists logistic regression will always find it since the log likelihood can be driven to in this case exercise
the logistic regression solution shares some other qualitative features with the separating hyperplane solution
the coefficient vector is defined by weighted least squares fit of zero mean linearized response on the input features and the weights are larger for points near the decision boundary than for those further away
when the data are not separable there will be no feasible solution to this problem and an alternative formulation is needed
again one can enlarge the space using basis transformations but this can lead to artificial
exercises separation through over fitting
in chapter we discuss more attractive alternative known as the support vector machine which allows for overlap but minimizes measure of the extent of this overlap
bibliographic notes good general texts on classification include duda et al
hand mclachlan and ripley
mardia et al have concise discussion of linear discriminant analysis
michie et al compare large number of popular classifiers on benchmark datasets
linear separating hyperplanes are discussed in vapnik
our account of the perceptron learning algorithm follows ripley
exercises ex
show how to solve the generalized eigenvalue problem max at ba subject to at wa by transforming to standard eigenvalue problem
suppose we have features irp two class response with class sizes and the target coded as show that the lda rule classifies to class if xt log log and class otherwise consider minimization of the least squares criterion yi xi
show that the solution satisfies after simplification where hence show that is in the direction and thus
therefore the least squares regression coefficient is identical to the lda coefficient up to scalar multiple
linear methods for classification show that this result holds for any distinct coding of the two classes find the solution and hence the predicted values
consider the following rule classify to class if and class otherwise
show this is not the same as the lda rule unless the classes have equal numbers of observations
fisher ripley ex
suppose we transform the original predictors to via linear regression
in detail let xt xt xb where is the indicator response matrix
similarly for any input irp we get transformed vector irk
show that lda using is identical to lda in the original space
consider the multilogit model with classes
let be the vector consisting of all the coefficients
define suitably enlarged version of the input vector to accommodate this vectorized coefficient matrix
derive the newton raphson algorithm for maximizing the multinomial log likelihood and describe how you would implement this algorithm
consider two class logistic regression problem with ir
characterize the maximum likelihood estimates of the slope and intercept parameter if the sample xi for the two classes are separated by point ir
generalize this result to irp see figure and more than two classes
suppose we have points xi in irp in general position with class labels yi
prove that the perceptron learning algorithm converges to separating hyperplane in finite number of steps denote hyperplane by or in more compact notation where and
let zi
show that separability implies the existence of sep such that yi sep zi given current old the perceptron algorithm identifies point zi that is misclassified and produces the update new old yi zi
show that new sep old sep and hence that the algorithm converges to separating hyperplane in no more than start sep steps ripley
consider the criterion yi xti
exercises generalization of where we sum over all the observations
consider minimizing subject to
describe this criterion in words
does it solve the optimal separating hyperplane problem
consider the multivariate gaussian model k with the additional restriction that rank k max
derive the constrained mles for the k and
show that the bayes classification rule is equivalent to classifying in the reduced subspace computed by lda hastie and tibshirani
write computer program to perform quadratic discriminant analysis by fitting separate gaussian model per class
try it out on the vowel data and compute the misclassification error for the test data
the data can be found in the book website www stat stanford edu elemstatlearn
linear methods for classification
this is page printer opaque this basis expansions and regularization introduction we have already made use of models linear in the input features both for regression and classification
linear regression linear discriminant analysis logistic regression and separating hyperplanes all rely on linear model
it is extremely unlikely that the true function is actually linear in
in regression problems will typically be nonlinear and nonadditive in and representing by linear model is usually convenient and sometimes necessary approximation
convenient because linear model is easy to interpret and is the first order taylor approximation to
sometimes necessary because with small and or large linear model might be all we are able to fit to the data without overfitting
likewise in classification linear bayes optimal decision boundary implies that some monotone transformation of pr is linear in
this is inevitably an approximation
in this chapter and the next we discuss popular methods for moving beyond linearity
the core idea in this chapter is to augment replace the vector of inputs with additional variables which are transformations of and then use linear models in this new space of derived input features
denote by hm irp ir the mth transformation of
we then model hm
basis expansions and regularization linear basis expansion in
the beauty of this approach is that once the basis functions hm have been determined the models are linear in these new variables and the fitting proceeds as before
note however that the number of variables grows exponentially in the degree of the polynomial
by breaking the range of xk up into mk such nonoverlapping regions results in model with piecewise constant contribution for xk
sometimes the problem at hand will call for particular basis functions hm such as logarithms or power functions
more often however we use the basis expansions as device to achieve more flexible representations for
polynomials are an example of the latter although they are limited by their global nature tweaking the coefficients to achieve functional form in one region can cause the function to flap about madly in remote regions
in this chapter we consider more useful families of piecewise polynomials and splines that allow for local polynomial representations
we also discuss the wavelet bases especially useful for modeling signals and images
these methods produce dictionary consisting of typically very large number of basis functions far more than we can afford to fit to our data
along with the dictionary we require method for controlling the complexity of our model using basis functions from the dictionary
additivity is an example where we assume that our model has the form fj xj mj jm hjm xj

piecewise polynomials and splines the size of the model is limited by the number of basis functions mj used for each component function fj
here the variable selection techniques discussed in chapter are useful
the stagewise greedy approaches such as cart mars and boosting fall into this category as well
ridge regression is simple example of regularization approach while the lasso is both regularization and selection method
here we discuss these and more sophisticated methods for regularization
piecewise polynomials and splines we assume until section that is one dimensional
piecewise polynomial function is obtained by dividing the domain of into contiguous intervals and representing by separate polynomial in each interval
figure shows two simple piecewise polynomials
the first is piecewise constant with three basis functions be be be be
over disjoint regions the least squares estimate of since these are positive the model hm amounts to the mean of in the mth region
the top right panel shows piecewise linear fit
three additional basis functions are needed hm hm
except in special cases we would typically prefer the third panel which is also piecewise linear but restricted to be continuous at the two knots
these continuity restrictions lead to linear constraints on the parameters for example be be implies that be be
in this case since there are two restrictions we expect to get back two parameters leaving four free parameters
more direct way to proceed in this case is to use basis that incorporates the constraints be be where denotes the positive part
the function is shown in the lower right panel of figure
we often prefer smoother functions and these can be achieved by increasing the order of the local polynomial
figure shows series of piecewise cubic polynomials fit to the same data with
the top left panel shows piecewise constant function fit to some artificial data
the broken vertical lines indicate the positions of the two knots be and be
the blue curve represents the true function from which the data were generated with gaussian noise
the remaining two panels show piecewise linear functions fit to the same data the top right unrestricted and the lower left restricted to be continuous at the knots
the lower right panel shows piecewise linear basis function be continuous at be
the black points indicate the sample evaluations xi
series of piecewise cubic polynomials with increasing orders of continuity increasing orders of continuity at the knots
the function in the lower right panel is continuous and has continuous first and second derivatives at the knots
it is known as cubic spline
enforcing one more order of continuity would lead to global cubic polynomial
it is not hard to show exercise that the following basis represents cubic spline with knots at be and be be be
there are six basis functions corresponding to six dimensional linear space of functions
quick check confirms the parameter count regions parameters per region knots constraints per knot
basis expansions and regularization more generally an order spline with knots be is piecewise polynomial of order and has continuous derivatives up to order
cubic spline has
in fact the piecewise constant function in figure is an order spline while the continuous piecewise linear function is an order spline
likewise the general form for the truncated power basis set would be hj hm be
it is claimed that cubic splines are the lowest order spline for which the knot discontinuity is not visible to the human eye
there is seldom any good reason to go beyond cubic splines unless one is interested in smooth derivatives
in practice the most widely used orders are and
these fixed knot splines are also known as regression splines
one needs to select the order of the spline the number of knots and their placement
one simple approach is to parameterize family of splines by the number of basis functions or degrees of freedom and have the observations xi determine the positions of the knots
for example the expression bs df in generates basis matrix of cubic spline functions evaluated at the observations in with the interior knots at the appropriate percentiles of and th
one can be more explicit however bs degree knots generates basis for linear splines with three interior knots and returns an matrix
since the space of spline functions of particular order and knot sequence is vector space there are many equivalent bases for representing them just as there are for ordinary polynomials
while the truncated power basis is conceptually simple it is not too attractive numerically powers of large numbers can lead to severe rounding problems
the spline basis described in the appendix to this chapter allows for efficient computations even when the number of knots is large
natural cubic splines we know that the behavior of polynomials fit to data tends to be erratic near the boundaries and extrapolation can be dangerous
these problems are exacerbated with splines
the polynomials fit beyond the boundary knots behave even more wildly than the corresponding global polynomials in that region
this can be conveniently summarized in terms of the pointwise variance of spline functions fit by least squares see the example in the next section for details on these variance calculations
figure compares cubic spline with four knots is eight dimensional
the bs function omits by default the constant term in the basis since terms like this are typically included with other terms in the model
pointwise variance curves for four different models with consisting of points drawn at random from and an assumed error model with constant variance
the linear and cubic polynomial fits have two and four degrees of freedom respectively while the cubic spline and natural cubic spline each have six degrees of freedom
the cubic spline has two knots at and while the natural spline has boundary knots at and and four interior knots uniformly spaced between them the pointwise variances for variety of different models
the explosion of the variance near the boundaries is clear and inevitably is worst for cubic splines
natural cubic spline adds additional constraints namely that the function is linear beyond the boundary knots
this frees up four degrees of freedom two constraints each in both boundary regions which can be spent more profitably by sprinkling more knots in the interior region
this tradeoff is illustrated in terms of variance in figure
there will be price paid in bias near the boundaries but assuming the function is linear near the boundaries where we have less information anyway is often considered reasonable
natural cubic spline with knots is represented by basis functions
one can start from basis for cubic splines and derive the reduced basis by imposing the boundary constraints
for example starting from the truncated power series basis described in section we arrive at exercise nk dk dk
basis expansions and regularization where be be dk
be be each of these basis functions can be seen to have zero second and third derivative for be
example south african heart disease continued in section we fit linear logistic regression models to the south african heart disease data
here we explore nonlinearities in the functions using natural splines
the functional form of the model is logit pr chd hp xp where each of the are vectors of coefficients multiplying their associated vector of natural spline basis functions hj
we use four natural spline bases for each term in the model
for example with representing sbp is basis consisting of four basis functions
this actually implies three rather than two interior knots chosen at uniform quantiles of sbp plus two boundary knots at the extremes of the data since we exclude the constant term from each of the hj
since famhist is two level factor it is coded by simple binary or dummy variable and is associated with single coefficient in the fit of the model
more compactly we can combine all vectors of basis functions and the constant term into one big vector and then pthe model is simply with total number of parameters df dfj the sum of the parameters in each component term
each basis function is evaluated at each of the samples resulting in df basis matrix
at this point the model is like any other linear logistic model and the algorithms described in section apply
we carried out backward stepwise deletion process dropping terms from this model while preserving the group structure of each term rather than dropping one coefficient at time
the aic statistic section was used to drop terms and all the terms remaining in the final model would cause aic to increase if deleted from the model see table
figure shows plot of the final model selected by the stepwise regression
the functions displayed are xj hj xj for each variable xj
the covariance matrix cov is estimated by ht wh where is the diagonal weight matrix from the logistic regression
hence vj xj var xj hj xj jj hj xj is the pointwise variance function of where cov jj is the appropriate sub matrix of
the shaded region in each panel is defined by xj vj xj
the aic statistic is slightly more generous than the likelihood ratio test deviance test
both sbp and obesity are included in this model while
fitted natural spline functions for each of the terms in the final model selected by the stepwise procedure
included are pointwise standard error bands
the rug plot at the base of each figure indicates the location of each of the sample values for that variable jittered to break ties
basis expansions and regularization table
final logistic regression model after stepwise deletion of natural splines terms
the column labeled lrt is the likelihood ratio test statistic when that term is deleted from the model and is the change in deviance from the full model labeled none
terms df deviance aic lrt value none sbp tobacco ldl famhist obesity age they were not in the linear model
the figure explains why since their contributions are inherently nonlinear
these effects at first may come as surprise but an explanation lies in the nature of the retrospective data
these measurements were made sometime after the patients suffered heart attack and in many cases they had already benefited from healthier diet and lifestyle hence the apparent increase in risk at low values for obesity and sbp
table shows summary of the selected model
example phoneme recognition in this example we use splines to reduce flexibility rather than increase it the application comes under the general heading of functional modeling
in the top panel of figure are displayed sample of log periodograms for each of the two phonemes aa and ao measured at frequencies
the goal is to use such data to classify spoken phoneme
these two phonemes were chosen because they are difficult to separate
the input feature is vector of length which we can think of as vector of evaluations of function over grid of frequencies
in reality there is continuous analog signal which is function of frequency and we have sampled version of it
the gray lines in the lower panel of figure show the coefficients of linear logistic regression model fit by maximum likelihood to training sample of drawn from the total of aa and ao
the coefficients are also plotted as function of frequency and in fact we can think of the model in terms of its continuous counterpart pr aa log df pr ao
the top panel displays the log periodogram as function of frequency for examples each of the phonemes aa and ao sampled from total of aa and ao
each log periodogram is measured at uniformly spaced frequencies
the lower panel shows the coefficients as function of frequency of logistic regression fit to the data by maximum likelihood using the log periodogram values as inputs
the coefficients are restricted to be smooth in the red curve and are unrestricted in the jagged gray curve
basis expansions and regularization which we approximate by fj fj xj
the coefficients compute contrast functional and will have appreciable values in regions of frequency where the log periodograms differ between the two classes
the gray curves are very rough
since the input signals have fairly strong positive autocorrelation this results in negative autocorrelation in the coefficients
in addition the sample size effectively provides only four observations per coefficient
applications such as this permit natural regularization
we force the coefficients to vary smoothly as function of frequency
the red curve in the lower panel of figure shows such smooth coefficient curve fit to these data
we see that the lower frequencies offer the most discriminatory power
not only does the smoothing allow easier interpretation of the contrast it also produces more accurate classifier raw regularized training error test error the smooth red curve was obtained through very simple use of natural pmcan represent the coefficient function as an expansion of cubic splines
we splines hm
in practice this means that where is basis matrix of natural cubic splines defined on the set of frequencies
here we used basis functions with knots uniformly placed over the integers representing the frequencies
since xt xt we can simply replace the input features by their filtered versions ht and fit by linear logistic regression on the
the red curve is thus
filtering and feature extraction in the previous example we constructed basis matrix and then transformed our features into new features ht
these filtered versions of the features were then used as inputs into learning procedure in the previous example this was linear logistic regression
preprocessing of high dimensional features is very general and powerful method for improving the performance of learning algorithm
the preprocessing need not be linear as it was above but can be general
smoothing splines nonlinear function of the form
the derived features can then be used as inputs into any linear or nonlinear learning procedure
for example for signal or image recognition popular approach is to first transform the raw features via wavelet transform ht section and then use the features as inputs into neural network chapter
wavelets are effective in capturing discrete jumps or edges and the neural network is powerful tool for constructing nonlinear functions of these features for predicting the target variable
by using domain knowledge to construct appropriate features one can often improve upon learning method that has only the raw features at its disposal
smoothing splines here we discuss spline basis method that avoids the knot selection problem completely by using maximal set of knots
the complexity of the fit is controlled by regularization
consider the following problem among all functions with two continuous derivatives find one that minimizes the penalized residual sum of squares xn rss bb yi xi bb dt where bb is fixed smoothing parameter
the first term measures closeness to the data while the second term penalizes curvature in the function and bb establishes tradeoff between the two
two special cases are bb can be any function that interpolates the data
bb the simple least squares line fit since no second derivative can be tolerated
these vary from very rough to very smooth and the hope is that bb indexes an interesting class of functions in between
the criterion is defined on an infinite dimensional function spacein fact sobolev space of functions for which the second term is defined
remarkably it can be shown that has an explicit finite dimensional unique minimizer which is natural cubic spline with knots at the unique values of the xi exercise
at face value it seems that the family is still over parametrized since there are as many as knots which implies degrees of freedom
however the penalty term translates to penalty on the spline coefficients which are shrunk some of the way toward the linear fit
since the solution is natural spline we can write it as nj
the response is the relative change in bone mineral density measured at the spine in adolescents as function of age
separate smoothing spline was fit to the males and females with bb
this choice corresponds to about degrees of freedom where the nj are an dimensional set of basis functions for representing this family of natural splines section and exercise
the criterion thus reduces to rss bb bb where ij nj xi and jk nj nk dt
the solution is easily seen to be nt bb nt generalized ridge regression
the fitted smoothing spline is given by nj
efficient computational techniques for smoothing splines are discussed in the appendix to this chapter
figure shows smoothing spline fit to some data on bone mineral density bmd in adolescents
the response is relative change in spinal bmd over two consecutive visits typically about one year apart
the data are color coded by gender and two separate curves were fit
this simple
smoothing splines summary reinforces the evidence in the data that the growth spurt for females precedes that for males by about two years
in both cases the smoothing parameter bb was approximately this choice is discussed in the next section
degrees of freedom and smoother matrices we have not yet indicated how bb is chosen for the smoothing spline
later in this chapter we describe automatic methods using techniques such as cross validation
in this section we discuss intuitive ways of prespecifying the amount of smoothing
smoothing spline with prechosen bb is an example of linear smoother as in linear operator
this is because the estimated parameters in are linear combination of the yi
denote by the vector of fitted values xi at the training predictors xi
then nt bb nt bb
again the fit is linear in and the finite linear operator bb is known as the smoother matrix
one consequence of this linearity is that the recipe for producing from does not depend on itself bb depends only on the xi and bb
linear operators are familiar in more traditional least squares fitting as well
suppose be is matrix of cubic spline basis functions evaluated at the training points xi with knot sequence be and
then the vector of fitted spline values is given by be bt be be bt be be
here the linear operator be is projection operator also known as the hat matrix in statistics
this is consequence of the shrinking nature of bb which we discuss further below
the expression trace be gives the dimension of the projection space which is also the number of basis functions and hence the number of parameters involved in the fit
by analogy we define the effective degrees of
basis expansions and regularization freedom of smoothing spline to be df bb trace bb the sum of the diagonal elements of bb
this very useful definition allows us more intuitive way to parameterize the smoothing spline and indeed many other smoothers as well in consistent fashion
for example in figure we specified df bb for each of the curves and the corresponding bb was derived numerically by solving trace bb
there are many arguments supporting this definition of degrees of freedom and we cover some of them here
since bb is symmetric and positive semidefinite it has real eigendecomposition
before we proceed it is convenient to rewrite bb in the reinsch form bb bb where does not depend on bb exercise
since bb solves min bb kf is known as the penalty matrix and indeed quadratic form in has representation in terms of weighted sum of squared divided second differences
the eigen decomposition of bb is bb bb uk utk with bb bb dk and dk the corresponding eigenvalue of
figure top shows the results of applying cubic smoothing spline to some air pollution data observations
two fits are given smoother fit corresponding to larger penalty bb and rougher fit for smaller penalty
the lower panels represent the eigenvalues lower left and some eigenvectors lower right of the corresponding smoother matrices
this is to be contrasted with basis regression method where the components are
top smoothing spline fit of ozone concentration versus daggot pressure gradient
the two fits correspond to different values of the smoothing parameter chosen to achieve five and eleven effective degrees of freedom defined by df bb trace bb
lower left first eigenvalues for the two smoothing spline matrices
the first two are exactly and all are
lower right third to sixth eigenvectors of the spline smoother matrices
in each case uk is plotted against and as such is viewed as function of
the rug at the base of the plots indicate the occurrence of data points
the damped functions represent the smoothed versions of these functions using the df smoother
basis expansions and regularization either left alone or shrunk to zero that is projection matrix such as be above has eigenvalues equal to and the rest are
for this reason smoothing splines are referred to as shrinking smoothers while regression splines are projection smoothers see figure on page
indeed they have the zero crossing behavior of polynomials of increasing degree
since bb uk bb uk we see how each of the eigenvectors themselves are shrunk by the smoothing spline the higher the complexity the more they are shrunk
if the domain of is periodic then the uk are sines and cosines at different frequencies
in this case the smoothing spline solves min ky bb where has columns uk and is diagonal matrix with elements dk
for projection smoothers all the eigenvalues are each one corresponding to dimension of the projection subspace
figure depicts smoothing spline matrix with the rows ordered with
the banded nature of this representation suggests that smoothing spline is local fitting method much like the locally weighted regression procedures in chapter
the right panel shows in detail selected rows of which we call the equivalent kernels
as bb df bb and bb the dimensional identity matrix
as bb df bb and bb the hat matrix for linear regression on
automatic selection of the smoothing parameters the smoothing parameters for regression splines encompass the degree of the splines and the number and placement of the knots
for smoothing
the smoother matrix for smoothing spline is nearly banded indicating an equivalent kernel with local support
the left panel represents the elements of as an image
the right panel shows the equivalent kernel or weighting function in detail for the indicated rows
basis expansions and regularization splines we have only the penalty parameter bb to select since the knots are at all the unique training x's and cubic degree is almost always used in practice
selecting the placement and number of knots for regression splines can be combinatorially complex task unless some simplifications are enforced
the mars procedure in chapter uses greedy algorithm with some additional approximations to achieve practical compromise
we will not discuss this further here
fixing the degrees of freedom since df bb trace bb is monotone in bb for smoothing splines we can invert the relationship and specify bb by fixing df
in practice this can be achieved by simple numerical methods
so for example in one can use smooth spline df to specify the amount of smoothing
this encourages more traditional mode of model selection where we might try couple of different values of df and select one based on approximate tests residual plots and other more subjective criteria
using df in this way provides uniform approach to compare many different smoothing methods
it is particularly useful in generalized additive models chapter where several smoothing methods can be simultaneously used in one model
the bias variance tradeoff figure shows the effect of the choice of df bb when using smoothing spline on simple example sin with and
our training sample consists of pairs xi yi drawn independently from this model
the fitted splines for three different values of df bb are shown
the yellow shaded region in the figure represents the pointwise standard error of bb that is we have shaded the region between bb se bb
since bb cov bb cov st bb bb st bb
the diagonal contains the pointwise variances at the training xi
the bias is given by bias bb
the top left panel shows the epe bb and cv bb curves for realization from nonlinear additive error model
the remaining panels show the data the true functions in purple and the fitted curves in green with yellow shaded standard error bands for three different values of df bb
basis expansions and regularization where is the unknown vector of evaluations of the true at the training x's
the expectations and variances are with respect to repeated draws of samples of size from the model
in similar fashion var bb and bias bb can be computed at any point exercise
the three fits displayed in the figure give visual demonstration of the bias variance tradeoff associated with selecting the smoothing parameter df bb the spline under fits and clearly trims down the hills and fills in the valleys
this leads to bias that is most dramatic in regions of high curvature
the standard error band is very narrow so we estimate badly biased version of the true function with great reliability
df bb here the fitted function is close to the true function although slight amount of bias seems evident
the variance has not increased appreciably df bb the fitted function is somewhat wiggly but close to the true function
the wiggliness also accounts for the increased width of the standard error bands the curve is starting to follow some individual points too closely
note that in these figures we are seeing single realization of data and hence fitted spline in each case while the bias involves an expectation
we leave it as an exercise to compute similar figures where the bias is shown as well
the middle curve seems just right in that it has achieved good compromise between bias and variance
the integrated squared prediction error epe combines both bias and variance in single summary epe bb bb var bias bb var bb mse bb
note that this is averaged both over the training sample giving rise to bb and the values of the independently chosen prediction points
epe is natural quantity of interest and does create tradeoff between bias and variance
the blue points in the top left panel of figure suggest that df bb is spot on
since we don't know the true function we do not have access to epe and need an estimate
this topic is discussed in some detail in chapter and techniques such as fold cross validation gcv and cp are all in common use
in figure we include the fold leave one out cross validation curve
nonparametric logistic regression cv bb yi bb xi
yi bb xi bb which can remarkably be computed for each value of bb from the original fitted values and the diagonal elements bb of bb exercise
the epe and cv curves have similar shape but the entire cv curve is above the epe curve
for some realizations this is reversed and overall the cv curve is approximately unbiased as an estimate of the epe curve
nonparametric logistic regression the smoothing spline problem in section is posed in regression setting
it is typically straightforward to transfer this technology to other domains
here we consider logistic regression with single quantitative input
the model is pr log pr which implies ef pr
ef fitting in smooth fashion leads to smooth estimate of the conditional probability pr which can be used for classification or risk scoring
we construct the penalized log likelihood criterion bb yi log xi yi log xi bb dt xn yi xi log ef xi bb dt where we have abbreviated pr
the first term in this expression is the log likelihood based on the binomial distribution
chapter page
arguments similar to those used in section show that the optimal is finite dimensional natural spline with knots at the unique
basis expansions and regularization pn values of
this means that we can represent nj
we compute the first and second derivatives nt bb nt wn bb where is the vector with elements xi and is diagonal matrix of weights xi xi
the first derivative is nonlinear in so we need to use an iterative algorithm as in section
using newton raphson as in and for linear logistic regression the update equation can be written new nt wn bb nt old nt wn bb nt wz
we can also express this update in terms of the fitted values new nt wn bb nt old bb
referring back to and we see that the update fits weighted smoothing spline to the working response exercise
the form of is suggestive
it is tempting to replace bb by any nonparametric weighted regression operator and obtain general families of nonparametric logistic regression models
although here is onedimensional this procedure generalizes naturally to higher dimensional
these extensions are at the heart of generalized additive models which we pursue in chapter
multidimensional splines so far we have focused on one dimensional spline models
each of the approaches have multidimensional analogs
suppose ir and we have basis of functions for representing functions of coordinate and likewise set of functions for coordinate
then the dimensional tensor product basis defined by gjk can be used for representing two dimensional function jk gjk

tensor product basis of splines showing some selected pairs
each two dimensional function is the tensor product of the corresponding one dimensional marginals
figure illustrates tensor product basis using splines
the coefficients can be fit by least squares as before
this can be generalized to dimensions but note that the dimension of the basis grows exponentially fast yet another manifestation of the curse of dimensionality
the mars procedure discussed in chapter is greedy forward algorithm for including only those tensor products that are deemed necessary by least squares
figure illustrates the difference between additive and tensor product natural splines on the simulated classification example from chapter
logistic regression model logit pr is fit to the binary response and the estimated decision boundary is the contour
the tensor product basis can achieve more flexibility at the decision boundary but introduces some spurious structure along the way
error
the simulation example of figure
the upper panel shows the decision boundary of an additive logistic regression model using natural splines in each of the two coordinates total df
the lower panel shows the results of using tensor product of natural spline bases in each coordinate total df
the broken purple boundary is the bayes decision boundary for this problem
multidimensional splines one dimensional smoothing splines via regularization generalize to higher dimensions as well
suppose we have pairs yi xi with xi ird and we seek dimensional regression function
the idea is to set up the problem xn min yi xi bb where is an appropriate penalty functional for stabilizing function in ird
for example natural generalization of the one dimensional roughness penalty for functions on ir is h dx dx
ir optimizing with this penalty leads to smooth two dimensional surface known as thin plate spline
the solution has the form hj where hj xj log xj
these hj are examples of radial basis functions which are discussed in more detail in the next section
the coefficients are found by plugging into which reduces to finite dimensional penalized least squares problem
for the penalty to be finite the coefficients have to satisfy set of linear constraints see exercise
thin plate splines are defined more generally for arbitrary dimension for which an appropriately more general is used
there are number of hybrid approaches that are popular in practice both for computational and conceptual simplicity
unlike one dimensional smoothing splines the computational complexity for thin plate splines is since there is not in general any sparse structure that can be exploited
however as with univariate smoothing splines we can get away with substantially less than the knots prescribed by the solution
thin plate spline fit to the heart disease data displayed as contour plot
the response is systolic blood pressure modeled as function of age and obesity
the data points are indicated as well as the lattice of points used as knots
care should be taken to use knots from the lattice inside the convex hull of the data red and ignore those outside green
in practice it is usually sufficient to work with lattice of knots covering the domain
the penalty is computed for the reduced expansion just as before
using knots reduces the computations to
figure shows the result of fitting thin plate spline to some heart disease risk factors representing the surface as contour plot
indicated are the location of the input features as well as the knots used in the fit
note that bb was specified via df bb trace bb
more generally one can represent ird as an expansion in any arbitrarily large collection of basis functions and control the complexity by applying regularizer such as
for example we could construct basis by forming the tensor products of all pairs of univariate smoothing spline basis functions as in using for example the univariate splines recommended in section as ingredients
this leads to an exponential
regularization and reproducing kernel hilbert spaces growth in basis functions as the dimension increases and typically we have to reduce the number of functions per coordinate accordingly
the additive spline models discussed in chapter are restricted class of multidimensional splines
they can be represented in this general formulation as well that is there exists penalty that guarantees that the solution has the form fd xd and that each of the functions fj are univariate splines
in this case the penalty is somewhat degenerate and it is more natural to assume that is additive and then simply impose an additional penalty on each of the component functions fd xd fj tj dtj
these are naturally extended to anova spline decompositions fj xj fjk xj xk where each of the components are splines of the required dimension
in many cases when the number of potential dimensions features is large automatic methods are more desirable
the mars and mart procedures chapters and respectively both fall into this category
regularization and reproducing kernel hilbert spaces in this section we cast splines into the larger context of regularization methods and reproducing kernel hilbert spaces
this section is quite technical and can be skipped by the disinterested or intimidated reader
basis expansions and regularization general class of regularization problems has the form min yi xi bb where is loss function is penalty functional and is space of functions on which is defined
girosi et al describe quite general penalty functionals of the form dc ds ird where dc denotes the fourier transform of and is some positive function that falls off to zero as
the idea is that increases the penalty for high frequency components of
under some additional assumptions they show that the solutions have the form xi where the span the null space of the penalty functional and is the inverse fourier transform of
smoothing splines and thin plate splines fall into this framework
the remarkable feature of this solution is that while the criterion is defined over an infinite dimensional space the solution is finite dimensional
in the next sections we look at some specific examples
spaces of functions generated by kernels an important subclass of problems of the form are generated by positive definite kernel and the corresponding space of functions hk is called reproducing kernel hilbert space rkhs
the penalty functional is defined in terms of the kernel as well
we give brief and simplified introduction to this class of models adapted from wahba and girosi et al and nicely summarized in evgeniou et al
let irp
we consider the space of functions generated by the linear span of irp arbitrary linear combinations of the form ym where each kernel term is viewed as function of the first argument and indexed by the second
suppose that has an eigen expansion with
elements of hk have an expansion in terms of these eigen functions ci
regularization and reproducing kernel hilbert spaces with the constraint that def hk where hk is the norm induced by
the penalty functional in for the space hk is defined to be the squared norm hk
the quantity can be interpreted as generalized ridge penalty where functions with large eigenvalues in the expansion get penalized less and vice versa
rewriting we have min yi xi bb hk hk or equivalently ee xn min yi cj xi bb fb
cj it can be shown wahba see also exercise that the solution to is finite dimensional and has the form xi
the basis function hi xi as function of the first argument is known as the representer of evaluation at xi in hk since for hk it is easily seen that hk xi ihk xi
similarly hk xi xj ihk xi xj the reproducing property of hk and hence xi xj pn for xi
in light of and reduces to finite dimensional criterion min bb
we are using vector notation in which is the matrix with ijth entry xi xj and so on
simple numerical algorithms can be used to optimize
this phenomenon whereby the infinite dimensional problem or reduces to finite dimensional optimization problem has been dubbed the kernel property in the literature on support vector machines see chapter
basis expansions and regularization there is bayesian interpretation of this class of models in which is interpreted as realization of zero mean stationary gaussian process with prior covariance function
the eigen decomposition produces series of orthogonal eigen functions with associated variances
the typical scenario is that smooth functions have large prior variance while rough have small prior variances
the penalty in is the contribution of the prior to the joint likelihood and penalizes more those components with smaller prior variance compare with
for simplicity we have dealt with the case here where all members of are penalized as in
more generally there may be some components in that we wish to leave alone such as the linear functions for cubic smoothing splines in section
the multidimensional thin plate splines of section and tensor product splines fall into this category as well
in these cases there is more convenient representation with the null space consisting of for example low degree polynomials in that do not get penalized
the penalty becomes kp where is the pmorthogonal projection pn of onto
the solution has the form hj xi where the first term represents an expansion in
from bayesian perspective the coefficients of components in have improper priors with infinite variance
examples of rkhs the machinery above is driven by the choice of the kernel and the loss function
we consider first regression using squared error loss
in this case specializes to penalized least squares and the solution can be characterized in two equivalent ways corresponding to or eb min ed cj xi bb cj an infinite dimensional generalized ridge regression problem or min bb
the solution for is obtained simply as bb and xj
regularization and reproducing kernel hilbert spaces the vector of fitted values is given by bb bb
the estimate also arises as the kriging estimate of gaussian random field in spatial statistics cressie
compare also with the smoothing spline fit on page
penalized polynomial regression hx yi vapnik for ir phas the kernel eigen functions that span the space of polynomials in ir of total degree
for example with and and hm hm with
one can represent in terms of the orthogonal eigen functions and eigenvalues of vd where diag and is and orthogonal
suppose we wish to solve the penalized polynomial regression problem
min yi hm xi bb
substituting into we get an expression of the form to optimize exercise
the number of basis functions can be very large often much larger than
equation tells us that if we use the kernel representation for the solution function we have only to evaluate the kernel times and can compute the solution in operations
this simplicity is not without implications
each of the polynomials hm in inherits scaling factor from the particular form of which has bearing on the impact of the penalty in
we elaborate on this in the next section
radial kernels kk for the mixture data with scale parameter bd
the kernels are centered at five points xm chosen at random from the
gaussian radial basis functions in the preceding example the kernel is chosen because it represents an expansion of polynomials and can conveniently compute high dimensional inner products
in this example the kernel is chosen because of its functional form in the representation
the gaussian kernel bd along with squared error loss for example leads to regression model that is an expansion in gaussian radial basis functions km bd xm each one centered at one of the training feature vectors xm
the coefficients are estimated using
figure illustrates radial kernels in ir using the first coordinate of the mixture example from chapter
we show five of the kernel basis functions km xm
figure illustrates the implicit feature space for the radial kernel with ir
we computed the kernel matrix and its eigendecomposition
we can think of the columns of and the corresponding eigenvalues in as empirical estimates of the eigen expansion
although the eigenvectors are discrete we can represent them as functions on ir exercise
figure shows the largest eigenvalues of
the leading eigenfunctions are smooth and they are successively more wiggly as the order increases
this brings to life the penalty in where we see the coefficients of higher order functions get penalized more than lower order ones
the right panel in figure shows the correspond the th column of is an estimate of evaluated at each of the observations
alternatively the ith row of is the estimated vector of basis functions xi evaluated at the point xi
although in principle there can be infinitely many elements in our estimate has at most elements
left panel the first normalized eigenvectors of the kernel matrix for the first coordinate of the mixture data
these are viewed as estimates of the eigenfunctions in and are represented as functions in ir with the observed values superimposed in color
they arevarranged in rows starting at the top left
the largest eigenvalues of all those beyond the th are effectively zero
basis expansions and regularization ing feature space representation of the eigenfunctions
note that hh xi xi xi xi
the scaling by the eigenvalues quickly shrinks most of the functions down to zero leaving an effective dimension of about in this case
the corresponding optimization problem is standard ridge regression as in
so although in principle the implicit feature space is infinite dimensional the effective dimension is dramatically lower because of the relative amounts of shrinkage applied to each basis function
the kernel scale parameter bd plays role here as well larger bd implies more local km functions and increases the effective dimension of the feature space
see hastie and zhu for more details
it is also known girosi et al that thin plate spline section is an expansion in radial basis functions generated by the kernel kx yk log kx yk
radial basis functions are discussed in more detail in section
support vector classifiers the support vector machines of chapterpn for two class classification problem have the form xi where the parameters are chosen to minimize bb min yi xi where yi and denotes the positive part of
this can be viewed as quadratic optimization problem with linear constraints and requires quadratic programming algorithm for its solution
the name support vector arises from the fact that typically many of the due to the piecewise zero nature of the loss function in and so is an expansion in subset of the xi
see section for more details
wavelet smoothing we have seen two different modes of operation with dictionaries of basis functions
with regression splines we select subset of the bases using either subject matter knowledge or else automatically
the more adaptive procedures such as mars chapter can capture both smooth and nonsmooth behavior
with smoothing splines we use complete basis but then shrink the coefficients toward smoothness
some selected wavelets at different translations and dilations for the haar and symmlet families
the functions have been scaled to suit the display
wavelets typically use complete orthonormal basis to represent functions but then shrink and select the coefficients toward sparse representation
just as smooth function can be represented by few spline basis functions mostly flat function with few isolated bumps can be represented with few bumpy basis functions
wavelets bases are very popular in signal processing and compression since they are able to represent both smooth and or locally bumpy functions in an efficient way phenomenon dubbed time and frequency localization
in contrast the traditional fourier basis allows only frequency localization
before we give details let's look at the haar wavelets in the left panel of figure to get an intuitive idea of how wavelet smoothing works
the vertical axis indicates the scale frequency of the wavelets from low scale at the bottom to high scale at the top
at each scale the wavelets are packed in side by side to completely fill the time axis we have only shown
basis expansions and regularization selected subset
wavelet smoothing fits the coefficients for this basis by least squares and then thresholds discards filters the smaller coefficients
since there are many basis functions at each scale it can use bases where it needs them and discard the ones it does not need to achieve time and frequency localization
the haar wavelets are simple to understand but not smooth enough for most purposes
the symmlet wavelets in the right panel of figure have the same orthonormal properties but are smoother
figure displays an nmr nuclear magnetic resonance signal which appears to be composed of smooth components and isolated spikes plus some noise
the wavelet transform using symmlet basis is shown in the lower left panel
the wavelet coefficients are arranged in rows from lowest scale at the bottom to highest scale at the top
the length of each line segment indicates the size of the coefficient
the bottom right panel shows the wavelet coefficients after they have been thresholded
the threshold procedure given below in equation is the same soft thresholding rule that arises in the lasso procedure for linear regression section
notice that many of the smaller coefficients have been set to zero
the green curve in the top panel shows the back transform of the thresholded coefficients this is the smoothed version of the original signal
in the next section we give the details of this process including the construction of wavelets and the thresholding rule
wavelet bases and the wavelet transform in this section we give details on the construction and filtering of wavelets
wavelet bases are generated by translations and dilations of single scaling function also known as the father
the red curves in figure are the haar and symmlet scaling functions
the haar basis is particularly easy to understand especially for anyone with experience in analysis of variance or trees since it produces piecewise constant representation
thus if then an integer generates an orthonormal basis for functions with jumps at the integers
call this reference space
the dilations form an orthonormal basis for space of functions piecewise constant on intervals of length
in fact more generally we have where each vj is spanned by
now to the definition of wavelets
in analysis of variance we often represent pair of means and by their grand mean and then contrast
simplification occurs if the contrast is very small because then we can set it to zero
in similar manner we might represent function in vj by component in vj plus the component in the orthogonal complement wj of vj to vj written as vj vj wj
the component in wj represents detail and we might wish to set some elements of this component to zero
it is easy to see that the functions
the top panel shows an nmr signal with the wavelet shrunk version superimposed in green
the lower left panel represents the wavelet transform of the original signal down to using the symmlet basis
each coefficient is represented by the height positive or negative of the vertical bar
the lower right panel represents the wavelet coefficients after being shrunken using the waveshrink function in plus which implements the sureshrink method of wavelet adaptation of donoho and johnstone
the haar and symmlet father scaling wavelet and mother wavelet generated by the mother wavelet form an orthonormal basis for for the haar family
likewise form basis for wj
now vj vj wj vj wj wj so besides representing function by its level detail and level rough components the latter can be broken down to level detail and rough and so on
finally we get representation of the form vj wj
figure on page shows particular wavelets
notice that since these spaces are orthogonal all the basis functions are orthonormal
in fact if the domain is discrete with time points this is as far as we can go
there are basis elements at level and adding up we have total of elements in the wj and one in
this structured orthonormal basis allows for multiresolution analysis which we illustrate in the next section
while helpful for understanding the construction above the haar basis is often too coarse for practical purposes
fortunately many clever wavelet bases have been invented
figures and include the daubechies symmlet basis
more generally the symmlet family has support of consecutive intervals
the wider the support the more time the wavelet has to die to zero and so it can
wavelet smoothing achieve this more smoothly
note that the effective support seems to be much narrower
one implication is that any order polynomial over the times points is reproduced exactly in exercise
in this sense is equivalent to the null space of the smoothing spline penalty
the haar wavelets have one vanishing moment and can reproduce any constant function
the symmlet scaling functions are one of many families of wavelet generators
adaptive wavelet filtering wavelets are particularly useful when the data are measured on uniform lattice such as discretized signal image or time series
we will focus on the one dimensional case and having lattice points is convenient
suppose is the response vector and is the orthonormal wavelet basis matrix evaluated at the uniformly spaced observations
then wt is called the wavelet transform of and is the full least squares regression coefficient
popular method for adaptive wavelet fitting is known as sure shrinkage stein unbiased risk estimation donoho and johnstone
we start with the criterion min bb which is the same as the lasso criterion in chapter
because is orthonormal this leads to the simple solution sign yj yj bb
the least squares coefficients are translated toward zero and truncated at zero
the fitted function vector is then given by the inverse wavelet transform
basis expansions and regularization simple choice for bb is bb log where is an estimate of the standard deviation of the noise
we can give some motivation for this choice
since is an orthonormal transformation if the elements of are white noise independent gaussian variates with mean and variance then so are
furthermore if random variables zn are white noise the expected maximum of zv is approximately log
hence all coefficients below log are likely to be noise and are set to zero
the space could be any basis of orthonormal functions polynomials natural splines or cosinusoids
what makes wavelets special is the particular form of basis functions used which allows for representation localized in time and in frequency
let's look again at the nmr signal of figure
the wavelet transform was computed using symmlet basis
notice that the coefficients do not descend all the way to but stop at which has basis functions
as we ascend to each level of detail the coefficients get smaller except in locations where spiky behavior is present
the wavelet coefficients represent characteristics of the signal localized in time the basis functions at each level are translations of each other and localized in frequency
each dilation increases the detail by factor of two and in this sense corresponds to doubling the frequency in traditional fourier representation
in fact more mathematical understanding of wavelets reveals that the wavelets at particular scale have fourier transform that is restricted to limited range or octave of frequencies
the shrinking truncation in the right panel was achieved using the sure approach described in the introduction to this section
the orthonormal basis matrix has columns which are the wavelet basis functions evaluated at the time points
in particular in this case there will be columns corresponding to the and the remainder devoted to the
in practice bb depends on the noise variance and has to be estimated from the data such as the variance of the coefficients at the highest level
early versions of sure shrinkage treated all scales equally
the wavelets function waveshrink has many options some of which allow for differential shrinkage
exercises more generally smoothing splines achieve compression of the original signal by imposing smoothness while wavelets impose sparsity
figure compares wavelet fit using sure shrinkage to smoothing spline fit using cross validation on two examples different in nature
for the nmr data in the upper panel the smoothing spline introduces detail everywhere in order to capture the detail in the isolated spikes the wavelet fit nicely localizes the spikes
in the lower panel the true function is smooth and the noise is relatively high
the wavelet fit has let in some additional and unnecessary wiggles price it pays in variance for the additional adaptivity
the wavelet transform is not performed by matrix multiplication as in wt
in fact using clever pyramidal schemes can be obtained in computations which is even faster than the log of the fast fourier transform fft
while the general construction is beyond the scope of this book it is easy to see for the haar basis exercise
likewise the inverse wavelet transform is also
this has been very brief glimpse of this vast and growing field
there is very large mathematical and computational base built on wavelets
modern image compression is often performed using two dimensional wavelet representations
bibliographic notes splines and splines are discussed in detail in de boor
green and silverman and wahba give thorough treatment of smoothing splines and thin plate splines the latter also covers reproducing kernel hilbert spaces
see also girosi et al and evgeniou et al for connections between many nonparametric regression techniques using rkhs approaches
modeling functional data as in section is covered in detail in ramsay and silverman
daubechies is classic and mathematical treatment of wavelets
other useful sources are chui and wickerhauser
donoho and johnstone developed the sure shrinkage and selection technology from statistical estimation framework see also vidakovic
bruce and gao is useful applied introduction which also describes the wavelet software in plus
exercises ex
show that the truncated power basis functions in represent basis for cubic spline with the two knots as indicated
wavelet smoothing compared with smoothing splines on two examples
each panel compares the sure shrunk wavelet fit to the cross validated smoothing spline fit
exercises ex
suppose that bi is an order spline defined in the appendix on page through the sequence show by induction that bi for
this shows for example that the support of cubic splines is at most knots show by induction that bi for
the splines are positive in the interior of their support
pk show by induction that bi be be show that bi is piecewise polynomial of order degree on be be with breaks only at the knots be be show that an order spline basis function is the density function of convolution of uniform random variables
write program to reproduce figure on page
consider the truncated power series representation for cubic splines with interior knots
let be
prove that the natural boundary conditions for natural cubic splines section imply the following linear constraints on the coefficients pk pkk be
hence derive the basis and
write program to classify the phoneme data using quadratic discriminant analysis section
since there are many correlated features you should filter them using smooth basis of natural cubic splines section
decide beforehand on series of five different choices for the number and position of the knots and use tenfold cross validation to make the final selection
the phoneme data are available from the book website www stat stanford edu elemstatlearn
suppose you wish to fit periodic function with known period
describe how you could modify the truncated power series basis to achieve this goal
derivation of smoothing splines green and silverman
suppose that and that is the natural cubic spline interpolant to the pairs xi zi with xn
this is natural spline
basis expansions and regularization with knot at every xi being an dimensional space of functions we can determine the coefficients such that it interpolates the sequence zi exactly
let be any other differentiable function on that interpolates the pairs let
use integration by parts and the fact that is natural cubic spline to show that dx xj xj hence show that dt dt and that equality can only hold if is identically zero in consider the penalized least squares problem min yi xi bb dt use to argue that the minimizer must be cubic spline with knots at each of the xi
in the appendix to this chapter we show how the smoothing spline computations could be more efficiently carried out using dimensional basis of splines
describe slightly simpler scheme using dimensional spline basis defined on the interior knots
derive the reinsch form bb bb for the smoothing spline
derive an expression for var bb and bias bb
using the example create version of figure where the mean and several pointwise quantiles of bb are shown
prove that for smoothing spline the null space of is spanned by functions linear in
characterize the solution to the following problem min rss bb wi yi xi bb dt where the wi are observation weights
characterize the solution to the smoothing spline problem when the training data have ties in
exercises ex
you have fitted smoothing spline bb to sample of pairs xi yi
suppose you augment your original sample with the pair bb and refit describe the result
use this to derive the fold cross validation formula
derive the constraints on the in the thin plate spline expansion to guarantee that the penalty is finite
how else could one ensure that the penalty was finite
this exercise derives some of the results quoted in section
suppose satisfying the conditions and let hk
show that hk xi ihk xi hk xi xj ihk xi xj
pn if xi then xi xj suppose that with hk and orthogonal in hk to each of xi
show that yi xi bb yi xi bb with equality iff
consider the ridge regression problem and assume
assume you have kernel that computes the inner product pm hm hm derive on page in the text
how would you compute the matrices and given
hence show that is equivalent to show that bb where is the matrix of evaluations hm xi and hht the matrix of inner products xi xj
basis expansions and regularization show that xi and bb how would you modify your solution if
show how to convert the discrete eigen decomposition of in section to estimates of the eigenfunctions of
the wavelet function of the symmlet wavelet basis has vanishing moments up to order
show that this implies that polynomials of order are represented exactly in defined on page
show that the haar wavelet transform of signal of length can be computed in computations
appendix computations for splines in this appendix we describe the spline basis for representing polynomial splines
we also discuss their use in the computations of smoothing splines
splines before we can get started we need to augment the knot sequence defined in section
let be be and be be be two boundary knots which typically define the domain over which we wish to evaluate our spline
the actual values of these additional knots beyond the boundary are arbitrary and it is customary to make them all the same and equal to be and be respectively
denote by bi the ith spline basis function of order for the knot sequence
they are defined recursively in terms of divided
appendix computations for splines differences as follows bd if bi otherwise for
these are also known as haar basis functions bi bi bi for
thus with bi are the cubic spline basis functions for the knot sequence be
this recursion can be continued and will generate the spline basis for any order spline
figure shows the sequence of splines up to order four with knots at the points
since we have created some duplicate knots some care has to be taken to avoid division by zero
if we adopt the convention that bi if then by induction bi if

note also that in the construction above only the subset bi are required for the spline basis of order with knots be
to fully understand the properties of these functions and to show that they do indeed span the space of cubic splines for the knot sequence requires additional mathematical machinery including the properties of divided differences
exercise explores these issues
the scope of splines is in fact bigger than advertised here and has to do with knot duplication
if we duplicate an interior knot in the construction of the sequence above and then generate the spline sequence as before the resulting basis spans the space of piecewise polynomials with one less continuous derivative at the duplicated knot
in general if in addition to the repeated boundary knots we include the interior knot be rj times then the lowest order derivative to be discontinuous at be will be order rj
thus for cubic splines with no repeats rj and at each interior knot the third derivatives are discontinuous
repeating the jth knot three times leads to discontinuous st derivative repeating it four times leads to discontinuous zeroth derivative the function is discontinuous at be
this is exactly what happens at the boundary knots we repeat the knots times so the spline becomes discontinuous at the boundary knots undefined beyond the boundary
the local support of splines has important computational implications especially when the number of knots is large
least squares computations with observations and variables basis functions take flops floating point operations
if is some appreciable fraction of this leads to algorithms which becomes
the sequence of splines up to order four with ten knots evenly spaced from to
the splines have local support they are nonzero on an interval spanned by knots
appendix computations for splines unacceptable for large
if the observations are sorted the regression matrix consisting of the spline basis functions evaluated at the points has many zeros which can be exploited to reduce the computational complexity back to
we take this up further in the next section
computations for smoothing splines although natural splines section provide basis for smoothing to operate in the larger space splines it is computationally more convenient of unconstrained splines
we write bj where are coefficients and the bj are the cubic spline basis functions
the solution looks the same as before bt bb bt except now the matrix is replaced by the matrix and similarly the penalty matrix replaces the dimensional
although at face value it seems that there are no boundary derivative constraints it turns out that the penalty term automatically imposes them by giving effectively infinite weight to any non zero derivative beyond the boundary
in practice is restricted to linear subspace for which the penalty is always finite
since the columns of are the evaluated splines in order from left to right and evaluated at the sorted values of and the cubic splines have local support is lower banded
consequently the matrix bt bb is banded and hence its cholesky decomposition llt can be computed easily
one then solves llt bt by back substitution to give and hence the solution in operations
in practice when is large it is unnecessary to use all interior knots and any reasonable thinning strategy will save in computations and have negligible effect on the fit
for example the smooth spline function in splus uses an approximately logarithmic strategy if all knots are included but even at only knots are used
basis expansions and regularization
this is page printer opaque this kernel smoothing methods in this chapter we describe class of regression techniques that achieve flexibility in estimating the regression function over the domain irp by fitting different but simple model separately at each query point
this is done by using only those observations close to the target point to fit the simple model and in such way that the resulting estimated function is smooth in irp
this localization is achieved via weighting function or kernel bb xi which assigns weight to xi based on its distance from
the kernels bb are typically indexed by parameter bb that dictates the width of the neighborhood
these memory based methods require in principle little or no training all the work gets done at evaluation time
the only parameter that needs to be determined from the training data is bb
the model however is the entire training data set
we also discuss more general classes of kernel based techniques which tie in with structured methods in other chapters and are useful for density estimation and classification
the techniques in this chapter should not be confused with those associated with the more recent usage of the phrase kernel methods
in this chapter kernels are mostly used as device for localization
we discuss kernel methods in sections and chapter in those contexts the kernel computes an inner product in high dimensional implicit feature space and is used for regularized nonlinear modeling
we make some connections to the methodology in this chapter at the end of section
in each panel pairs xi yi are generated at random from the blue curve with gaussian errors sin
in the left panel the green curve is the result of nearest neighbor running mean smoother
the red point is the fitted constant and the red circles indicate those observations contributing to the fit at
the solid yellow region indicates the weights assigned to observations
in the right panel the green curve is the kernel weighted average using an epanechnikov kernel with half window width bb
one dimensional kernel smoothers in chapter we motivated the nearest neighbor average ave yi xi nk as an estimate of the regression function
here nk is the set of points nearest to in squared distance and ave denotes the average mean
the idea is to relax the definition of conditional expectation as illustrated in the left panel of figure and compute an average in neighborhood of the target point
in this case we have used the nearest neighborhood the fit at is the average of the pairs whose xi values are closest to
the green curve is traced out as we apply this definition at different values
the green curve is bumpy since is discontinuous in
as we move from left to right the nearest neighborhood remains constant until point xi to the right of becomes closer than the furthest point xi in the neighborhood to the left of at which time xi replaces xi
the average in changes in discrete way leading to discontinuous
this discontinuity is ugly and unnecessary
rather than give all the points in the neighborhood equal weight we can assign weights that die off smoothly with distance from the target point
the right panel shows an example of this using the so called nadaraya watson kernel weighted
one dimensional kernel smoothers average pn bb xi yi pi bb xi with the epanechnikov quadratic kernel bb bb with bd if otherwise
the fitted function is now continuous and quite smooth in the right panel of figure
as we move the target from left to right points enter the neighborhood initially with weight zero and then their contribution slowly increases see exercise
in the right panel we used metric window size bb for the kernel fit which does not change as we move the target point while the size of the nearest neighbor smoothing window adapts to the local density of the xi
one can however also use such adaptive neighborhoods with kernels but we need to use more general notation
let bb be width function indexed by bb that determines the width of the neighborhood at
then more generally we have bb
bb in bb bb is constant
for nearest neighborhoods the neighborhood size replaces bb and we have hk where is the kth closest xi to
large bb implies lower variance averages over more observations but higher bias we essentially assume the true function is constant within the window
nearest neighbor window widths exhibit the opposite behavior the variance stays constant and the absolute bias varies inversely with local density
with most smoothing techniques one can simply reduce the data set by averaging the yi at tied values of and supplementing these new observations at the unique values of xi with an additional weight wi which multiples the kernel weight
comparison of three popular kernels for local smoothing
each has been calibrated to integrate to
the tri cube kernel is compact and has two continuous derivatives at the boundary of its support while the epanechnikov kernel has none
the gaussian kernel is continuously differentiable but has infinite support
operationally we simply multiply them by the kernel weights before computing the weighted average
with nearest neighborhoods it to insist on neighborhoods with total weight content is now natural relative to wi
in the event of overflow the last observation needed in neighborhood has weight wj which causes the sum of weights to exceed the budget then fractional parts can be used
the metric neighborhoods tend to contain less points on the boundaries while the nearest neighborhoods get wider
another popular compact kernel is based on the tri cube function bd if otherwise this is flatter on the top like the nearest neighbor box and is differentiable at the boundary of its support
the gaussian density function is popular noncompact kernel with the standarddeviation playing the role of the window size
figure compares the three
local linear regression we have progressed from the raw moving average to smoothly varying locally weighted average by using kernel weighting
the smooth kernel fit still has problems however as exhibited in figure left panel
locallyweighted averages can be badly biased on the boundaries of the domain
the locally weighted average has bias problems at or near the boundaries of the domain
the true function is approximately linear here but most of the observations in the neighborhood have higher mean than the target point so despite weighting their mean will be biased upwards
by fitting locally weighted linear regression right panel this bias is removed to first order because of the asymmetry of the kernel in that region
by fitting straight lines rather than constants locally we can remove this bias exactly to first order see figure right panel
actually this bias can be present in the interior of the domain as well if the values are not equally spaced for the same reasons but usually less severe
again locally weighted linear regression will make first order correction
locally weighted regression solves separate weighted least squares problem at each target point min bb xi yi xi
the estimate is then
notice that although we fit an entire linear model to the data in the region we only use it to evaluate the fit at the single point
define the vector valued function
let be the regression matrix with ith row xi and the diagonal matrix with ith diagonal element bb xi
then bt bt li yi
equation gives an explicit expression for the local linear regression estimate and highlights the fact that the estimate is linear in the
the green points show the equivalent kernel li for local regression
these are the weights in pn li yi plotted against their corresponding xi
for display purposes these have been rescaled since in fact they sum to
since the yellow shaded region is the rescaled equivalent kernel for the nadaraya watson local average we see how local regression automatically modifies the weighting kernel to correct for biases due to asymmetry in the smoothing window yi the li do not involve
these weights li combine the weighting kernel bb and the least squares operations and are sometimes referred to as the equivalent kernel
figure illustrates the effect of local linear regression on the equivalent kernel
historically the bias in the nadaraya watson and other local average kernel methods were corrected by modifying the kernel
these modifications were based on theoretical asymptotic mean square error considerations and besides being tedious to implement are only approximate for finite sample sizes
local linear regression automatically modifies the kernel to correct the bias exactly to first order phenomenon dubbed as automatic kernel carpentry
consider the following expansion for ef using the linearity of local regression and series expansion of the true function around ef li xi li xi li xi li where the remainder term involves thirdand higher order derivatives of and is typically small under suitable smoothness assumptions
it can be
local linear fits exhibit bias in regions of curvature of the true function
local quadratic fits tend to eliminate this bias
pn shown exercise that for local linear regression li and pn xi li
hence the middle term equals and since the bias is ef we see that it depends only on quadratic and higher order terms in the expansion of
local polynomial regression why stop at local linear fits
we can fit local polynomial fits of any degree ee xn min bb xi yi xji fb pd with solution xj
in fact an expansion such as will tell us that the bias will only have components of degree and higher exercise
figure illustrates local quadratic regression
local linear fits tend to be biased in regions of curvature of the true function phenomenon referred to as trimming the hills and filling the valleys
local quadratic regression is generally able to correct this bias
there is of course price to be paid for this bias reduction and that is increased variance
the fit in the right panel of figure is slightly more wiggly especially in the tails
assuming the model yi xi with independent and identically distributed with mean zero and variance var where is the vector of equivalent kernel weights at
it can be shown exercise that increases with and so there is bias variance tradeoff in selecting the polynomial degree
figure illustrates these variance curves for degree zero one and two
the variances functions for local constant linear and quadratic regression for metric bandwidth bb tri cube kernel local polynomials
local quadratic fits do little at the boundaries for bias but increase the variance lot
this is largely due to the fact that asymptotically the mse is dominated by boundary effects
while it may be helpful to tinker and move from local linear fits at the boundary to local quadratic fits in the interior we do not recommend such strategies
usually the application will dictate the degree of the fit
for example if we are interested in extrapolation then the boundary is of more interest and local linear fits are probably more reliable
equivalent kernels for local linear regression smoother tri cube kernel orange and smoothing spline blue with matching degrees of freedom
the vertical spikes indicates the target points
the bias will tend to be small again because each of the yi xi should be close to
the bias will be higher because we are now using observations xi further from and there is no guarantee that xi will be close to
similar arguments apply to local regression estimates say local linear as the width goes to zero the estimates approach piecewise linear function that interpolates the training data as the width gets infinitely large the fit approaches the global linear least squares fit to the data
the discussion in chapter on selecting the regularization parameter for smoothing splines applies here and will not be repeated
local regression smoothers are linear estimators the smoother matrix in bb is built up from the equivalent kernels and has ijth entry bb ij li xj
leaveone out cross validation is particularly simple exercise as is generalized cross validation cp exercise and fold cross validation
the effective degrees of freedom is again defined as trace bb and can be used to calibrate the amount of smoothing
figure compares the equivalent kernels for smoothing spline and local linear regression
the local regression smoother has span of which results in df trace bb
the smoothing spline was calibrated to have the same df and their equivalent kernels are qualitatively quite similar
with uniformly spaced with irregularly spaced the behavior can deteriorate
kernel smoothing methods local regression in irp kernel smoothing and local regression generalize very naturally to two or more dimensions
the nadaraya watson kernel smoother fits constant locally with weights supplied by dimensional kernel
local linear regression will fit hyperplane locally in by weighted least squares with weights supplied by dimensional kernel
it is simple to implement and is generally preferred to the local constant fit for its superior performance on the boundaries
let be vector of polynomial terms in of maximum degree
for example with and we get with we get and trivially with we get
at each irp solve min bb xi yi xi to produce the fit
typically the kernel will be radial function such as the radial epanechnikov or tri cube kernel bb bb where is the euclidean norm
since the euclidean norm depends on the units in each coordinate it makes most sense to standardize each predictor for example to unit standard deviation prior to smoothing
while boundary effects are problem in one dimensional smoothing they are much bigger problem in two or higher dimensions since the fraction of points on the boundary is larger
in fact one of the manifestations of the curse of dimensionality is that the fraction of points close to the boundary increases to one as the dimension grows
directly modifying the kernel to accommodate two dimensional boundaries becomes very messy especially for irregular boundaries
local polynomial regression seamlessly performs boundary correction to the desired order in any dimensions
figure illustrates local linear regression on some measurements from an astronomical study with an unusual predictor design star shaped
here the boundary is extremely irregular and the fitted surface must also interpolate over regions of increasing data sparsity as we approach the boundary
local regression becomes less useful in dimensions much higher than two or three
we have discussed in some detail the problems of dimensionality for example in chapter
it is impossible to simultaneously maintain localness low bias and sizable sample in the neighborhood low variance as the dimension increases without the total sample size increasing exponentially in
visualization of also becomes difficult in higher dimensions and this is often one of the primary goals of smoothing
the left panel shows three dimensional data where the response is the velocity measurements on galaxy and the two predictors record positions on the celestial sphere
the unusual star shaped design indicates the way the measurements were made and results in an extremely irregular boundary
the right panel shows the results of local linear regression smoothing in ir using nearest neighbor window with of the data
although the scatter cloud and wire frame pictures in figure look attractive it is quite difficult to interpret the results except at gross level
from data analysis perspective conditional plots are far more useful
figure shows an analysis of some environmental data with three predictors
the trellis display here shows ozone as function of radiation conditioned on the other two variables temperature and wind speed
however conditioning on the value of variable really implies local to that value as in local regression
above each of the panels in figure is an indication of the range of values present in that panel for each of the conditioning values
in the panel itself the data subsets are displayed response versus remaining variable and one dimensional local linear regression is fit to the data
although this is not quite the same as looking at slices of fitted three dimensional surface it is probably more useful in terms of understanding the joint behavior of the data
structured local regression models in irp when the dimension to sample size ratio is unfavorable local regression does not help us much unless we are willing to make some structural assumptions about the model
much of this book is about structured regression and classification models
here we focus on some approaches directly related to kernel methods
three dimensional smoothing example
the response is cube root of ozone concentration and the three predictors are temperature wind speed and radiation
the trellis display shows ozone as function of radiation conditioned on intervals of temperature and wind speed indicated by darker green or orange shaded bars
each panel contains about of the range of each of the conditioned variables
the curve in each panel is univariate local linear regression fit to the data in the panel
structured local regression models in irp structured kernels one line of approach is to modify the kernel
the default spherical kernel gives equal weight to each coordinate and so natural default strategy is to standardize each variable to unit standard deviation
more general approach is to use positive semidefinite matrix to weigh the different coordinates bb
bb entire coordinates or directions can be downgraded or omitted by imposing appropriate restrictions on
for example if is diagonal then we can increase or decrease the influence of individual predictors xj by increasing or decreasing ajj
often the predictors are many and highly correlated such as those arising from digitized analog signals or images
the covariance function of the predictors can be used to tailor metric that focuses less say on high frequency contrasts exercise
proposals have been made for learning the parameters for multidimensional kernels
for example the projection pursuit regression model discussed in chapter is of this flavor where low rank versions of imply ridge functions for
more general models for are cumbersome and we favor instead the structured forms for the regression function discussed next
structured regression functions we are trying to fit regression function xp in irp in which every level of interaction is potentially present
it is natural to consider analysis of variance anova decompositions of the form xp gj xj gk xk and then introduce structure by eliminating some of the higher order pp terms
additive models assume only main effect terms gj xj second order models will have terms with interactions of order at most two and so on
in chapter we describe iterative backfitting algorithms for fitting such low order interaction models
in the additive model for example if all but the kthpterm is assumed known then we can estimate gk by local regression of gj xj on xk
this is done for each function in turn repeatedly until convergence
the important detail is that at any stage one dimensional local regression is all that is needed
the same ideas can be used to fit low dimensional anova decompositions
an important special case of these structured models are the class of varying coefficient models
suppose for example that we divide the predictors in into set xq with and the remainder of
in each panel the aorta diameter is modeled as linear function of age
the coefficients of this model vary with gender and depth down the aorta left is near the top right is low down
there is clear trend in the coefficients of the linear model the variables we collect in the vector
we then assume the conditionally linear model xq
for given this is linear model but each of the coefficients can vary with
it is natural to fit such model by locally weighted least squares min bb zi yi xqi
figure illustrates the idea on measurements of the human aorta
longstanding claim has been that the aorta thickens with age
here we model the diameter of the aorta as linear function of age but allow the coefficients to vary with gender and depth down the aorta
we used local regression model separately for males and females
while the aorta clearly does thicken with age at the higher regions of the aorta the relationship fades with distance down the aorta
figure shows the intercept and slope as function of depth
the intercept and slope of age as function of distance down the aorta separately for males and females
the yellow bands indicate one standard error
local likelihood and other models the concept of local regression and varying coefficient models is extremely broad any parametric model can be made local if the fitting method accommodates observation weights
we can model more flexibly by using the likelihood local to for inference of xt bb xi yi xti many likelihood models in particular the family of generalized linear models including logistic and log linear models involve the covariates in linear fashion
local likelihood allows relaxation from globally linear model to one that is locally linear
this will fit varying coefficient model by maximizing the local likelihood
denoting the lag set by zt yt yt yt the model looks like standard linear model yt ztt and is typically fit by least squares
fitting by local least squares with kernel zt allows the model to vary according to the short term history of the series
this is to be distinguished from the more traditional dynamic linear models that vary by windowing time
as an illustration of local likelihood we consider the local version of the multiclass linear logistic regression model of chapter
the data consist of features xi and an associated categorical response gi and the linear model has the form pr pj

each plot shows the binary response chd coronary heart disease as function of risk factor for the south african heart disease data
for each plot we have computed the fitted prevalence of chd using local linear logistic regression model
the unexpected increase in the prevalence of chd at the lower ends of the ranges is because these are retrospective data and some of the subjects had already undergone treatment to reduce their blood pressure and weight
the shaded region in the plot indicates an estimated pointwise standard error band
this model can be used for flexible multiclass classification in moderately low dimensions although successes have been reported with the highdimensional zip code classification problem
generalized additive models chapter using kernel smoothing methods are closely related and avoid dimensionality problems by assuming an additive structure for the regression function
as simple illustration we fit two class local linear logistic model to the heart disease data of chapter
figure shows the univariate local logistic models fit to two of the risk factors separately
this is useful screening device for detecting nonlinearities when the data themselves have little visual information to offer
in this case an unexpected anomaly is uncovered in the data which may have gone unnoticed with traditional methods
since chd is binary indicator we could estimate the conditional prevalence pr by simply smoothing this binary response directly without resorting to likelihood formulation
this amounts to fitting locally constant logistic regression model exercise
in order to enjoy the biascorrection of local linear smoothing it is more natural to operate on the unrestricted logit scale
typically with logistic regression we compute parameter estimates as well as their standard errors
this can be done locally as well and so
kernel density estimate for systolic blood pressure for the chd group
the density estimate at each point is the average contribution from each of the kernels at that point
we have scaled the kernels down by factor of to make the graph readable we can produce as shown in the plot estimated pointwise standard error bands about our fitted prevalence
kernel density estimation and classification kernel density estimation is an unsupervised learning procedure which historically precedes kernel regression
it also leads naturally to simple family of procedures for nonparametric classification
kernel density estimation suppose we have random sample xn drawn from probability density fx and we wish to estimate fx at point
for simplicity we assume for now that ir
arguing as before natural local estimate has the form xi bb where is small metric neighborhood around of width bb
this estimate is bumpy and the smooth parzen estimate is preferred bb xi bb
the left panel shows the two separate density estimates for systolic blood pressure in the chd versus no chd groups using gaussian kernel density estimate in each
the right panel shows the estimated posterior probabilities for chd using because it counts observations close to with weights that decrease with distance from
in this case popular choice for bb is the gaussian kernel bb bb
figure shows gaussian kernel density fit to the sample values for systolic blood pressure for the chd group
letting bb denote the gaussian density with mean zero and standard deviation bb then has the form bb xi bb the convolution of the sample empirical distribution with bb
the distribution puts mass at each of the observed xi and is jumpy in we have smoothed by adding independent gaussian noise to each observation xi
the parzen density estimate is the equivalent of the local average and improvements have been proposed along the lines of local regression on the log scale for densities see loader
we will not pursue these here
in irp the natural generalization of the gaussian density estimate amounts to using the gaussian product kernel in xn xi bb
bb
the population class densities may have interesting structure left that disappears when the posterior probabilities are formed right
kernel density classification one can use nonparametric density estimates for classification in straightforward fashion using bayes theorem
suppose for class problem we fit nonparametric density estimates separately in each of the classes and we also have estimates of the class priors usually the sample proportions
then pj
fk figure uses this method to estimate the prevalence of chd for the heart risk factor study and should be compared with the left panel of figure
the main difference occurs in the region of high sbp in the right panel of figure
in this region the data are sparse for both classes and since the gaussian kernel density estimates use metric kernels the density estimates are low and of poor quality high variance in these regions
the local logistic regression method uses the tri cube kernel with nn bandwidth this effectively widens the kernel in this region and makes use of the local linear assumption to smooth out the estimate on the logit scale
if classification is the ultimate goal then learning the separate class densities well may be unnecessary and can in fact be misleading
figure shows an example where the densities are both multimodal but the posterior ratio is quite smooth
in learning the separate densities from data one might decide to settle for rougher high variance fit to capture these features which are irrelevant for the purposes of estimating the posterior probabilities
in fact if classification is the ultimate goal then we need only to estimate the posterior well near the decision boundary for two classes this is the set pr
the naive bayes classifier this is technique that has remained popular over the years despite its name also known as idiot's bayes
it is especially appropriate when
kernel density estimation and classification the dimension of the feature space is high making density estimation unattractive
the naive bayes model assumes that given class the features xk are independent fj fjk xk
this is in fact generalization of the original naive bayes procedures which used univariate gaussians to represent these marginals
this provides seamless way of mixing variable types in feature vector
despite these rather optimistic assumptions naive bayes classifiers often outperform far more sophisticated alternatives
the reasons are related to figure although the individual class density estimates may be biased this bias might not hurt the posterior probabilities as much especially near the decision regions
in fact the problem may be able to withstand considerable bias for the savings in variance such naive assumption earns
starting from we can derive the logit transform using class as the base pr log log pr fj qp xk log qp fjk xk xk log log fjk xk xk this has the form of generalized additive model which is described in more detail in chapter
the models are fit in quite different ways though their differences are explored in exercise
the relationship between naive bayes and generalized additive models is analogous to that between linear discriminant analysis and logistic regression section
kernel smoothing methods radial basis functions and kernels pm functions are represented as expansions in basis functions in chapter hj
the art of flexible modeling using basis expansions consists of picking an appropriate family of basis functions and then controlling the complexity of the representation by selection regularization or both
some of the families of basis functions have elements that are defined locally for example splines are defined locally in ir
if more flexibility is desired in particular region then that region needs to be represented by more basis functions which in the case of splines translates to more knots
tensor products of ir local basis functions deliver basis functions local in irp
not all basis functions are local for example the truncated power bases for splines or the sigmoidal basis functions used in neural networks see chapter
the composed function can nevertheless show local behavior because of the particular signs and values of the coefficients causing cancellations of global effects
for example the truncated power basis has an equivalent spline basis for the same space of functions the cancellation is exact in this case
kernel methods achieve flexibility by fitting simple models in region local to the target point
localization is achieved via weighting kernel bb and individual observations receive weights bb xi
radial basis functions combine these ideas by treating the kernel functions bb be as basis functions
this leads to the model bb be be bb where each basis element is indexed by location or prototype parameter be and scale parameter bb
popular choice for is the standard gaussian density function
there are several approaches to learning the parameters bb be
for simplicity we will focus on least squares methods for regression and use the gaussian kernel
this criterion is nonconvex
gaussian radial basis functions in ir with fixed width can leave holes top panel
renormalized gaussian radial basis functions avoid this problem and produce basis functions similar in some respects to splines with multiple local minima and the algorithms for optimization are similar to those used for neural networks
given the former the estimation of the latter is simple least squares problem
often the kernel parameters bb and be are chosen in an unsupervised way using the distribution alone
one of the methods is to fit gaussian mixture density model to the training xi which provides both the centers be and the scales bb
other even more adhoc approaches use clustering methods to locate the prototypes be and treat bb bb as hyper parameter
the obvious drawback of these approaches is that the conditional distribution pr and in particular is having no say in where the action is concentrated
on the positive side they are much simpler to implement
while it would seem attractive to reduce the parameter set and assume constant value for bb bb this can have an undesirable side effect of creating holes regions of irp where none of the kernels has appreciable support as illustrated in figure upper panel
renormalized radial basis functions be bb hj pm be bb avoid this problem lower panel
the nadaraya watson kernel regression estimator in irp can be viewed as an expansion in renormalized radial basis functions pn yi pnk bb xi bb xi pn yi hi
kernel smoothing methods with basis function hi located at every observation and coefficients yi that is be xi yi
note the similarity between the expansion and the solution on page to the regularization problem induced by the kernel
radial basis functions form the bridge between the modern kernel methods and local fitting technology
mixture models for density estimation and classification the mixture model is useful tool for density estimation and can be viewed as kind of kernel method
the gaussian mixture model has the form xm m with mixing proportions and each gaussian density has mean m and covariance matrix
in general mixture models can use any component densities in place of the gaussian in the gaussian mixture model is by far the most popular
the parameters are usually fit by maximum likelihood using the em algorithm as described in chapter
using bayes theorem separate mixture densities in each class lead to flexible models for pr this is taken up in some detail in chapter
figure shows an application of mixtures to the heart disease riskfactor study
in the top row are histograms of age for the no chd and chd groups separately and then combined on the right
using the combined data we fit two component mixture of the form with the scalars and not constrained to be equal
fitting was done via the em algorithm chapter note that the procedure does not use knowledge of the chd labels
the resulting estimates were
the component densities and are shown in the lowerleft and middle panels
the lower right panel shows these component densities orange and blue along with the estimated mixture density green
application of mixtures to the heart disease risk factor study
top row histograms of age for the no chd and chd groups separately and combined
bottom row estimated component densities from gaussian mixture model bottom left bottom middle bottom right estimated component densities blue and orange along with the estimated mixture density green
the orange density has very large standard deviation and approximates uniform density
the mixture model also provides an estimate of the probability that observation belongs to component xi im pm xi where xi is age in our example
suppose we threshold each value and hence define
then we can compare the classification of each observation by chd and the mixture model mixture model chd no yes although the mixture model did not use the chd labels it has done fair job in discovering the two chd subpopulations
linear logistic regression using the chd as response achieves the same error rate when fit to these data using maximum likelihood section
kernel smoothing methods computational considerations kernel and local regression and density estimation are memory based methods the model is the entire training data set and the fitting is done at evaluation or prediction time
for many real time applications this can make this class of methods infeasible
the computational cost to fit at single observation is flops except in oversimplified cases such as square kernels
by comparison an expansion in basis functions costs for one evaluation and typically log
basis function methods have an initial cost of at least
the smoothing parameter bb for kernel methods are typically determined off line for example using cross validation at cost of flops
popular implementations of local regression such as the loess function in plus and and the locfit procedure loader use triangulation schemes to reduce the computations
they compute the fit exactly at carefully chosen locations and then use blending techniques to interpolate the fit elsewhere per evaluation
bibliographic notes there is vast literature on kernel methods which we will not attempt to summarize
rather we will point to few good references that themselves have extensive bibliographies
loader gives excellent coverage of local regression and likelihood and also describes state of the art software for fitting these models
fan and gijbels cover these models from more theoretical aspect
hastie and tibshirani discuss local regression in the context of additive modeling
silverman gives good overview of density estimation as does scott
exercises ex
show that the nadaraya watson kernel smooth with fixed metric bandwidth bb and gaussian kernel is differentiable
what can be said for the epanechnikov kernel
what can be said for the epanechnikov kernel with adaptive nearest neighbor bandwidth bb
pn ex
show that xi li for local linear regression
define pn bj xi li
show that for local polynomial regression of any degree including local constants
show that bj for all for local polynomial regression of degree
what are the implications of this on the bias
exercises ex
show that section increases with the degree of the local polynomial
suppose that the predictors arise from sampling relatively smooth analog curves at uniformly spaced abscissa values
denote by cov the conditional covariance matrix of the predictors and assume this does not change much with
discuss the nature of mahalanobis choice for the metric in
how does this compare with
how might you construct kernel that downweights high frequency components in the distance metric ignores them completely
show that fitting locally constant multinomial logit model of the form amounts to smoothing the binary response indicators for each class separately using nadaraya watson kernel smoother with kernel weights bb xi
suppose that all you have is software for fitting local regression but you can specify exactly which monomials are included in the fit
how could you use this software to fit varying coefficient model in some of the variables
derive an expression for the leave one out cross validated residual sum of squares for local polynomial regression
suppose that for continuous response and predictor we model the joint density of using multivariate gaussian kernel estimator
note that the kernel in this case would be the product kernel bb bb
show that the conditional mean derived from this estimate is nadaraya watson estimator
extend this result to classification by providing suitable kernel for the estimation of the joint distribution of continuous and discrete
explore the differences between the naive bayes model and generalized additive logistic regression model in terms of model assumptions and estimation
if all the variables xk are discrete what can you say about the corresponding gam
suppose we have samples generated from the model yi xi with independent and identically distributed with mean zero and variance the xi assumed fixed non random
we estimate using linear smoother local regression smoothing spline etc with smoothing parameter bb
thus the vector of fitted values is given by bb
consider the in sample prediction error pe bb bb xi
kernel smoothing methods for predicting new responses at the input values
show that the average squared residual on the training data asr bb is biased estimate optimistic for pe bb while bb asr bb trace bb is unbiased
show that for the gaussian mixture model the likelihood is maximized at and describe how
write computer program to perform local linear discriminant analysis
at each query point the training data receive weights bb xi from weighting kernel and the ingredients for the linear decision boundaries see section are computed by weighted averages
try out your program on the zipcode data and show the training and test errors for series of five pre chosen values of bb
the zipcode data are available from the book website www stat stanford edu elemstatlearn
this is page printer opaque this model assessment and selection introduction the generalization performance of learning method relates to its prediction capability on independent test data
assessment of this performance is extremely important in practice since it guides the choice of learning method or model and gives us measure of the quality of the ultimately chosen model
in this chapter we describe and illustrate the key methods for performance assessment and show how they are used to select models
we begin the chapter with discussion of the interplay between bias variance and model complexity
bias variance and model complexity figure illustrates the important issue in assessing the ability of learning method to generalize
consider first the case of quantitative or interval scale response
we have target variable vector of inputs and prediction model that has been estimated from training set
the loss function for measuring errors between and is denoted by
typical choices are squared error absolute error
behavior of test sample and training sample error as the model complexity is varied
the light blue curves show the training error err while the light red curves show the conditional test error errt for training sets of size each as the model complexity is increased
the solid curves show the expected test error err and the expected training error err
test error also referred to as generalization error is the prediction error over an independent test sample errt where both and are drawn randomly from their joint distribution population
here the training set is fixed and test error refers to the error for this specific training set
related quantity is the expected prediction error or expected test error err errt
note that this expectation averages over everything that is random including the randomness in the training set that produced
figure shows the prediction error light red curves errt for simulated training sets each of size
the lasso section was used to produce the sequence of fits
the solid red curve is the average and hence an estimate of err
estimation of errt will be our goal although we will see that err is more amenable to statistical analysis and most methods effectively estimate the expected error
it does not seem possible to estimate conditional
bias variance and model complexity error effectively given only the information in the same training set
some discussion of this point is given in section
training error is the average loss over the training sample err yi xi
we would like to know the expected test error of our estimated model
as the model becomes more and more complex it uses the training data more and is able to adapt to more complicated underlying structures
hence there is decrease in bias but an increase in variance
there is some intermediate model complexity that gives minimum expected test error
unfortunately training error is not good estimate of the test error as seen in figure
training error consistently decreases with model complexity typically dropping to zero if we increase the model complexity enough
however model with zero training error is overfit to the training data and will typically generalize poorly
the story is similar for qualitative or categorical response taking one of values in set labeled for convenience as
typically we model the probabilities pk pr or some monotone transformations fk and then arg maxk
in some cases such as nearest neighbor classification chapters and we produce directly
typical loss functions are loss log log log likelihood
the quantity the log likelihood is sometimes referred to as the deviance
again test error here is errt the population misclassification error of the classifier trained on and err is the expected misclassification error
training error is the sample analogue for example err log gi xi the sample log likelihood for the model
the log likelihood can be used as loss function for general response densities such as the poisson gamma exponential log normal and others
if pr is the density of indexed by parameter that depends on the predictor then log pr
model assessment and selection the in the definition makes the log likelihood loss for the gaussian distribution match squared error loss
for ease of exposition for the remainder of this chapter we will use and to represent all of the above situations since we focus mainly on the quantitative response squared error loss setting
for the other situations the appropriate translations are obvious
in this chapter we describe number of methods for estimating the expected test error for model
typically our model will have tuning parameter or parameters and so we can write our predictions as
the tuning parameter varies the complexity of our model and we wish to find the value of that minimizes error that is produces the minimum of the average test error curve in figure
having said this for brevity we will often suppress the dependence of on
it is important to note that there are in fact two separate goals that we might have in mind model selection estimating the performance of different models in order to choose the best one
model assessment having chosen final model estimating its prediction error generalization error on new data
if we are in data rich situation the best approach for both problems is to randomly divide the dataset into three parts training set validation set and test set
the training set is used to fit the models the validation set is used to estimate prediction error for model selection the test set is used for assessment of the generalization error of the final chosen model
ideally the test set should be kept in vault and be brought out only at the end of the data analysis
suppose instead that we use the test set repeatedly choosing the model with smallest test set error
then the test set error of the final chosen model will underestimate the true test error sometimes substantially
it is difficult to give general rule on how to choose the number of observations in each of the three parts as this depends on the signal tonoise ratio in the data and the training sample size
typical split might be for training and each for validation and testing train validation test the methods in this chapter are designed for situations where there is insufficient data to split it into three parts
again it is too difficult to give general rule on how much training data is enough among other things this depends on the signal to noise ratio of the underlying function and the complexity of the models being fit to the data
the bias variance decomposition the methods of this chapter approximate the validation step either analytically aic bic mdl srm or by efficient sample re use crossvalidation and the bootstrap
besides their use in model selection we also examine to what extent each method provides reliable estimate of test error of the final chosen model
before jumping into these topics we first explore in more detail the nature of test error and the bias variance tradeoff
the bias variance decomposition as in chapter if we assume that where and var we can derive an expression for the expected prediction error of regression fit at an input point using squared error loss err ef ef bias var irreducible error bias variance
the first term is the variance of the target around its true mean and cannot be avoided no matter how well we estimate unless
the second term is the squared bias the amount by which the average of our estimate differs from the true mean the last term is the variance the expected squared deviation of around its mean
typically the more complex we make the model the lower the squared bias but the higher the variance
for the nearest neighbor regression fit these expressions have the simple form err
here we assume for simplicity that training inputs xi are fixed and the randomness arises from the yi
the number of neighbors is inversely related to the model complexity
for small the estimate can potentially adapt itself better to the underlying
as we increase the bias the squared difference between and the average of at the nearest neighbors will typically increase while the variance decreases
for linear model fit xt where the parameter vector with components is fit by least squares we have err
model assessment and selection ef
here xt the vector of linear weights that produce the fit xt xt and hence var
while this variance changes with its average with taken to be each of the sample values xi is and hence err xi xi ef xi the in sample error
here model complexity is directly related to the number of parameters
the test error err for ridge regression fit is identical in form to except the linear weights in the variance term are different xt
the bias term will also be different
for linear model family such as ridge regression we can break down the bias more finely
let denote the parameters of the best fitting linear approximation to arg min
here the expectation is taken with respect to the distribution of the input variables
then we can write the average squared bias as ex ef ex xt ex xt ext ave model bias ave estimation bias the first term on the right hand side is the average squared model bias the error between the best fitting linear approximation and the true function
the second term is the average squared estimation bias the error between the average estimate xt and the best fitting linear approximation
for linear models fit by ordinary least squares the estimation bias is zero
for restricted fits such as ridge regression it is positive and we trade it off with the benefits of reduced variance
the model bias can only be reduced by enlarging the class of linear models to richer collection of models by including interactions and transformations of the variables in the model
figure shows the bias variance tradeoff schematically
in the case of linear models the model space is the set of all linear predictions from inputs and the black dot labeled closest fit is xt
the blue shaded region indicates the error with which we see the truth in the training sample
also shown is the variance of the least squares fit indicated by the large yellow circle centered at the black dot labeled closest fit in population
schematic of the behavior of bias and variance
the model space is the set of all possible predictions from the model with the closest fit labeled with black dot
the model bias from the truth is shown along with the variance indicated by the large yellow circle centered at the black dot labeled closest fit in population
shrunken or regularized fit is also shown having additional estimation bias but smaller prediction error due to its decreased variance
model assessment and selection now if we were to fit model with fewer predictors or regularize the coefficients by shrinking them toward zero say we would get the shrunken fit shown in the figure
this fit has an additional estimation bias due to the fact that it is not the closest fit in the model space
on the other hand it has smaller variance
if the decrease in variance exceeds the increase in squared bias then this is worthwhile
example bias variance tradeoff figure shows the bias variance tradeoff for two simulated examples
there are observations and predictors uniformly distributed in the hypercube
the situations are as follows left panels is if and if and we apply nearest neighbors
right panels is if xj is greater than and otherwise and we use best subset linear regression of size
the top row is regression with squared error loss the bottom row is classification with loss
the figures show the prediction error red squared bias green and variance blue all computed for large test sample
in the regression problems bias and variance add to produce the prediction error curves with minima at about for nearest neighbors and for the linear model
for classification loss bottom figures some interesting phenomena can be seen
the bias and variance curves are the same as in the top figures and prediction error now refers to misclassification rate
we see that prediction error is no longer the sum of squared bias and variance
for the nearest neighbor classifier prediction error decreases or stays the same as the number of neighbors is increased to despite the fact that the squared bias is rising
for the linear model classifier the minimum occurs for as in regression but the improvement over the model is more dramatic
we see that bias and variance seem to interact in determining prediction error
why does this happen
there is simple explanation for the first phenomenon
suppose at given input point the true probability of class is while the expected value of our estimate is
then the squared bias is considerable but the prediction error is zero since we make the correct decision
in other words estimation errors that leave us on the right side of the decision boundary don't hurt
exercise demonstrates this phenomenon analytically and also shows the interaction effect between bias and variance
the overall point is that the bias variance tradeoff behaves differently for loss than it does for squared error loss
this in turn means that the best choices of tuning parameters may differ substantially in the two
expected prediction error orange squared bias green and variance blue for simulated example
the top row is regression with squared error loss the bottom row is classification with loss
the models are nearest neighbors left and best subset regression of size right
the variance and bias curves are the same in regression and classification but the prediction error curve is different
model assessment and selection settings
one should base the choice of tuning parameter on an estimate of prediction error as described in the following sections
optimism of the training error rate discussions of error rate estimation can be confusing because we have to make clear which quantities are fixed and which are random
before we continue we need few definitions elaborating on the material of section
given training set xn yn the generalization error of model is errt ex note that the training set is fixed in expression
the point is new test data point drawn from the joint distribution of the data
averaging over training sets yields the expected error err et ex which is more amenable to statistical analysis
as mentioned earlier it turns out that most methods effectively estimate the expected error rather than et see section for more on this point
now typically the training error err yi xi will be less than the true error errt because the same data is being used to fit the method and assess its error see exercise
fitting method typically adapts to the training data and hence the apparent or training error err will be an overly optimistic estimate of the generalization error errt
part of the discrepancy is due to where the evaluation points occur
the quantity errt can be thought of as extra sample error since the test input vectors don't need to coincide with the training input vectors
the nature of the optimism in err is easiest to understand when we focus instead on the in sample error errin ey yi xi the notation indicates that we observe new response values at each of the training points xi
we define the optimism as indeed in the first edition of our book this section wasn't sufficiently clear
optimism of the training error rate the difference between errin and the training error err op errin err
this is typically positive since err is usually biased downward as an estimate of prediction error
finally the average optimism is the expectation of the optimism over training sets ey op
here the predictors in the training set are fixed and the expectation is over the training set outcome values hence we have used the notation ey instead of et
we can usually estimate only the expected error rather than op in the same way that we can estimate the expected error err rather than the conditional error errt
for squared error and other loss functions one can show quite generally that cov yi where cov indicates covariance
thus the amount by which err underestimates the true error depends on how strongly yi affects its own prediction
the harder we fit the data the greater cov yi will be thereby increasing the optimism
exercise proves this result for squared error loss where is the fitted value from the regression
for loss is the classification at xi and for entropy loss is the fitted probability of class at xi
in summary we have the important relation ey errin ey err cov yi
this expression simplifies if is obtained by linear fit with inputs or basis functions
for example cov yi for the additive error model and so ey errin ey err
expression is the basis for the definition of the effective number of parameters discussed in section the optimism increases linearly with
model assessment and selection the number of inputs or basis functions we use but decreases as the training sample size increases
versions of hold approximately for other error models such as binary data and entropy loss
an obvious way to estimate prediction error is to estimate the optimism and then add it to the training error err
the methods described in the next section cp aic bic and others work in this way for special class of estimates that are linear in their parameters
in contrast cross validation and bootstrap methods described later in the chapter are direct estimates of the extra sample error err
these general tools can be used with any loss function and with nonlinear adaptive fitting techniques
in sample error is not usually of direct interest since future values of the features are not likely to coincide with their training set values
but for comparison between models in sample error is convenient and often leads to effective model selection
the reason is that the relative rather than absolute size of the error is what matters
estimates of in sample prediction error the general form of the in sample estimates is din err err where is an estimate of the average optimism
using expression applicable when parameters are fit under squared error loss leads to version of the so called cp statistic cp err
here is an estimate of the noise variance obtained from the meansquared error of low bias model
using this criterion we adjust the training error by factor proportional to the number of basis functions used
the akaike information criterion is similar but more generally applicable estimate of errin when log likelihood loss function is used
it relies on relationship similar to that holds asymptotically as log pr loglik
here pr is family of densities for containing the true density is the maximum likelihood estimate of and loglik is the maximized log likelihood xn loglik log pr yi
estimates of in sample prediction error for example for the logistic regression model using the binomial loglikelihood we have aic loglik
for the gaussian model with variance assumed known the aic statistic is equivalent to cp and so we refer to them collectively as aic
to use aic for model selection we simply choose the model giving smallest aic over the set of models considered
for nonlinear and other complex models we need to replace by some measure of model complexity
we discuss this in section
given set of models indexed by tuning parameter denote by err and the training error and number of parameters for each model
then for this set of models we define aic err
the function aic provides an estimate of the test error curve and we find the tuning parameter that minimizes it
our final chosen model is
note that if the basis functions are chosen adaptively no longer holds
for example if we have total of inputs and we choose the best fitting linear model with inputs the optimism will exceed
put another way by choosing the best fitting model with inputs the effective number of parameters fit is more than
figure shows aic in action for the phoneme recognition example of section on page
the input vector is the log periodogram of the spoken vowel quantized to uniformly spaced frequencies
linear logistic regression model pm is used to predict the phoneme class with coefficient function hm an expansion in spline basis functions
for any given basis of natural cubic splines is used for the hm with knots chosen uniformly over the range of frequencies so
using aic to select the number of basis functions will approximately minimize err for both entropy and loss
the simple formula cov yi holds exactly for linear models with additive errors and squared error loss and approximately for linear models and log likelihoods
in particular the formula does not hold in general for loss efron although many authors nevertheless use it in that context right panel of figure
aic used for model selection for the phoneme recognition example of section
the logistic regression coefficient function hm is modeled as an expansion in spline basis functions
in the left panel we see the aic statistic used to estimate errin using log likelihood loss
included is an estimate of err based on an independent test sample
it does well except for the extremely over parametrized case parameters for observations
in the right panel the same is done for loss
although the aic formula does not strictly apply here it does reasonable job in this case
the effective number of parameters the concept of number of parameters can be generalized especially to models where regularization is used in the fitting
suppose we stack the outcomes yn into vector and similarly for the predictions
then linear fitting method is one for which we can write sy where is an matrix depending on the input vectors xi but not on the yi
linear fitting methods include linear regression on the original features or on derived basis set and smoothing methods that use quadratic shrinkage such as ridge regression and cubic smoothing splines
then the effective number of parameters is defined as df trace the sum of the diagonal elements of also known as the effective degreesof freedom
note that if is an orthogonal projection matrix onto basis
the bayesian approach and bic set spanned by features then trace
it turns out that trace is exactly the correct quantity to replace as the number of parameters in the cp statistic
if arises from an additive error model pn with var then one can show that cov yi trace which motivates the more general definition pn cov yi df exercises and
section on page gives some more intuition for the definition df trace in the context of smoothing splines
for models like neural networks in which we minimizep an error function with weight decay penalty regularization wm the effective number of parameters has the form df where the are the eigenvalues of the hessian matrix wt
expression follows from if we make quadratic approximation to the error function at the solution bishop
the bayesian approach and bic the bayesian information criterion bic like aic is applicable in settings where the fitting is carried out by maximization of log likelihood
the generic form of bic is bic loglik log
the bic statistic times is also known as the schwarz criterion schwarz
under the gaussian model assuming the variance is known loglik equals up to constant yi xi which is err for squared error loss
hence we can write nh bic err log
therefore bic is proportional to aic cp with the factor replaced by log
assuming bic tends to penalize complex models more heavily giving preference to simpler models in selection
as with aic is typically estimated by the mean squared error of low bias model
for classification problems use of the multinomial log likelihood leads to similar relationship with the aic using cross entropy as the error measure
model assessment and selection note however that the misclassification error measure does not arise in the bic context since it does not correspond to the log likelihood of the data under any probability model
despite its similarity with aic bic is motivated in quite different way
it arises in the bayesian approach to model selection which we now describe
suppose we have set of candidate models mm and corresponding model parameters and we wish to choose best model from among them
assuming we have prior distribution pr mm for the parameters of each model mm the posterior probability of given model is pr mm pr mm pr mm pr mm pr mm pr mm where represents the training data xi yi
to compare two models mm and we form the posterior odds pr mm pr mm pr mm
pr pr pr if the odds are greater than one we choose model otherwise we choose model
the rightmost quantity pr mm bf pr is called the bayes factor the contribution of the data toward the posterior odds
typically we assume that the prior over models is uniform so that pr mm is constant
we need some way of approximating pr mm
so called laplace approximation to the integral followed by some other simplifications ripley page to gives dm log pr mm log pr mm log
here is maximum likelihood estimate and dm is the number of free parameters in model mm
if we define our loss function to be log pr mm this is equivalent to the bic criterion of equation
therefore choosing the model with minimum bic is equivalent to choosing the model with largest approximate posterior probability
but this framework gives us more
if we compute the bic criterion for set of
minimum description length models giving bicm then we can estimate the posterior probability of each model mm as bicm pm
bic thus we can estimate not only the best model but also assess the relative merits of the models considered
for model selection purposes there is no clear choice between aic and bic
bic is asymptotically consistent as selection criterion
what this means is that given family of models including the true model the probability that bic will select the correct model approaches one as the sample size
this is not the case for aic which tends to choose models which are too complex as
on the other hand for finite samples bic often chooses models that are too simple because of its heavy penalty on complexity
minimum description length the minimum description length mdl approach gives selection criterion formally identical to the bic approach but is motivated from an optimal coding viewpoint
we first review the theory of coding for data compression and then apply it to model selection
we think of our datum as message that we want to encode and send to someone else the receiver
we think of our model as way of encoding the datum and will choose the most parsimonious model that is the shortest code for the transmission
suppose first that the possible messages we might want to transmit are zm
our code uses finite alphabet of length for example we might use binary code of length
here is an example with four possible messages and binary coding message code this code is known as an instantaneous prefix code no code is the prefix of any other and the receiver who knows all of the possible codes knows exactly when the message has been completely sent
we restrict our discussion to such instantaneous prefix codes
one could use the coding in or we could permute the codes for example use codes for
how do we decide which to use
it depends on how often we will be sending each of the messages
if for example we will be sending most often it makes sense to use the shortest code for
using this kind of strategy shorter codes for more frequent messages the average message length will be shorter
model assessment and selection in general if messages are sent with probabilities pr zi famous theorem due to shannon says we should use code lengths li log pr zi and the average message length satisfies length pr zi log pr zi
the right hand side above is also called the entropy of the distribution pr zi
the inequality is an equality when the probabilities satisfy pi li
in our example if pr zi respectively then the coding shown in is optimal and achieves the entropy lower bound
in general the lower bound cannot be achieved but procedures like the huffmann coding scheme can get close to the bound
note that with an infinite set of messages the entropy is replaced by pr log pr dz
from this result we glean the following to transmit random variable having probability density function pr we require about log pr bits of information
we henceforth change notation from log pr to log pr loge pr this is for convenience and just introduces an unimportant multiplicative constant
now we apply this result to the problem of model selection
we have model with parameters and data consisting of both inputs and outputs
let the conditional probability of the outputs under the model be pr assume the receiver knows all of the inputs and we wish to transmit the outputs
then the message length required to transmit the outputs is length log pr log pr the log probability of the target values given the inputs
the second term is the average code length for transmitting the model parameters while the first term is the average code length for transmitting the discrepancy between the model and actual target values
for example suppose we have single target with parameter and no input for simplicity
then the message length is length constant log
note that the smaller is the shorter on average is the message length since is more concentrated around
the mdl principle says that we should choose the model that minimizes
we recognize as the negative log posterior distribution and hence minimizing description length is equivalent to maximizing posterior probability
hence the bic criterion derived as approximation to log posterior probability can also be viewed as device for approximate model choice by minimum description length
the solid curve is the function sin for
the green solid and blue hollow points illustrate how the associated indicator function sin can shatter separate an arbitrarily large number of points by choosing an appropriately high frequency
note that we have ignored the precision with which random variable is coded
with finite code length we cannot code continuous variable exactly
however if we code within tolerance the message length needed is the log of the probability in the interval which is well approximated by zpr if is small
since log zpr log log pr this means we can just ignore the constant log and use log pr as our measure of message length as we did above
the preceding view of mdl for model selection says that we should choose the model with highest posterior probability
however many bayesians would instead do inference by sampling from the posterior distribution
vapnik chervonenkis dimension difficulty in using estimates of in sample error is the need to specify the number of parameters or the complexity used in the fit
although the effective number of parameters introduced in section is useful for some nonlinear models it is not fully general
the vapnik chervonenkis vc theory provides such general measure of complexity and gives associated bounds on the optimism
here we give brief review of this theory
suppose we have class of functions indexed by parameter vector with irp
assume for now that is an indicator function that is takes the values or
if and is the linear indicator function then it seems reasonable to say that the complexity of the class is the number of parameters
but what about sin where is any real number and ir
the function sin is shown in figure
this is very wiggly function that gets even rougher as the frequency increases but it has only one parameter despite this it doesn't seem reasonable to conclude that it has less complexity than the linear indicator function in dimension
the first three panels show that the class of lines in the plane can shatter three points
the last panel shows that this class cannot shatter four points as no line will put the hollow points on one side and the solid points on the other
hence the vc dimension of the class of straight lines in the plane is three
note that class of nonlinear curves could shatter four points and hence has vc dimension greater than three
the vapnik chervonenkis dimension is way of measuring the complexity of class of functions by assessing how wiggly its members can be
the vc dimension of the class is defined to be the largest number of points in some configuration that can be shattered by members of
set of points is said to be shattered by class of functions if no matter how we assign binary label to each point member of the class can perfectly separate them
figure shows that the vc dimension of linear indicator functions in the plane is but not since no four points can be shattered by set of lines
in general linear indicator function in dimensions has vc dimension which is also the number of free parameters
on the other hand it can be shown that the family sin has infinite vc dimension as figure suggests
by appropriate choice of any set of points can be shattered by this class exercise
so far we have discussed the vc dimension only of indicator functions but this can be extended to real valued functions
the vc dimension of class of real valued functions is defined to be the vc dimension of the indicator class where takes values over the range of
one can use the vc dimension in constructing an estimate of extrasample prediction error different types of results are available
using the concept of vc dimension one can prove results about the optimism of the training error when using class of functions
an example of such result is the following
if we fit training points using class of functions having vc dimension then with probability at least over training
vapnik chervonenkis dimension sets err errt err binary classification err errt regression log log where and these bounds hold simultaneously for all members and are taken from cherkassky and mulier pages
they recommend the value
for regression they suggest and for classification they make no recommendation with and corresponding to worst case scenarios
they also give an alternative practical bound for regression
log errt err log with which is free of tuning constants
the bounds suggest that the optimism increases with and decreases with in qualitative agreement with the aic correction given is
however the results in are stronger rather than giving the expected optimism for each fixed function they give probabilistic upper bounds for all functions and hence allow for searching over the class
vapnik's structural risk minimization srm approach fits nested sequence of models of increasing vc dimensions and then chooses the model with the smallest value of the upper bound
we note that upper bounds like the ones in are often very loose but that doesn't rule them out as good criteria for model selection where the relative not absolute size of the test error is important
the main drawback of this approach is the difficulty in calculating the vc dimension of class of functions
often only crude upper bound for vc dimension is obtainable and this may not be adequate
an example in which the structural risk minimization program can be successfully carried out is the support vector classifier discussed in section
example continued figure shows the results when aic bic and srm are used to select the model size for the examples of figure
for the examples labeled knn the model index refers to neighborhood size while for those labeled reg refers to subset size
using each selection method aic we estimated the best model and found its true prediction error errt on test set
for the same training set we computed the prediction error of the best
boxplots show the distribution of the relative error errt min errt max errt min errt over the four scenarios of figure
this is the error in using the chosen model relative to the best model
there are training sets each of size represented in each boxplot with the errors computed on test sets of size
cross validation and worst possible model choices min errt and max errt
the boxplots show the distribution of the quantity errt min errt max errt min errt which represents the error in using the chosen model relative to the best model
for linear regression the model complexity was measured by the number of features as mentioned in section this underestimates the df since it does not charge for the search for the best model of that size
this was also used for the vc dimension of the linear classifier
for knearest neighbors we used the quantity
under an additive error regression model this can be justified as the exact effective degrees of freedom exercise we do not know if it corresponds to the vc dimension
we used for the constants in the results for srm changed with different constants and this choice gave the most favorable results
we repeated the srm selection using the alternative practical bound and got almost identical results
for misclassification error we used err for the least restrictive model for knn since results in zero training error
the aic criterion seems to work well in all four scenarios despite the lack of theoretical support with loss
bic does nearly as well while the performance of srm is mixed
cross validation probably the simplest and most widely used method for estimating prediction error is cross validation
this method directly estimates the expected extra sample error err the average generalization error when the method is applied to an independent test sample from the joint distribution of and
as mentioned earlier we might hope that cross validation estimates the conditional error with the training set held fixed
but as we will see in section cross validation typically estimates well only the expected prediction error
fold cross validation ideally if we had enough data we would set aside validation set and use it to assess the performance of our prediction model
since data are often scarce this is usually not possible
to finesse the problem fold crossvalidation uses part of the available data to fit the model and different part to test it
we split the data into roughly equal sized parts for example when the scenario looks like this
model assessment and selection train train validation train train for the kth part third above we fit the model to the other parts of the data and calculate the prediction error of the fitted model when predicting the kth part of the data
we do this for and combine the estimates of prediction error
here are more details
let ba be an indexing function that indicates the partition to which observation is allocated by the randomization
denote by the fitted function computed with the kth part of the data removed
then the cross validation estimate of prediction error is cv yi ba xi
typical choices of are or see below
the case is known as leave one out cross validation
in this case ba and for the ith observation the fit is computed using all the data except the ith
given set of models indexed by tuning parameter denote by the th model fit with the kth part of the data removed
then for this set of models we define cv yi ba xi
the function cv provides an estimate of the test error curve and we find the tuning parameter that minimizes it
our final chosen model is which we then fit to all the data
it is interesting to wonder about what quantity fold cross validation estimates
with or we might guess that it estimates the expected error err since the training sets in each fold are quite different from the original training set
on the other hand if we might guess that cross validation estimates the conditional error errt
it turns out that cross validation only estimates effectively the average error err as discussed in section
what value should we choose for
with the cross validation estimator is approximately unbiased for the true expected prediction error but can have high variance because the training sets are so similar to one another
the computational burden is also considerable requiring applications of the learning method
in certain special problems this computation can be done quickly see exercises and
hypothetical learning curve for classifier on given task plot of err versus the size of the training set
with dataset of observations fold cross validation would use training sets of size which would behave much like the full set
however with dataset of observations fivefold cross validation would use training sets of size and this would result in considerable overestimate of prediction error
on the other hand with say cross validation has lower variance
but bias could be problem depending on how the performance of the learning method varies with the size of the training set
figure shows hypothetical learning curve for classifier on given task plot of err versus the size of the training set
the performance of the classifier improves as the training set size increases to observations increasing the number further to brings only small benefit
if our training set had observations fivefold cross validation would estimate the performance of our classifier over training sets of size which from figure is virtually the same as the performance for training set size
thus cross validation would not suffer from much bias
however if the training set had observations fivefold cross validation would estimate the performance of our classifier over training sets of size and from the figure that would be an underestimate of err
hence as an estimate of err cross validation would be biased upward
to summarize if the learning curve has considerable slope at the given training set size fiveor tenfold cross validation will overestimate the true prediction error
whether this bias is drawback in practice depends on the objective
on the other hand leave one out cross validation has low bias but can have high variance
overall fiveor tenfold cross validation are recommended as good compromise see breiman and spector and kohavi
figure shows the prediction error and tenfold cross validation curve estimated from single training set from the scenario in the bottom right panel of figure
this is two class classification problem using lin
prediction error orange and tenfold cross validation curve blue estimated from single training set from the scenario in the bottom right panel of figure ear model with best subsets regression of subset size
standard error bars are shown which are the standard errors of the individual misclassification error rates for each of the ten parts
both curves have minima at although the cv curve is rather flat beyond
often one standard error rule is used with cross validation in which we choose the most parsimonious model whose error is no more than one standard error above the error of the best model
here it looks like model with about predictors would be chosen while the true model uses
generalized cross validation provides convenient approximation to leaveone out cross validation for linear fitting under squared error loss
as defined in section linear fitting method is one for which we can write sy
now for many linear fitting methods xh yi xi yi xi sii where sii is the ith diagonal element of see exercise
the gcv approximation is xi gcv
trace
cross validation the quantity trace is the effective number of parameters as defined in section
gcv can have computational advantage in some settings where the trace of can be computed more easily than the individual elements sii
in smoothing problems gcv can also alleviate the tendency of crossvalidation to undersmooth
the similarity between gcv and aic can be seen from the approximation exercise
the wrong and right way to do cross validation consider classification problem with large number of predictors as may arise for example in genomic or proteomic applications
typical strategy for analysis might be as follows
screen the predictors find subset of good predictors that show fairly strong univariate correlation with the class labels
using just this subset of predictors build multivariate classifier
use cross validation to estimate the unknown tuning parameters and to estimate the prediction error of the final model
is this correct application of cross validation
consider scenario with samples in two equal sized classes and quantitative predictors standard gaussian that are independent of the class labels
the true test error rate of any classifier is
we carried out the above recipe choosing in step the predictors having highest correlation with the class labels and then using nearest neighbor classifier based on just these predictors in step
over simulations from this setting the average cv error rate was
this is far lower than the true error rate of
what has happened
the problem is that the predictors have an unfair advantage as they were chosen in step on the basis of all of the samples
leaving samples out after the variables have been selected does not correctly mimic the application of the classifier to completely independent test set since these predictors have already seen the left out samples
figure top panel illustrates the problem
we selected the predictors having largest correlation with the class labels over all samples
then we chose random set of samples as we would do in five fold crossvalidation and computed the correlations of the pre selected predictors with the class labels over just these samples top panel
we see that the correlations average about rather than as one might expect
here is the correct way to carry out cross validation in this example
divide the samples into cross validation folds groups at random
for each fold
cross validation the wrong and right way histograms shows the correlation of class labels in randomly chosen samples with the predictors chosen using the incorrect upper red and correct lower green versions of cross validation find subset of good predictors that show fairly strong univariate correlation with the class labels using all of the samples except those in fold using just this subset of predictors build multivariate classifier using all of the samples except those in fold use the classifier to predict the class labels for the samples in fold
the error estimates from step are then accumulated over all folds to produce the cross validation estimate of prediction error
the lower panel of figure shows the correlations of class labels with the predictors chosen in step of the correct procedure over the samples in typical fold
we see that they average about zero as they should
in general with multistep modeling procedure cross validation must be applied to the entire sequence of modeling steps
in particular samples must be left out before any selection or filtering steps are applied
there is one qualification initial unsupervised screening steps can be done before samples are left out
for example we could select the predictors
cross validation with highest variance across all samples before starting cross validation
since this filtering does not involve the class labels it does not give the predictors an unfair advantage
while this point may seem obvious to the reader we have seen this blunder committed many times in published papers in top rank journals
with the large numbers of predictors that are so common in genomic and other areas the potential consequences of this error have also increased dramatically see ambroise and mclachlan for detailed discussion of this issue
does cross validation really work
we once again examine the behavior of cross validation in high dimensional classification problem
consider scenario with samples in two equal sized classes and quantitative predictors that are independent of the class labels
once again the true error rate of any classifier is
consider simple univariate classifier single split that minimizes the misclassification error stump
stumps are trees with single split and are used in boosting methods chapter
simple argument suggests that cross validation will not work properly in this setting fitting to the entire training set we will find predictor that splits the data very well if we do fold cross validation this same predictor should split any ths and th of the data well too and hence its cross validation error will be small much less than thus cv does not give an accurate estimate of error
to investigate whether this argument is correct figure shows the result of simulation from this setting
there are predictors and samples in each of two equal sized classes with all predictors having standard gaussian distribution
the panel in the top left shows the number of training errors for each of the stumps fit to the training data
we have marked in color the six predictors yielding the fewest errors
in the top right panel the training errors are shown for stumps fit to random ths partition of the data samples and tested on the remaining th four samples
the colored points indicate the same predictors marked in the top left panel
we see that the stump for the blue predictor whose stump was the best in the top left panel makes two out of four test errors and is no better than random
what has happened
the preceding argument has ignored the fact that in cross validation the model must be completely retrained for each fold this argument was made to us by scientist at proteomics lab meeting and led to material in this section
simulation study to investigate the performance of cross validation in high dimensional problem where the predictors are independent of the class labels
the top left panel shows the number of errors made by individual stump classifiers on the full training set observations
the top right panel shows the errors made by individual stumps trained on random split of the dataset into ths observations and tested on the remaining th observations
the best performers are depicted by colored dots in each panel
the bottom left panel shows the effect of re estimating the split point in each fold the colored points correspond to the four samples in the ths validation set
the split point derived from the full dataset classifies all four samples correctly but when the split point is re estimated on the ths data as it should be it commits two errors on the four validation samples
in the bottom right we see the overall result of five fold cross validation applied to simulated datasets
the average error rate is about as it should be
bootstrap methods of the process
in the present example this means that the best predictor and corresponding split point are found from ths of the data
the effect of predictor choice is seen in the top right panel
since the class labels are independent of the predictors the performance of stump on the ths training data contains no information about its performance in the remaining th
the effect of the choice of split point is shown in the bottom left panel
here we see the data for predictor corresponding to the blue dot in the top left plot
the colored points indicate the th data while the remaining points belong to the ths
the optimal split points for this predictor based on both the full training set and ths data are indicated
the split based on the full data makes no errors on the ths data
but cross validation must base its split on the ths data and this incurs two errors out of four samples
the results of applying five fold cross validation to each of simulated datasets is shown in the bottom right panel
as we would hope the average cross validation error is around which is the true expected prediction error for this classifier
hence cross validation has behaved as it should
on the other hand there is considerable variability in the error underscoring the importance of reporting the estimated standard error of the cv estimate
see exercise for another variation of this problem
bootstrap methods the bootstrap is general tool for assessing statistical accuracy
first we describe the bootstrap in general and then show how it can be used to estimate extra sample prediction error
as with cross validation the bootstrap seeks to estimate the conditional error errt but typically estimates well only the expected prediction error err
suppose we have model fit to set of training data
we denote the training set by zn where zi xi yi
the basic idea is to randomly draw datasets with replacement from the training data each sample the same size as the original training set
this is done times say producing bootstrap datasets as shown in figure
then we refit the model to each of the bootstrap datasets and examine the behavior of the fits over the replications
in the figure is any quantity computed from the data for example the prediction at some input point
from the bootstrap sampling we can estimate any aspect of the distribution of for example its variance var
schematic of the bootstrap process
we wish to assess the statistical accuracy of quantity computed from our dataset
training sets each of size are drawn with replacement from the original dataset
the quantity of interest is computed from each bootstrap training set and the values are used to assess the statistical accuracy of
where
note that var can be thought of as monte carlo estimate of the variance of under sampling from the empirical distribution function for the data zn
how can we apply the bootstrap to estimate prediction error
one approach would be to fit the model in question on set of bootstrap samples and then keep track of how well it predicts the original training set
if xi is the predicted value at xi from the model fitted to the bth bootstrap dataset our estimate is dboot err yi xi
bn dboot does not provide good estimate in however it is easy to see that err general
the reason is that the bootstrap datasets are acting as the training samples while the original training set is acting as the test sample and these two samples have observations in common
this overlap can make overfit predictions look unrealistically good and is the reason that crossvalidation explicitly uses non overlapping data for the training and test samples
consider for example nearest neighbor classifier applied to two class classification problem with the same number of observations in
bootstrap methods each class in which the predictors and class labels are in fact independent
then the true error rate is
but the contributions to the bootstrap dboot will be zero unless the observation does not appear in the estimate err bootstrap sample
in this latter case it will have the correct expectation
now pr observation bootstrap sample
hence the expectation of errdboot is about far below the correct error rate
by mimicking cross validation better bootstrap estimate can be obtained
for each observation we only keep track of predictions from bootstrap samples not containing that observation
the leave one out bootstrap estimate of prediction error is defined by err yi xi
here is the set of indices of the bootstrap samples that do not contain observation and is the number of such samples
in computing err we either have to choose large enough to ensure that all of the are greater than zero or we can just leave out the terms in corresponding to that are zero
the leave one out bootstrap solves the overfitting problem suffered by dboot but has the training set size bias mentioned in the discussion of err cross validation
the average number of distinct observations in each bootstrap sample is about so its bias will roughly behave like that of twofold cross validation
thus if the learning curve has considerable slope at sample size the leave one out bootstrap will be biased upward as an estimate of the true error
the estimator is designed to alleviate this bias
it is defined by err err err
the derivation of the estimator is complex intuitively it pulls the leave one out bootstrap estimate down toward the training error rate and hence reduces its upward bias
the use of the constant relates to
the estimator works well in light fitting situations but can break down in overfit ones
here is an example due to breiman et al
suppose we have two equal size classes with the targets independent of the class labels and we apply one nearest neighbor rule
then err
model assessment and selection and so err err
however the true error rate is
one can improve the estimator by taking into account the amount of overfitting
first we define to be the no information error rate this is the error rate of our prediction rule if the inputs and class labels were independent
an estimate of is obtained by evaluating the prediction rule on all possible combinations of targets yi and predictors xi xx yi xi
for example consider the dichotomous classification problem let be the observed proportion of responses yi equaling and let be the observed proportion of predictions xi equaling
then
with rule like nearest neighbors for which the value of is
the multi category generalization of is
using this the relative overfitting rate is defined to be err err err quantity that ranges from if there is no overfitting errd err to if the overfitting equals the no information value err
finally we define the estimator by err err err with
the weight ranges from if to if so err ranges from err
again the derivation of is complito err cated roughly speaking it produces compromise between the leave oneout bootstrap and the training error rate that depends on the amount of overfitting
for the nearest neighbor problem with class labels indepen dent of the inputs so err which has the correct err expectation of
in other problems with less overfitting err will lie somewhere between err and err example continued figure shows the results of tenfold cross validation and the bootstrap estimate in the same four problems of figures
as in that figure
boxplots show the distribution of the relative error err min err max err min err over the four scenarios of figure
this is the error in using the chosen model relative to the best model
there are training sets represented in each boxplot
figure shows boxplots of err min err max err min err the error in using the chosen model relative to the best model
there are different training sets represented in each boxplot
both measures perform well overall perhaps the same or slightly worse that the aic in figure
our conclusion is that for these particular problems and fitting methods minimization of either aic cross validation or bootstrap yields model fairly close to the best available
note that for the purpose of model selection any of the measures could be biased and it wouldn't affect things as long as the bias did not change the relative performance of the methods
for example the addition of constant to any of the measures would not change the resulting chosen model
however for many adaptive nonlinear techniques like trees estimation of the effective number of parameters is very difficult
this makes methods like aic impractical and leaves us with cross validation or bootstrap as the methods of choice
different question is how well does each method estimate test error
on the average the aic criterion overestimated prediction error of its cho
model assessment and selection sen model by and respectively over the four scenarios with bic performing similarly
in contrast cross validation overestimated the error by and with the bootstrap doing about the same
hence the extra work involved in computing cross validation or bootstrap measure is worthwhile if an accurate estimate of test error is required
with other fitting methods like trees cross validation and bootstrap can underestimate the true error by because the search for best tree is strongly affected by the validation set
in these situations only separate test set will provide an unbiased estimate of test error
conditional or expected test error
figures and examine the question of whether cross validation does good job in estimating errt the error conditional on given training set expression on page as opposed to the expected test error
for each of training sets generated from the reg linear setting in the top right panel of figure figure shows the conditional error curves errt as function of subset size top left
the next two panels show fold and fold cross validation the latter also known as leave one out loo
the thick red curve in each plot is the expected error err while the thick black curves are the expected cross validation curves
the lower right panel shows how well cross validation approximates the conditional and expected error
one might have expected fold cv to approximate errt well since it almost uses the full training sample to fit new test point fold cv on the other hand might be expected to estimate err well since it averages over somewhat different training sets
from the figure it appears fold does better job than fold in estimating errt and estimates err even better
indeed the similarity of the two black curves with the red curve suggests both cv curves are approximately unbiased for err with fold having less variance
similar trends were reported by efron
figure shows scatterplots of both fold and fold cross validation error estimates versus the true conditional error for the simulations
although the scatterplots do not indicate much correlation the lower right panel shows that for the most part the correlations are negative curious phenomenon that has been observed before
this negative correlation explains why neither form of cv estimates errt well
the broken lines in each plot are drawn at err the expected error for the best subset of size
we see again that both forms of cv are approximately unbiased for expected error but the variation in test error for different training sets is quite substantial
among the four experimental conditions in this reg linear scenario showed the highest correlation between actual and predicted test error
conditional or expected test error
conditional prediction error errt fold cross validation and leave one out cross validation curves for simulations from the top right panel in figure
the thick red curve is the expected prediction error err while the thick black curves are the expected cv curves et cv and et cvn
the lower right panel shows the mean absolute deviation of the cv curves from the conditional error et cvk errt for blue and green as well as from the expected error et cv err orange
plots of the cv estimates of error versus the true conditional error for each of the training sets for the simulation setup in the top right panel figure
both fold and leave one out cv are depicted in different colors
the first three panels correspond to different subset sizes and vertical and horizontal lines are drawn at err
although there appears to be little correlation in these plots we see in the lower right panel that for the most part the correlation is negative
exercises phenomenon also occurs for bootstrap estimates of error and we would guess for any other estimate of conditional prediction error
we conclude that estimation of test error for particular training set is not easy in general given just the data from that same training set
instead cross validation and related methods may provide reasonable estimates of the expected error err
bibliographic notes key references for cross validation are stone stone and allen
the aic was proposed by akaike while the bic was introduced by schwarz
madigan and raftery give an overview of bayesian model selection
the mdl criterion is due to rissanen
cover and thomas contains good description of coding theory and complexity
vc dimension is described in vapnik
stone showed that the aic and leave one out cross validation are asymptotically equivalent
generalized cross validation is described by golub et al and wahba further discussion of the topic may be found in the monograph by wahba
see also hastie and tibshirani chapter
the bootstrap is due to efron see efron and tibshirani for an overview
efron proposes number of bootstrap estimates of prediction error including the optimism and estimates
efron compares cv gcv and bootstrap estimates of error rates
the use of cross validation and the bootstrap for model selection is studied by breiman and spector breiman shao zhang and kohavi
the estimator was proposed by efron and tibshirani
cherkassky and ma published study on the performance of srm for model selection in regression in response to our study of section
they complained that we had been unfair to srm because had not applied it properly
our response can be found in the same issue of the journal hastie et al
exercises ex
derive the estimate of in sample error
for loss with and pr show that err pr errb pr
model assessment and selection where is the bayes classifier and errb pr the irreducible bayes error at
using the approximation ef var show that
sign ef pr
var in the above exp dt the cumulative gaussian distribution function
this is an increasing function with value at and value at
we can think of sign ef as kind of boundarybias term as it depends on the true only through which side of the boundary that it lies
notice also that the bias and variance combine in multiplicative rather than additive fashion
if ef is on the same side of as then the bias is negative and decreasing the variance will decrease the misclassification error
on the other hand if ef is on the opposite side of to then the bias is positive and it pays to increase the variance
such an increase will improve the chance that falls on the correct side of friedman
let sy be linear smoothing of if sii is the ith diagonal element of show that for arising from least squares projections and cubic smoothing splines the cross validated residual can be written as yi xi yi xi
sii use this result to show that yi xi yi xi find general conditions on any smoother to make result hold
consider the in sample prediction error and the training error err in the case of squared error loss errin ey yi xi err yi xi

exercises add and subtract xi and ef xi in each expression and expand
hence establish that the average optimism in the training error is cov yi as given in
for linear smoother sy show that cov yi trace which justifies its use as the effective number of parameters
show that for an additive error model the effective degrees offreedom for the nearest neighbors regression fit is
use the approximation to expose the relationship between cp aic and gcv the main difference being the model used to estimate the noise variance
show that the set of functions sin can shatter the following points on the line for any
hence the vc dimension of the class sin is infinite
for the prostate data of chapter carry out best subset linear regression analysis as in table third column from left
compute the aic bic fiveand tenfold cross validation and bootstrap estimates of prediction error
discuss the results
referring to the example in section suppose instead that all of the predictors are binary and hence there is no need to estimate split points
the predictors are independent of the class labels as before
then if is very large we can probably find predictor that splits the entire training data perfectly and hence would split the validation data one fifth of data perfectly as well
this predictor would therefore have zero cross validation error
does this mean that cross validation does not provide good estimate of test error in this situation
this question was suggested by li ma
model assessment and selection
this is page printer opaque this model inference and averaging introduction for most of this book the fitting learning of models has been achieved by minimizing sum of squares for regression or by minimizing cross entropy for classification
in fact both of these minimizations are instances of the maximum likelihood approach to fitting
in this chapter we provide general exposition of the maximum likelihood approach as well as the bayesian method for inference
the bootstrap introduced in chapter is discussed in this context and its relation to maximum likelihood and bayes is described
finally we present some related techniques for model averaging and improvement including committee methods bagging stacking and bumping
the bootstrap and maximum likelihood methods smoothing example the bootstrap method provides direct computational way of assessing uncertainty by sampling from the training data
here we illustrate the bootstrap in simple one dimensional smoothing problem and show its connection to maximum likelihood
left panel data for smoothing example
right panel set of seven spline basis functions
the broken vertical lines indicate the placement of the three knots
denote the training data by zn with zi xi yi
here xi is one dimensional input and yi the outcome either continuous or categorical
as an example consider the data points shown in the left panel of figure
suppose we decide to fit cubic spline to the data with three knots placed at the quartiles of the values
this is seven dimensional linear space of functions and can be represented for example by linear expansion of spline basis functions see section hj
here the hj are the seven functions shown in the right panel of figure
we can think of as representing the conditional mean
let be the matrix with ijth element hj xi
the usual estimate of obtained by minimizing the squared error over the training set is given by ht ht
the corresponding fit hj is shown in the top left panel of figure
the estimated covariance matrix of is ht var pn where we have estimated the noise variance by yi xi
letting the standard error of predic
top left spline smooth of data
top right spline smooth plus and minus standard error bands
bottom left ten bootstrap replicates of the spline smooth
bottom right spline smooth with standard error bands computed from the bootstrap distribution
model inference and averaging tion is se ht
in the top right panel of figure we have plotted se since is the point of the standard normal distribution these represent approximate pointwise confidence bands for
here is how we could apply the bootstrap in this example
we draw datasets each of size with replacement from our training data the sampling unit being the pair zi xi yi
to each bootstrap dataset we fit cubic spline the fits from ten such samples are shown in the bottom left panel of figure
using bootstrap samples we can form pointwise confidence band from the percentiles at each we find the fifth largest and smallest values at each
these are plotted in the bottom right panel of figure
the bands look similar to those in the top right being little wider at the endpoints
there is actually close connection between the least squares estimates and the bootstrap and maximum likelihood
suppose we further assume that the model errors are gaussian hj
the bootstrap method described above in which we sample with replacement from the training data is called the nonparametric bootstrap
this really means that the method is model free since it uses the raw data not specific parametric model to generate new datasets
consider variation of the bootstrap called the parametric bootstrap in which we simulate new responses by adding gaussian noise to the predicted values yi xi
this process is repeated times where say
the resulting boot strap datasets have the form xn yn and we recompute the spline smooth on each
the confidence bands from this method will exactly equal the least squares bands in the top right panel as the number of bootstrap samples goes to infinity
function estimated from bootstrap sample is given by ht ht and has distribution ht
notice that the mean of this distribution is the least squares estimate and the standard deviation is the same as the approximate formula
the bootstrap and maximum likelihood methods maximum likelihood inference it turns out that the parametric bootstrap agrees with least squares in the previous example because the model has additive gaussian errors
in general the parametric bootstrap agrees not with least squares but with maximum likelihood which we now review
we begin by specifying probability density or probability mass function for our observations zi
in this expression represents one or more unknown parameters that govern the distribution of
this is called parametric model for
as an example if has normal distribution with mean and variance then and
maximum likelihood is based on the likelihood function given by zi the probability of the observed data under the model
the likelihood is defined only up to positive multiplier which we have taken to be one
we think of as function of with our data fixed
denote the logarithm of by zi log zi which we will sometimes abbreviate as
this expression is called the log likelihood and each value zi log zi is called log likelihood component
the method of maximum likelihood chooses the value to maximize
the likelihood function can be used to assess the precision of
we need few more definitions
the score function is defined by zi
model inference and averaging where zi zi
assuming that the likelihood takes its maximum in the interior of the parameter space
the information matrix is zi
when is evaluated at it is often called the observed information
the fisher information or expected information is
finally let denote the true value of
standard result says that the sampling distribution of the maximum likelihood estimator has limiting normal distribution as
here we are independently sampling from
this suggests that the sampling distribution of may be approximated by or where represents the maximum likelihood estimate from the observed data
the corresponding estimates for the standard errors of are obtained from jj and jj
confidence points for can be constructed from either approximation in
such confidence point has the form jj or jj respectively where is the percentile of the standard normal distribution
more accurate confidence intervals can be derived from the likelihood function by using the chi squared approximation where is the number of components in
the resulting confi dence interval is the set of all such that where is the percentile of the chi squared distribution with degrees of freedom
bayesian methods let's return to our smoothing example to see what maximum likelihood yields
the parameters are
the log likelihood is log yi xi
the maximum likelihood estimate is obtained by setting and giving ht ht yi xi which are the same as the usual estimates given in and below
the information matrix for is block diagonal and the block corresponding to is ht so that the estimated variance ht agrees with the least squares estimate
bootstrap versus maximum likelihood in essence the bootstrap is computer implementation of nonparametric or parametric maximum likelihood
the advantage of the bootstrap over the maximum likelihood formula is that it allows us to compute maximum likelihood estimates of standard errors and other quantities in settings where no formulas are available
in our example suppose that we adaptively choose by cross validation the number and position of the knots that define the splines rather than fix them in advance
denote by bb the collection of knots and their positions
then the standard errors and confidence bands should account for the adaptive choice of bb but there is no way to do this analytically
with the bootstrap we compute the spline smooth with an adaptive choice of knots for each bootstrap sample
the percentiles of the resulting curves capture the variability from both the noise in the targets as well as that from bb
in this particular example the confidence bands not shown don't look much different than the fixed bb bands
but in other problems where more adaptation is used this can be an important effect to capture
bayesian methods in the bayesian approach to inference we specify sampling model pr density or probability mass function for our data given the parameters
model inference and averaging and prior distribution for the parameters pr reflecting our knowledge about before we see the data
we then compute the posterior distribution pr pr pr pr pr which represents our updated knowledge about after we see the data
to understand this posterior distribution one might draw samples from it or summarize by computing its mean or mode
the bayesian approach differs from the standard frequentist method for inference in its use of prior distribution to express the uncertainty present before seeing the data and to allow the uncertainty remaining after seeing the data to be expressed in the form of posterior distribution
the posterior distribution also provides the basis for predicting the values of future observation new via the predictive distribution pr new pr new pr
in contrast the maximum likelihood approach would use pr new the data density evaluated at the maximum likelihood estimate to predict future data
unlike the predictive distribution this does not account for the uncertainty in estimating
let's walk through the bayesian approach in our smoothing example
we start with the parametric model given by equation and assume for the moment that is known
we assume that the observed feature values xn are fixed so that the randomness in the data comes solely from varying around its mean
the second ingredient we need is prior distribution
distributions on functions are fairly complex entities one approach is to use gaussian process prior in which we specify the prior covariance between any two function values and wahba neal
here we take simpler route by considering finite spline basis for we can instead provide prior for the coefficients and this implicitly defines prior for
we choose gaussian prior centered at zero with the choices of the prior correlation matrix and variance to be discussed below
the implicit process prior for is hence gaussian with covariance kernel cov
smoothing example ten draws from the gaussian prior distribution for the function
the posterior distribution for is also gaussian with mean and covariance ht ht cov with the corresponding posterior values for ht ht cov
how do we choose the prior correlation matrix
in some settings the prior can be chosen from subject matter knowledge about the parameters
here we are willing to say the function should be smooth and have guaranteed this by expressing in smooth low dimensional basis of bsplines
hence we can take the prior correlation matrix to be the identity
when the number of basis functions is large this might not be sufficient and additional smoothness can be enforced by imposing restrictions on this is exactly the case with smoothing splines section
figure shows ten draws from the corresponding prior for
to generate posterior values of the function we generate values from its posterior giving corresponding posterior value hj
ten such posterior curves are shown in figure
two different values were used for the prior variance and
notice how similar the right panel looks to the bootstrap distribution in the bottom left panel
smoothing example ten draws from the posterior distribution for the function for two different values of the prior variance
the purple curves are the posterior means of figure on page
this similarity is no accident
as the posterior distribution and the bootstrap distribution coincide
on the other hand for the posterior curves in the left panel of figure are smoother than the bootstrap curves because we have imposed more prior weight on smoothness
the distribution with is called noninformative prior for
in gaussian models maximum likelihood and parametric bootstrap analyses tend to agree with bayesian analyses that use noninformative prior for the free parameters
these tend to agree because with constant prior the posterior distribution is proportional to the likelihood
this correspondence also extends to the nonparametric case where the nonparametric bootstrap approximates noninformative bayes analysis section has the details
we have however done some things that are not proper from bayesian point of view
we have used noninformative constant prior for and replaced it with the maximum likelihood estimate in the posterior
more standard bayesian analysis would also put prior on typically calculate joint posterior for and and then integrate out rather than just extract the maximum of the posterior distribution map estimate
relationship between the bootstrap and bayesian inference relationship between the bootstrap and bayesian inference consider first very simple example in which we observe single observation from normal distribution
to carry out bayesian analysis for we need to specify prior
the most convenient and common choice would be giving posterior distribution
now the larger we take the more concentrated the posterior becomes around the maximum likelihood estimate
in the limit as we obtain noninformative constant prior and the posterior distribution is
this is the same as parametric bootstrap distribution in which we generate bootstrap values from the maximum likelihood estimate of the sampling density
there are three ingredients that make this correspondence work
the choice of noninformative prior for
the dependence of the log likelihood on the data only through the maximum likelihood estimate
hence we can write the log likelihood as
the symmetry of the log likelihood in and that is constant
properties and essentially only hold for the gaussian distribution
however they also hold approximately for the multinomial distribution leading to correspondence between the nonparametric bootstrap and bayes inference which we outline next
assume that we have discrete sample space with categories
let wj be the probability that sample point falls in category and the observed proportion in category
let wl
denote our estimator by take as prior distribution for symmetric dirichlet distribution with parameter dil
model inference and averaging ql that is the prior probability mass function is proportional to
then the posterior density of is dil where is the sample size
letting to obtain noninformative prior gives dil
now the bootstrap distribution obtained by sampling with replacement from the data can be expressed as sampling the category proportions from multinomial distribution
specifically mult where mult denotes multinomial distribution having probability mass function
this distribution is similar to the pos terior distribution above having the same support same mean and nearly the same covariance matrix
hence the bootstrap distribution of will closely approximate the posterior distribution of
in this sense the bootstrap distribution represents an approximate nonparametric noninformative posterior distribution for our parameter
but this bootstrap distribution is obtained painlessly without having to formally specify prior and without having to sample from the posterior distribution
hence we might think of the bootstrap distribution as poor man's bayes posterior
by perturbing the data the bootstrap approximates the bayesian effect of perturbing the parameters and is typically much simpler to carry out
the em algorithm the em algorithm is popular tool for simplifying difficult maximum likelihood problems
we first describe it in the context of simple mixture model
two component mixture model in this section we describe simple mixture model for density estimation and the associated em algorithm for carrying out maximum likelihood estimation
this has natural connection to gibbs sampling methods for bayesian inference
mixture models are discussed and demonstrated in several other parts of the book in particular sections and
the left panel of figure shows histogram of the fictitious data points in table
mixture example
left panel histogram of data
right panel maximum likelihood fit of gaussian densities solid red and responsibility dotted green of the left component density for observation as function of
twenty fictitious data points used in the two component mixture example in figure we would like to model the density of the data points and due to the apparent bi modality gaussian distribution would not be appropriate
there seems to be two separate underlying regimes so instead we model as mixture of two normal distributions where with pr
this generative representation is explicit generate with probability and then depending on the outcome deliver either or
let denote the normal density with parameters
then the density of is gy
now suppose we wish to fit this model to the data in figure by maximum likelihood
the parameters are
the log likelihood based on the training cases is log yi yi
model inference and averaging direct maximization of is quite difficult numerically because of the sum of terms inside the logarithm
there is however simpler approach
we consider unobserved latent variables taking values or as in if then yi comes from model otherwise it comes from model
suppose we knew the values of the
then the log likelihood would be log yi log yi log log and the maximum likelihood estimates of and would be the sample mean and variance for those data with and similarly those for and would be the sample mean and variance of the data with
the estimate of would be the proportion of
since the values of the are actually unknown we proceed in an iterative fashion substituting for each in its expected value pr also called the responsibility of model for observation
we use procedure called the em algorithm given in algorithm for the special case of gaussian mixtures
in the expectation step we do soft assignment of each observation to each model the current estimates of the parameters are used to assign responsibilities according to the relative density of the training points under each model
in the maximization step these responsibilities are used in weighted maximum likelihood fits to update the estimates of the parameters
good way to construct initial guesses for and is simply to choose two of the yi at random
both and can be set equal to the overall pn sample variance yi
the mixing proportion can be started at the value
note that the actual maximizer of the likelihood occurs when we put spike of infinite height at any one data point that is yi for some and
this gives infinite likelihood but is not useful solution
hence we are actually looking for good local maximum of the likelihood one for which
to further complicate matters there can be more than one local maximum having
in our example we ran the em algorithm with number of different initial guesses for the parameters all having and chose the run that gave us the highest maximized likelihood
figure shows the progressp of the em algorithm in maximizing the log likelihood
table shows the maximum likelihood estimate of the proportion of observations in class at selected iterations of the em procedure
the em algorithm algorithm em algorithm for two component gaussian mixture
take initial guesses for the parameters see text
expectation step compute the responsibilities yi
yi yi
maximization step compute the weighted means and variances pn pn yi yi pi pn pn pn yi yi pi pn pn and the mixing probability
iterate steps and until convergence
selected iterations of the em algorithm for mixture example
iteration the final maximum likelihood estimates are
the right panel of figure shows the estimated gaussian mixture density from this procedure solid red curve along with the responsibilities dotted green curve
note that mixtures are also useful for supervised learning in section we show how the gaussian mixture model leads to version of radial basis functions
em algorithm observed data log likelihood as function of the iteration number
the em algorithm in general the above procedure is an example of the em or baum welch algorithm for maximizing likelihoods in certain classes of problems
these problems are ones for which maximization of the likelihood is difficult but made easier by enlarging the sample with latent unobserved data
this is called data augmentation
here the latent data are the model memberships
in other problems the latent data are actual data that should have been observed but are missing
algorithm gives the general formulation of the em algorithm
our observed data is having log likelihood depending on parameters
the latent or missing data is zm so that the complete data is zm with log likelihood based on the complete density
in the mixture problem zm and is given in
in our mixture example is simply with the replaced by the responsibilities and the maximizers in step are just weighted means and variances
we now give an explanation of why the em algorithm works in general
since pr zm pr zm pr we can write pr pr
pr zm in terms of log likelihoods we have zm where is based on the conditional density pr zm
taking conditional expectations with respect to the distribution of governed by parameter gives zm
the em algorithm algorithm the em algorithm
start with initial guesses for the parameters
expectation step at the jth step compute as function of the dummy argument
maximization step determine the new estimate as the maximizer of over
iterate steps and until convergence

in the step the em algorithm maximizes over rather than the actual objective function
why does it succeed in maximizing
note that is the expectation of log likelihood of density indexed by with respect to the same density indexed by and hence by jensen's inequality is maximized as function of when see exercise
so if maximizes we see that
hence the em iteration never decreases the log likelihood
this argument also makes it clear that full maximization in the step is not necessary we need only to find value so that increases as function of the first argument that is
such procedures are called gem generalized em algorithms
the em algorithm can also be viewed as minorization procedure see exercise
em as maximization maximization procedure here is different view of the em procedure as joint maximization algorithm
consider the function ep ep log zm
here zm is any distribution over the latent data zm
in the mixture example zm comprises the set of probabilities pr
note that evaluated at zm pr zm is the log likelihood of
maximization maximization view of the em algorithm
shown are the contours of the augmented observed data log likelihood
the step is equivalent to maximizing the log likelihood over the parameters of the latent data distribution
the step maximizes it over the parameters of the log likelihood
the red curve corresponds to the observed data log likelihood profile obtained by maximizing for each value of the observed data from
the function expands the domain of the log likelihood to facilitate its maximization
the em algorithm can be viewed as joint maximization method for over and zm by fixing one argument and maximizing over the other
the maximizer over zm for fixed can be shown to be zm pr zm exercise
this is the distribution computed by the step for example in the mixture example
in the step we maximize over with fixed this is the same as maximizing the first term ep since the second term does not involve
finally since and the observed data log likelihood agree when zm pr zm maximization of the former accomplishes maximization of the latter
figure shows schematic view of this process
this view of the em algorithm leads to alternative maximization proce holds for all including
mcmc for sampling from the posterior algorithm gibbs sampler
take some initial values uk
repeat for for generate uk from pr uk uk uk uk
continue step until the joint distribution of uk does not change dures
for example one does not need to maximize with respect to all of the latent data parameters at once but could instead maximize over one of them at time alternating with the step
mcmc for sampling from the posterior having defined bayesian model one would like to draw samples from the resulting posterior distribution in order to make inferences about the parameters
except for simple models this is often difficult computational problem
in this section we discuss the markov chain monte carlo mcmc approach to posterior sampling
we will see that gibbs sampling an mcmc procedure is closely related to the em algorithm the main difference is that it samples from the conditional distributions rather than maximizing over them
consider first the following abstract problem
we have random variables uk and we wish to draw sample from their joint distribution
suppose this is difficult to do but it is easy to simulate from the conditional distributions pr uj uj uj uk
the gibbs sampling procedure alternatively simulates from each of these distributions and when the process stabilizes provides sample from the desired joint distribution
the procedure is defined in algorithm
under regularity conditions it can be shown that this procedure eventually stabilizes and the resulting random variables are indeed sample from the joint distribution of uk
this occurs despite the fact that the samples uk are clearly not independent for different
more formally gibbs sampling produces markov chain whose stationary distribution is the true joint distribution and hence the term markov chain monte carlo
it is not surprising that the true joint distribution is stationary under this process as the successive steps leave the marginal distributions of the uk unchanged
model inference and averaging note that we don't need to know the explicit form of the conditional densities but just need to be able to sample from them
after the procedure reaches stationarity the marginal density of any subset of the variables can be approximated by density estimate applied to the sample values
however if the explicit form of the conditional density pr uk is available better estimate of say the marginal density of uk can be obtained from exercise xm pr pr
here we have averaged over the last members of the sequence to allow for an initial burn in period before stationarity is reached
now getting back to bayesian inference our goal is to draw sample from the joint posterior of the parameters given the data
gibbs sampling will be helpful if it is easy to sample from the conditional distribution of each parameter given the other parameters and
an example the gaussian mixture problem is detailed next
there is close connection between gibbs sampling from posterior and the em algorithm in exponential family models
the key is to consider the latent data zm from the em procedure to be another parameter for the gibbs sampler
to make this explicit for the gaussian mixture problem we take our parameters to be zm
for simplicity we fix the variances and mixing proportion at their maximum likelihood values so that the only unknown parameters in are the means and
the gibbs sampler for the mixture problem is given in algorithm
we see that steps and are the same as the and steps of the em procedure except that we sample rather than maximize
in step rather than compute the maximum likelihood responsibilities the gibbs sampling procedure simulates the latent data from the distributions pr
in step rather than compute the maximizers of the posterior pr we simulate from the conditional distribution pr
figure shows iterations of gibbs sampling with the mean parameters lower and upper shown in the left panel and the proportion of class observations on the right
horizontal broken lines have been drawn at the maximum likelihood estimate values and in each case
the values seem to stabilize quite quickly and are distributed evenly around the maximum likelihood values
the above mixture model was simplified in order to make the clear connection between gibbs sampling and the em algorithm
more realistically one would put prior distribution on the variances and mixing proportion and include separate gibbs sampling steps in which we sample from their posterior distributions conditional on the other parameters
one can also incorporate proper informative priors for the mean param
mcmc for sampling from the posterior algorithm gibbs sampling for mixtures
take some initial values
repeat for for generate with pr from equation set pn yi pn pn yi pn and generate and
mixture example
left panel values of the two mean parameters from gibbs sampling horizontal lines are drawn at the maximum likelihood for each of the estimates
right panel proportion of values with ip gibbs sampling iterations horizontal line is drawn at
model inference and averaging eters
these priors must not be improper as this will lead to degenerate posterior with all the mixing weight on one component
gibbs sampling is just one of number of recently developed procedures for sampling from posterior distributions
it uses conditional sampling of each parameter given the rest and is useful when the structure of the problem makes this sampling easy to carry out
other methods do not require such structure for example the metropolis hastings algorithm
these and other computational bayesian methods have been applied to sophisticated learning algorithms such as gaussian process models and neural networks
details may be found in the references given in the bibliographic notes at the end of this chapter
bagging earlier we introduced the bootstrap as way of assessing the accuracy of parameter estimate or prediction
here we show how to use the bootstrap to improve the estimate or prediction itself
in section we investigated the relationship between the bootstrap and bayes approaches and found that the bootstrap mean is approximately posterior average
bagging further exploits this connection
consider first the regression problem
suppose we fit model to our training data xn yn obtaining the prediction at input
bootstrap aggregation or bagging averages this prediction over collection of bootstrap samples thereby reducing its variance
for each bootstrap sample we fit our model giving prediction
the bagging estimate is defined by bag
denote by the empirical distribution putting equal probability on each of the data points xi yi
in fact the true bagging estimate is defined by ep where yn and each xi yi
expression is monte carlo estimate of the true bagging estimate approaching it as
the bagged estimate will differ from the original estimate only when the latter is nonlinear or adaptive function of the data
for example to bag the spline smooth of section we average the curves in the bottom left panel of figure at each value of
the spline smoother is linear in the data if we fix the inputs hence if we sample using the parametric bootstrap in equation then bag as exercise
hence bagging just reproduces the original smooth in the
bagging top left panel of figure
the same is approximately true if we were to bag using the nonparametric bootstrap
more interesting example is regression tree where denotes the tree's prediction at input vector regression trees are described in chapter
each bootstrap tree will typically involve different features than the original and might have different number of terminal nodes
the bagged estimate is the average prediction at from these trees
now suppose our tree produces classifier for class response
here it is useful to consider an underlying indicator vector function with value single one and zeroes such that arg maxk
then the bagged estimate bag is vector pk with pk equal to the proportion of trees predicting class at
the bagged classifier selects the class with the most votes from the trees bag arg maxk bag
often we require the class probability estimates at rather than the classifications themselves
it is tempting to treat the voting proportions pk as estimates of these probabilities
simple two class example shows that they fail in this regard
suppose the true probability of class at is and each of the bagged classifiers accurately predict
then which is incorrect
for many classifiers however there is already an underlying function that estimates the class probabilities at for trees the class proportions in the terminal node
an alternative bagging strategy is to average these instead rather than the vote indicator vectors
not only does this produce improved estimates of the class probabilities but it also tends to produce bagged classifiers with lower variance especially for small see figure in the next example
example trees with simulated data we generated sample of size with two classes and features each having standard gaussian distribution with pairwise correlation
the response was generated according to pr pr
the bayes error is
test sample of size was also generated from the same population
we fit classification trees to the training sample and to each of bootstrap samples classification trees are described in chapter
no pruning was used
figure shows the original tree and eleven bootstrap trees
notice how the trees are all different with different splitting features and cutpoints
the test error for the original tree and the bagged tree is shown in figure
in this example the trees have high variance due to the correlation in the predictors
bagging succeeds in smoothing out this variance and hence reducing the test error
bagging can dramatically reduce the variance of unstable procedures like trees leading to improved prediction
simple argument shows why
bagging trees on simulated dataset
the top left panel shows the original tree
eleven trees grown on bootstrap samples are shown
for each tree the top split is annotated
error curves for the bagging example of figure
shown is the test error of the original tree and bagged trees as function of the number of bootstrap samples
the orange points correspond to the consensus vote while the green points average the probabilities bagging helps under squared error loss in short because averaging reduces variance and leaves bias unchanged
assume our training observations xi yi are independently drawn from distribution and consider the ideal aggregate estimator fag ep
here is fixed and the bootstrap dataset consists of observations yi sampled from
note that fag is bagging estimate drawing bootstrap samples from the actual population rather than the data
it is not an estimate that we can use in practice but is convenient for analysis
we can write ep ep fag fag ep fag ep fag ep fag
the extra error on the right hand side comes from the variance of around its mean fag
therefore true population aggregation never increases mean squared error
this suggests that bagging drawing samples from the training datawill often decrease mean squared error
the above argument does not hold for classification under loss because of the nonadditivity of bias and variance
in that setting bagging
model inference and averaging good classifier can make it better but bagging bad classifier can make it worse
here is simple example using randomized rule
suppose for all and the classifier predicts for all with probability and predicts for all with probability
then the misclassification error of is but that of the bagged classifier is
for classification we can understand the bagging effect in terms of consensus of independent weak learners dietterich
let the bayes optimal decision at be in two class example
suppose each of the weak learners have an error rate eb and let pb gb be the consensus vote for class
since the weak learners are assumed to be independent bin and pr as gets large
this concept has been popularized outside of statistics as the wisdom of crowds surowiecki the collective knowledge of diverse and independent body of people typically exceeds the knowledge of any single individual and can be harnessed by voting
of course the main caveat here is independent and bagged trees are not
figure illustrates the power of consensus vote in simulated example where only of the voters have some knowledge
in chapter we see how random forests improve on bagging by reducing the correlation between the sampled trees
note that when we bag model any simple structure in the model is lost
as an example bagged tree is no longer tree
for interpretation of the model this is clearly drawback
more stable procedures like nearest neighbors are typically not affected much by bagging
unfortunately the unstable models most helped by bagging are unstable because of the emphasis on interpretability and this is lost in the bagging process
figure shows an example where bagging doesn't help
the data points shown have two features and two classes separated by the gray linear boundary
we choose as our classifier single axis oriented split choosing the split along either or that produces the largest decrease in training misclassification error
the decision boundary obtained from bagging the decision rule over bootstrap samples is shown by the blue curve in the left panel
it does poor job of capturing the true boundary
the single split rule derived from the training data splits near the middle of the range of or and hence has little contribution away from the center
averaging the probabilities rather than the classifications does not help here
bagging estimates the expected class probabilities from the single split rule that is averaged over many replications
note that the expected class probabilities computed by bagging cannot be realized on any single replication in the same way that woman cannot have children
in this sense bagging increases somewhat the space of models of the individual base classifier
however it doesn't help in this and many other examples where greater enlargement of the model class is needed
boosting is way of doing this
simulated academy awards voting members vote in categories each with nominations
for any category only voters have some knowledge represented by their probability of selecting the correct candidate in that category so means they have no knowledge
for each category the experts are chosen at random from the
results show the expected correct based on simulations for the consensus as well as for the individuals
the error bars indicate one standard deviation
we see for example that if the informed for category have chance of selecting the correct candidate the consensus doubles the expected performance of an individual
data with two features and two classes separated by linear boundary
left panel decision boundary estimated from bagging the decision rule from single split axis oriented classifier
right panel decision boundary from boosting the decision rule of the same classifier
the test error rates are and respectively
boosting is described in chapter and is described in chapter
the decision boundary in the right panel is the result of the boosting procedure and it roughly captures the diagonal boundary
model averaging and stacking in section we viewed bootstrap values of an estimator as approximate posterior values of corresponding parameter from kind of nonparametric bayesian analysis
viewed in this way the bagged estimate is an approximate posterior bayesian mean
in contrast the training sample estimate corresponds to the mode of the posterior
since the posterior mean not mode minimizes squared error loss it is not surprising that bagging can often reduce mean squared error
here we discuss bayesian model averaging more generally
we have set of candidate models mm for our training set
these models may be of the same type with different parameter values subsets in linear regression or different models for the same task neural networks and regression trees
suppose is some quantity of interest for example prediction at some fixed feature value
the posterior distribution of is pr pr mm pr mm
model averaging and stacking with posterior mean mm pr mm
this bayesian prediction is weighted average of the individual predictions with weights proportional to the posterior probability of each model
this formulation leads to number of different model averaging strategies
committee methods take simple unweighted average of the predictions from each model essentially giving equal probability to each model
more ambitiously the development in section shows the bic criterion can be used to estimate posterior model probabilities
this is applicable in cases where the different models arise from the same parametric model with different parameter values
the bic gives weight to each model depending on how well it fits and how many parameters it uses
one can also carry out the bayesian recipe in full
if each model mm has parameters we write pr mm pr mm pr pr mm pr mm pr mm
in principle one can specify priors pr mm and numerically compute the posterior probabilities from to be used as model averaging weights
however we have seen no real evidence that this is worth all of the effort relative to the much simpler bic approximation
how can we approach model averaging from frequentist viewpoint
given predictions under squared error loss we can seek the weights wm such that argmin ep wm
here the input value is fixed and the observations in the dataset and the target are distributed according to
the solution is the population linear regression of on ep ep
now the full regression has smaller error than any single model xm ep fm ep so combining models never makes things worse at the population level
model inference and averaging of course the population linear regression is not available and it is natural to replace it with the linear regression over the training set
but there are simple examples where this does not work well
for example if represent the prediction from the best subset of inputs of size among total inputs then linear regression would put all of the weight on the largest model that is
the problem is that we have not put each of the models on the same footing by taking into account their complexity the number of inputs in this example
stacked generalization or stacking is way of doing this
let be the prediction at using model applied to the dataset with the ith training observation removed
the stacking estimate of the weights is obtained from the least squares linear regression of yi on xi
in detail the stacking weights are given by st argmin yi wm xi
st the final prediction is fm
by using the cross validated predictions stacking avoids giving unfairly high weight to models with higher complexity
better results can be obtained by restricting the weights to be nonnegative and to sum to
this seems like reasonable restriction if we interpret the weights as posterior model probabilities as in equation and it leads to tractable quadratic programming problem
there is close connection between stacking and model selection via leave one out cross validation section
if we restrict the minimization in to weight vectors that have one unit weight and the rest zero this leads to model choice with smallest leave one out cross validation error
rather than choose single model stacking combines them with estimated optimal weights
this will often lead to better prediction but less interpretability than the choice of only one of the models
the stacking idea is actually more general than described above
one can use any learning method not just linear regression to combine the models as in the weights could also depend on the input location
in this way learning methods are stacked on top of one another to improve prediction performance
stochastic search bumping the final method described in this chapter does not involve averaging or combining models but rather is technique for finding better single model
bumping uses bootstrap sampling to move randomly through model space
for problems where fitting method finds many local minima bumping can help the method to avoid getting stuck in poor solutions
data with two features and two classes blue and orange displaying pure interaction
the left panel shows the partition found by three splits of standard greedy tree growing algorithm
the vertical grey line near the left edge is the first split and the broken lines are the two subsequent splits
the algorithm has no idea where to make good initial split and makes poor choice
the right panel shows the near optimal splits found by bumping the tree growing algorithm times
as in bagging we draw bootstrap samples and fit model to each
but rather than average the predictions we choose the model estimated from bootstrap sample that best fits the training data
in detail we draw bootstrap samples and fit our model to each giving predictions at input point
we then choose the model that produces the smallest prediction error averaged over the original training set
for squared error for example we choose the model obtained from bootstrap sample where arg min yi xi
the corresponding model predictions are
by convention we also include the original training sample in the set of bootstrap samples so that the method is free to pick the original model if it has the lowest training error
by perturbing the data bumping tries to move the fitting procedure around to good areas of model space
for example if few data points are causing the procedure to find poor solution any bootstrap sample that omits those data points should procedure better solution
for another example consider the classification data in figure the notorious exclusive or xor problem
there are two classes blue and orange and two input features with the features exhibiting pure inter
model inference and averaging action
by splitting the data at and then splitting each resulting strata at or vice versa tree based classifier could achieve perfect discrimination
however the greedy short sighted cart algorithm section tries to find the best split on either feature and then splits the resulting strata
because of the balanced nature of the data all initial splits on or appear to be useless and the procedure essentially generates random split at the top level
the actual split found for these data is shown in the left panel of figure
by bootstrap sampling from the data bumping breaks the balance in the classes and with reasonable number of bootstrap samples here it will by chance produce at least one tree with initial split near either or
using just bootstrap samples bumping found the near optimal splits shown in the right panel of figure
this shortcoming of the greedy tree growing algorithm is exacerbated if we add number of noise features that are independent of the class label
then the tree growing algorithm cannot distinguish or from the others and gets seriously lost
since bumping compares different models on the training data one must ensure that the models have roughly the same complexity
in the case of trees this would mean growing trees with the same number of terminal nodes on each bootstrap sample
bumping can also help in problems where it is difficult to optimize the fitting criterion perhaps because of lack of smoothness
the trick is to optimize different more convenient criterion over the bootstrap samples and then choose the model producing the best results for the desired criterion on the training sample
bibliographic notes there are many books on classical statistical inference cox and hinkley and silvey give nontechnical accounts
the bootstrap is due to efron and is described more fully in efron and tibshirani and hall
good modern book on bayesian inference is gelman et al
lucid account of the application of bayesian methods to neural networks is given in neal
the statistical application of gibbs sampling is due to geman and geman and gelfand and smith with related work by tanner and wong
markov chain monte carlo methods including gibbs sampling and the metropolis hastings algorithm are discussed in spiegelhalter et al
the em algorithm is due to dempster et al as the discussants in that paper make clear there was much related earlier work
the view of em as joint maximization scheme for penalized complete data log likelihood was elucidated by neal and hinton they credit csiszar and tusna dy and hathaway as having noticed this connection earlier
bagging was proposed by breiman
stacking is due to wolpert
exercises breiman contains an accessible discussion for statisticians
leblanc and tibshirani describe variations on stacking based on the bootstrap
model averaging in the bayesian framework has been recently advocated by madigan and raftery
bumping was proposed by tibshirani and knight
exercises ex
let and be probability density functions
jensen's inequality states that for random variable and convex function
use jensen's inequality to show that eq log is maximized as function of when
hence show that as stated below equation
consider the maximization of the log likelihood over distributions zm such that zm and zm zm
use lagrange multipliers to show that the solution is the conditional distribution zm pr zm as in
justify the estimate using the relationship pr pr pr
consider the bagging method of section
let our estimate be the spline smoother of section
consider the parametric bootstrap of equation applied to this estimator
show that if we bag using the parametric bootstrap to generate the bootstrap samples the bagging estimate bag converges to the original estimate as
suggest generalizations of each of the loss functions in figure to more than two classes and design an appropriate plot to compare them
consider the bone mineral density data of figure fit cubic smooth spline to the relative change in spinal bmd as function of age
use cross validation to estimate the optimal amount of smoothing
construct pointwise confidence bands for the underlying function compute the posterior mean and covariance for the true function via and compare the posterior bands to those obtained in
model inference and averaging compute bootstrap replicates of the fitted curves as in the bottom left panel of figure
compare the results to those obtained in and
em as minorization algorithm hunter and lange wu and lange
function to said to minorize function if for all in the domain
this is useful for maximizing since is easy to show that is non decreasing under the update xs argmaxx xs there are analogous definitions for majorization for minimizing function
the resulting algorithms are known as mm algorithms for minorizemaximize or majorize minimize
show that the em algorithm section is an example of an mm algorithm using log pr to minorize the observed data log likelihood
note that only the first term involves the relevant parameter
this is page printer opaque this additive models trees and related methods in this chapter we begin our discussion of some specific methods for supervised learning
these techniques each assume different structured form for the unknown regression function and by doing so they finesse the curse of dimensionality
of course they pay the possible price of misspecifying the model and so in each case there is tradeoff that has to be made
they take off where chapters left off
we describe five related techniques generalized additive models trees multivariate adaptive regression splines the patient rule induction method and hierarchical mixtures of experts
generalized additive models regression models play an important role in many data analyses providing prediction and classification rules and data analytic tools for understanding the importance of different inputs
although attractively simple the traditional linear model often fails in these situations in real life effects are often not linear
in earlier chapters we described techniques that used predefined basis functions to achieve nonlinearities
this section describes more automatic flexible statistical methods that may be used to identify and characterize nonlinear regression effects
these methods are called generalized additive models
in the regression setting generalized additive model has the form xp fp xp
additive models trees and related methods as usual xp represent predictors and is the outcome the fj are unspecified smooth nonparametric functions
if we were to model each function using an expansion of basis functions as in chapter the resulting model could then be fit by simple least squares
our approach here is different we fit each function using scatterplot smoother cubic smoothing spline or kernel smoother and provide an algorithm for simultaneously estimating all functions section
for two class classification recall the logistic regression model for binary data discussed in section
we relate the mean of the binary response pr to the predictors via linear regression model and the logit link function log xp
the additive logistic regression model replaces each linear term by more general functional form log fp xp where again each fj is an unspecified smooth function
while the nonparametric form for the functions fj makes the model more flexible the additivity is retained and allows us to interpret the model in much the same way as before
the additive logistic regression model is an example of generalized additive model
in general the conditional mean of response is related to an additive function of the predictors via link function fp xp
all three of these arise from exponential family sampling models which in addition include the gamma and negative binomial distributions
these families generate the well known class of generalized linear models which are all extended in the same way to generalized additive models
the functions fj are estimated in flexible manner using an algorithm whose basic building block is scatterplot smoother
the estimated function can then reveal possible nonlinearities in the effect of xj
not all
generalized additive models of the functions fj need to be nonlinear
we can easily mix in linear and other parametric forms with the nonlinear terms necessity when some of the inputs are qualitative variables factors
the nonlinear terms are not restricted to main effects either we can have nonlinear components in two or more variables or separate curves in xj for each level of the factor xk
additive models can replace linear models in wide variety of settings for example an additive decomposition of time series yt st tt where st is seasonal component tt is trend and is an error term
fitting additive models in this section we describe modular algorithm for fitting additive models and their generalizations
the building block is the scatterplot smoother for fitting nonlinear effects in flexible way
for concreteness we use as our scatterplot smoother the cubic smoothing spline described in chapter
the additive model has the form fj xj where the error term has mean zero
given observations xi yi criterion like the penalized sum of squares of section can be specified for this problem
prss fp yi fj xij bb fj tj dtj where the bb are tuning parameters
it can be shown that the minimizer of is an additive cubic spline model each of the functions fj is
additive models trees and related methods algorithm the backfitting algorithm for additive models
initialize yi
cycle sj yi xik fj fj xij
until the functions change less than prespecified threshold cubic spline in the component xj with knots at each of the unique values of xij
however without further restrictions on the model the solution is not unique
the constant is not identifiable since we can add or subtract any constants to each of the functions fj and adjust pn accordingly
the standard convention is to assume that fj xij the functions average zero over the data
it is easily seen that ave yi in this case
if in addition to this restriction the matrix of input values having ijth entry xij has full column rank then is strictly convex criterion and the minimizer is unique
if the matrix is singular then the linear part of the components fj cannot be uniquely determined while the nonlinear parts can
buja et al
furthermore simple iterative procedure exists for finding the solution
we set ave yi and it never changes
we apply cubic smoothing spline sj to the targets yi xik as function of xij to obtain new estimate fj
this is done for each predictor in turn using the current estimates of the other functions when computing yi fk xik
the process is continued until the estimates fj stabilize
this procedure given in detail in algorithm is known as backfitting and the resulting fit is analogous to multiple regression for linear models
in principle the second step in of algorithm is not needed since the smoothing spline fit to mean zero response has mean zero exercise
in practice machine rounding can cause slippage and the adjustment is advised
if we consider the operation of smoother sj only at the training points it can be represented by an operator matrix sj see section
then the degrees of freedom for the jth term are approximately computed as df trace sj by analogy with degrees of freedom for smoothers discussed in chapters and
for large class of linear smoothers sj backfitting is equivalent to gauss seidel algorithm for solving certain linear system of equations
details are given in exercise
for the logistic regression model and other generalized additive models the appropriate criterion is penalized log likelihood
to maximize it the backfitting procedure is used in conjunction with likelihood maximizer
the usual newton raphson routine for maximizing log likelihoods in generalized linear models can be recast as an irls iteratively reweighted least squares algorithm
this involves repeatedly fitting weighted linear regression of working response variable on the covariates each regression yields new value of the parameter estimates which in turn give new working responses and weights and the process is iterated see section
in the generalized additive model the weighted linear regression is simply replaced by weighted backfitting algorithm
we describe the algorithm in more detail for logistic regression below and more generally in chapter of hastie and tibshirani
example additive logistic regression probably the most widely used model in medical research is the logistic model for binary data
in this model the outcome can be coded as or with indicating an event like death or relapse of disease and indicating no event
we wish to model pr the probability of an event given values of the prognostic factors xp
the goal is usually to understand the roles of the prognostic factors rather than to classify new individuals
logistic models are also used in applications where one is interested in estimating the class probabilities for use in risk screening
apart from medical applications credit risk screening is popular application
the generalized additive logistic model has the form pr log fp xp
pr the functions fp are estimated by backfitting algorithm within newton raphson procedure shown in algorithm
additive models trees and related methods algorithm local scoring algorithm for the additive logistic regression model
compute starting values log where ave yi the sample proportion of ones and set
define xij and exp
iterate construct the working target variable yi zi construct weights wi fit an additive model to the targets zi with weights wi using weighted backfitting algorithm
this gives new estimates
continue step until the change in the functions falls below prespecified threshold
the additive model fitting in step of algorithm requires weighted scatterplot smoother
most smoothing procedures can accept observation weights exercise see chapter of hastie and tibshirani for further details
the additive logistic regression model can be generalized further to handle more than two classes using the multilogit formulation as outlined in section
while the formulation is straightforward extension of the algorithms for fitting such models are more complex
see yee and wild for details and the vgam software currently available from http www stat auckland ac nz yee
example predicting email spam we apply generalized additive model to the spam data introduced in chapter
the data consists of information from email messages in study to screen email for spam junk email
the data is publicly available at ftp ics uci edu and was donated by george forman from hewlett packard laboratories palo alto california
examples include business address internet
generalized additive models table
test data confusion matrix for the additive logistic regression model fit to the spam training data
the overall test error rate is
predicted class true class email spam email spam free and george
the characters are ch ch ch ch ch and ch
we coded spam as and email as zero
test set of size was randomly chosen leaving observations in the training set
generalized additive model was fit using cubic smoothing spline with nominal four degrees of freedom for each predictor
what this means is that for each predictor xj the smoothing spline parameter bb was chosen so that trace sj bb where sj bb is the smoothing spline operator matrix constructed using the observed values xij
this is convenient way of specifying the amount of smoothing in such complex model
most of the spam predictors have very long tailed distribution
before fitting the gam model we log transformed each variable actually log but the plots in figure are shown as function of the original variables
the test error rates are shown in table the overall error rate is
by comparison linear logistic regression has test error rate of
table shows the predictors that are highly significant in the additive model
for ease of interpretation in table the contribution for each variable is decomposed into linear component and the remaining nonlinear component
the top block of predictors are positively correlated with spam while the bottom block is negatively correlated
the linear component is weighted least squares linear fit of the fitted curve on the predictor while the nonlinear part is the residual
the linear component of an estimated
additive models trees and related methods table
significant predictors from the additive model fit to the spam training data
the coefficients represent the linear part of along with their standard errors and score
the nonlinear value is for test of nonlinearity of
name num df coefficient std
error score nonlinear value positive effects our over remove internet free business hpl ch
ch capmax captot negative effects hp george re edu function is summarized by the coefficient standard error and score the latter is the coefficient divided by its standard error and is considered significant if it exceeds the appropriate quantile of standard normal distribution
the column labeled nonlinear value is test of nonlinearity of the estimated function
note however that the effect of each predictor is fully adjusted for the entire effects of the other predictors not just for their linear parts
the predictors shown in the table were judged significant by at least one of the tests linear or nonlinear at the level two sided
figure shows the estimated functions for the significant predictors appearing in table
many of the nonlinear effects appear to account for strong discontinuity at zero
for example the probability of spam drops significantly as the frequency of george increases from zero but then does not change much after that
this suggests that one might replace each of the frequency predictors by an indicator variable for zero count and resort to linear logistic model
this gave test error rate of including the linear effects of the frequencies as well dropped the test error to
it appears that the nonlinearities in the additive model have an additional predictive power
generalized additive models internet remove over our our over remove internet business free hpl hp free business hp hpl george edu re george re edu captot capmax ch ch
spam analysis estimated functions for significant predictors
the rug plot along the bottom of each frame indicates the observed values of the corresponding predictor
for many of the predictors the nonlinearity picks up the discontinuity at zero
additive models trees and related methods it is more serious to classify genuine email message as spam since then good email would be filtered out and would not reach the user
we can alter the balance between the class error rates by changing the losses see section
if we assign loss for predicting true class as class and for predicting true class as class then the estimated bayes rule predicts class if its probability is greater than
for example if we take then the true class and class error rates change to and
more ambitiously we can encourage the model to fit better data in the class by using weights for the class observations and for the class observations
as above we then use the estimated bayes rule to predict
this gave error rates of and in true class and class respectively
we discuss below the issue of unequal losses further in the context of tree based models
after fitting an additive model one should check whether the inclusion of some interactions can significantly improve the fit
this can be done manually by inserting products of some or all of the significant inputs or automatically via the mars procedure section
this example uses the additive model in an automatic fashion
as data analysis tool additive models are often used in more interactive fashion adding and dropping terms to determine their effect
by calibrating the amount of smoothing in terms of df one can move seamlessly between linear models df and partially linear models where some terms are modeled more flexibly
see hastie and tibshirani for more details
summary additive models provide useful extension of linear models making them more flexible while still retaining much of their interpretability
the familiar tools for modeling and inference in linear models are also available for additive models seen for example in table
the backfitting procedure for fitting these models is simple and modular allowing one to choose fitting method appropriate for each input variable
as result they have become widely used in the statistical community
however additive models can have limitations for large data mining applications
the backfitting algorithm fits all predictors which is not feasible or desirable when large number are available
the bruto procedure hastie and tibshirani chapter combines backfitting with selection of inputs but is not designed for large data mining problems
there has also been recent work using lasso type penalties to estimate sparse additive models for example the cosso procedure of lin and zhang and the spam proposal of ravikumar et al
for large problems forward stagewise approach such as boosting chapter is more effective and also allows for interactions to be included in the model
tree based methods tree based methods background tree based methods partition the feature space into set of rectangles and then fit simple model like constant in each one
they are conceptually simple yet powerful
we first describe popular method for tree based regression and classification called cart and later contrast it with major competitor
let's consider regression problem with continuous response and inputs and each taking values in the unit interval
the top left panel of figure shows partition of the feature space by lines that are parallel to the coordinate axes
in each partition element we can model with different constant
however there is problem although each partitioning line has simple description like some of the resulting regions are complicated to describe
to simplify matters we restrict attention to recursive binary partitions like that in the top right panel of figure
we first split the space into two regions and model the response by the mean of in each region
we choose the variable and split point to achieve the best fit
then one or both of these regions are split into two more regions and this process is continued until some stopping rule is applied
for example in the top right panel of figure we first split at
then the region is split at and the region is split at
finally the region is split at
the result of this process is partition into the five regions shown in the figure
the corresponding regression model predicts with constant cm in region rm that is cm rm
this same model can be represented by the binary tree in the bottom left panel of figure
the full dataset sits at the top of the tree
observations satisfying the condition at each junction are assigned to the left branch and the others to the right branch
the terminal nodes or leaves of the tree correspond to the regions
the bottom right panel of figure is perspective plot of the regression surface from this model
for illustration we chose the node means to make this plot
key advantage of the recursive binary tree is its interpretability
the feature space partition is fully described by single tree
with more than two inputs partitions like that in the top right panel of figure are difficult to draw but the binary tree representation works in the same way
this representation is also popular among medical scientists perhaps because it mimics the way that doctor thinks
the tree stratifies the
partitions and cart
top right panel shows partition of two dimensional feature space by recursive binary splitting as used in cart applied to some fake data
top left panel shows general partition that cannot be obtained from recursive binary splitting
bottom left panel shows the tree corresponding to the partition in the top right panel and perspective plot of the prediction surface appears in the bottom right panel
tree based methods population into strata of high and low outcome on the basis of patient characteristics
regression trees we now turn to the question of how to grow regression tree
our data consists of inputs and response for each of observations that is xi yi for with xi xi xi xip
the algorithm needs to automatically decide on the splitting variables and split points and also what topology shape the tree should have
suppose first that we have partition into regions rm and we model the response as constant cm in each region cm rm
if we adopt as our criterion minimization of the sum of squares yi xi it is easy to see that the best is just the average of yi in region rm ave yi xi rm
now finding the best binary partition in terms of minimum sum of squares is generally computationally infeasible
hence we proceed with greedy algorithm
starting with all of the data consider splitting variable and split point and define the pair of half planes xj and xj
then we seek the splitting variable and split point that solve min min yi min yi
xi xi for any choice and the inner minimization is solved by ave yi xi and ave yi xi
for each splitting variable the determination of the split point can be done very quickly and hence by scanning through all of the inputs determination of the best pair is feasible
having found the best split we partition the data into the two resulting regions and repeat the splitting process on each of the two regions
then this process is repeated on all of the resulting regions
how large should we grow the tree
clearly very large tree might overfit the data while small tree might not capture the important structure
additive models trees and related methods tree size is tuning parameter governing the model's complexity and the optimal tree size should be adaptively chosen from the data
one approach would be to split tree nodes only if the decrease in sum of squares due to the split exceeds some threshold
this strategy is too short sighted however since seemingly worthless split might lead to very good split below it
the preferred strategy is to grow large tree stopping the splitting process only when some minimum node size say is reached
then this large tree is pruned using cost complexity pruning which we now describe
we define subtree to be any tree that can be obtained by pruning that is collapsing any number of its internal non terminal nodes
we index terminal nodes by with node representing region rm
let denote the number of terminal nodes in
letting nm xi rm yi nm xi rm qm yi nm xi rm we define the cost complexity criterion nm qm
the idea is to find for each the subtree to minimize
the tuning parameter governs the tradeoff between tree size and its goodness of fit to the data
large values of result in smaller trees and conversely for smaller values of
as the notation suggests with the solution is the full tree
we discuss how to adaptively choose below
for each one can show that there is unique smallest subtree that minimizes
to find we use weakest link pruning we successively collapse the internal node that produces the smallest per node increase in and continue until we produce the single node root tree
this gives finite sequence of subtrees and one can show this sequence must contain
see breiman et al or ripley for details
estimation of is achieved by fiveor tenfold cross validation we choose the value to minimize the cross validated sum of squares
our final tree is
classification trees if the target is classification outcome taking values the only changes needed in the tree algorithm pertain to the criteria for splitting nodes and pruning the tree
for regression we used the squared error node
node impurity measures for two class classification as function of the proportion in class
cross entropy has been scaled to pass through impurity measure qm defined in but this is not suitable for classification
in node representing region rm with nm observations let mk yi nm xi rm the proportion of class observations in node
we classify the observations in node to class arg maxk mk the majority class in node
different measures qm of node impurity include the following misclassification error nm rm yi mk
pk gini index mk mk mk mk
pk cross entropy or deviance mk log mk
for two classes if is the proportion in the second class these three measures are max and log log respectively
they are shown in figure
all three are similar but crossentropy and the gini index are differentiable and hence more amenable to numerical optimization
comparing and we see that we need to weight the node impurity measures by the number nml and nmr of observations in the two child nodes created by splitting node
in addition cross entropy and the gini index are more sensitive to changes in the node probabilities than the misclassification rate
for example in two class problem with observations in each class denote this by suppose one split created nodes and while
additive models trees and related methods the other created nodes and
both splits produce misclassification rate of but the second split produces pure node and is probably preferable
both the gini index and cross entropy are lower for the second split
for this reason either the gini index or cross entropy should be used when growing the tree
to guide cost complexity pruning any of the three measures can be used but typically it is the misclassification rate
the gini index can be interpreted in two interesting ways
rather than classify observations to the majority class in the node we could classify them to class withpprobability mk
then the training error rate of this rule in the node is mk mk the gini index
similarly if we code each observation as for class and zero otherwise the variance over the node of this response is mk mk
summing over classes again gives the gini index
other issues categorical predictors when splitting predictor having possible unordered values there are possible partitions of the values into two groups and the computations become prohibitive for large
however with outcome this computation simplifies
we order the predictor classes according to the proportion falling in outcome class
then we split this predictor as if it were an ordered predictor
one can show this gives the optimal split in terms of cross entropy or gini index among all possible splits
this result also holds for quantitative outcome and square error loss the categories are ordered by increasing mean of the outcome
although intuitive the proofs of these assertions are not trivial
the proof for binary outcomes is given in breiman et al and ripley the proof for quantitative outcomes can be found in fisher
for multicategory outcomes no such simplifications are possible although various approximations have been proposed loh and vanichsetakul
the partitioning algorithm tends to favor categorical predictors with many levels the number of partitions grows exponentially in and the more choices we have the more likely we can find good one for the data at hand
this can lead to severe overfitting if is large and such variables should be avoided
the loss matrix in classification problems the consequences of misclassifying observations are more serious in some classes than others
for example it is probably worse to predict that person will not have heart attack when he she actually will than vice versa
to account for this we define loss matrix with lkk being the loss incurred for classifying class observation as class
typically no loss is incurred for correct classifications
tree based methods that is lkk
to incorporatepthe losses into the modeling process we could modify the gini index to lkk mk mk this would be the expected loss incurred by the randomized rule
this works for the multiclass case but in the two class case has no effect since the coefficient of mk mk is lkk lk
for two classes better approach is to weight the observations in class by lkk
this can be used in the multiclass case only if as function of lkk doesn't depend on
observation weighting can be used with the deviance as well
the effect of observation weighting is to alter the prior probability on the classes
in terminal node the empirical bayes rule implies that we classify to class arg mink
missing predictor values suppose our data has some missing predictor values in some or all of the variables
we might discard any observation with some missing values but this could lead to serious depletion of the training set
alternatively we might try to fill in impute the missing values with say the mean of that predictor over the nonmissing observations
for tree based models there are two better approaches
the first is applicable to categorical predictors we simply make new category for missing
from this we might discover that observations with missing values for some measurement behave differently than those with nonmissing values
the second more general approach is the construction of surrogate variables
when considering predictor for split we use only the observations for which that predictor is not missing
having chosen the best primary predictor and split point we form list of surrogate predictors and split points
the first surrogate is the predictor and corresponding split point that best mimics the split of the training data achieved by the primary split
the second surrogate is the predictor and corresponding split point that does second best and so on
when sending observations down the tree either in the training phase or during prediction we use the surrogate splits in order if the primary splitting predictor is missing
surrogate splits exploit correlations between predictors to try and alleviate the effect of missing data
the higher the correlation between the missing predictor and the other predictors the smaller the loss of information due to the missing value
the general problem of missing data is discussed in section
why binary splits
rather than splitting each node into just two groups at each stage as above we might consider multiway splits into more than two groups
while this can sometimes be useful it is not good general strategy
the problem is that multiway splits fragment the data too quickly leaving insufficient data at the next level down
hence we would want to use such splits only when needed
since multiway splits can be achieved by series of binary splits the latter are preferred
additive models trees and related methods other tree building procedures the discussion above focuses on the cart classification and regression tree implementation of trees
the other popular methodology is id and its later versions and quinlan
early versions of the program were limited to categorical predictors and used top down rule with no pruning
with more recent developments has become quite similar to cart
the most significant feature unique to is scheme for deriving rule sets
after tree is grown the splitting rules that define the terminal nodes can sometimes be simplified that is one or more condition can be dropped without changing the subset of observations that fall in the node
we end up with simplified set of rules defining each terminal node these no longer follow tree structure but their simplicity might make them more attractive to the user
linear combination splits rather than restricting splits to be of thepform xj one can allow splits along linear combinations of the form aj xj
the weights aj and split point are optimized to minimize the relevant criterion such as the gini index
while this can improve the predictive power of the tree it can hurt interpretability
computationally the discreteness of the split point search precludes the use of smooth optimization for the weights
better way to incorporate linear combination splits is in the hierarchical mixtures of experts hme model the topic of section
instability of trees one major problem with trees is their high variance
often small change in the data can result in very different series of splits making interpretation somewhat precarious
the major reason for this instability is the hierarchical nature of the process the effect of an error in the top split is propagated down to all of the splits below it
one can alleviate this to some degree by trying to use more stable split criterion but the inherent instability is not removed
it is the price to be paid for estimating simple tree based structure from the data
bagging section averages many trees to reduce this variance
lack of smoothness another limitation of trees is the lack of smoothness of the prediction surface as can be seen in the bottom right panel of figure
in classification with loss this doesn't hurt much since bias in estimation of the class probabilities has limited effect
however this can degrade performance in the regression setting where we would normally expect the underlying function to be smooth
the mars procedure described in section
tree based methods table
spam data confusion rates for the node tree chosen by cross validation on the test data
overall error rate is
predicted true email spam email spam can be viewed as modification of cart designed to alleviate this lack of smoothness
difficulty in capturing additive structure another problem with trees is their difficulty in modeling additive structure
in regression suppose for example that where is zero mean noise
then binary tree might make its first split on near
at the next level down it would have to split both nodes on at in order to capture the additive structure
this might happen with sufficient data but the model is given no special encouragement to find such structure
if there were ten rather than two additive effects it would take many fortuitous splits to recreate the structure and the data analyst would be hard pressed to recognize it in the estimated tree
the blame here can again be attributed to the binary tree structure which has both advantages and drawbacks
again the mars method section gives up this tree structure in order to capture additive structure
spam example continued we applied the classification tree methodology to the spam example introduced earlier
we used the deviance measure to grow the tree and misclassification rate to prune it
figure shows the fold cross validation error rate as function of the size of the pruned tree along with standard errors of the mean from the ten replications
the test error curve is shown in orange
note that the cross validation error rates are indexed by sequence of values of and not tree size for trees grown in different folds value of might imply different sizes
the sizes shown at the base of the plot refer to the sizes of the pruned original tree
the error flattens out at around terminal nodes giving the pruned tree in figure
of the distinct features chosen by the tree overlap with the significant features in the additive model table
the overall error rate shown in table is about higher than for the additive model in table
consider the rightmost branches of the tree
we branch to the right with spam warning if more than of the characters are the sign
results for spam example
the blue curve is the fold cross validation estimate of misclassification rate as function of tree size with standard error bars
the minimum occurs at tree size with about terminal nodes using the one standard error rule
the orange curve is the test error which tracks the cv error quite closely
the cross validation is indexed by values of shown above
the tree sizes shown below refer to the size of the original tree indexed by
however if in addition the phrase hp occurs frequently then this is likely to be company business and we classify as email
all of the cases in the test set satisfying these criteria were correctly classified
if the second condition is not met and in addition the average length of repeated capital letters capave is larger than then we classify as spam
of the test cases only seven were misclassified
in medical classification problems the terms sensitivity and specificity are used to characterize rule
they are defined as follows sensitivity probability of predicting disease given true state is disease
specificity probability of predicting non disease given true state is nondisease
tree based methods email ch ch email spam remove hp remove hp email spam spam email ch
george capave ch
the pruned tree for the spam example
the split variables are shown in blue on the branches and the classification is shown in every node the numbers under the terminal nodes indicate misclassification rates on the test data
roc curves for the classification rules fit to the spam data
curves that are closer to the northeast corner represent better classifiers
in this case the gam classifier dominates the trees
the weighted tree achieves better sensitivity for higher specificity than the unweighted tree
the numbers in the legend represent the area under the curve
if we think of spam and email as the presence and absence of disease respectively then from table we have sensitivity specificity
in this analysis we have used equal losses
as before let lkk be the loss associated with predicting class object as class
by varying the relative sizes of the losses and we increase the sensitivity and decrease the specificity of the rule or vice versa
in this example we want to avoid marking good email as spam and thus we want the specificity to be very high
we can achieve this by setting say with
the bayes rule in each terminal node classifies to class spam if the proportion of spam is and class zero otherwise
prim bump hunting receiver operating characteristic curve roc is commonly used summary for assessing the tradeoff between sensitivity and specificity
it is plot of the sensitivity versus specificity as we vary the parameters of classification rule
varying the loss between and and applying bayes rule to the node tree selected in figure produced the roc curve shown in figure
the standard error of each curve near is approximately and hence the standard error of the difference is about
we see that in order to achieve specificity of close to the sensitivity has to drop to about
the area under the curve is commonly used quantitative summary extending the curve linearly in each direction so that it is defined over the area is approximately
for comparison we have included the roc curve for the gam model fit to these data in section it gives better classification rule for any loss with an area of
rather than just modifying the bayes rule in the nodes it is better to take full account of the unequal losses in growing the tree as was done in section
with just two classes and losses may be incorporated into the tree growing process by using weight lk for an observation in class
here we chose and fit the same size tree as before
this tree has higher sensitivity at high values of the specificity than the original tree but does more poorly at the other extreme
its top few splits are the same as the original tree and then it departs from it
for this application the tree grown using is clearly better than the original tree
the area under the roc curve used above is sometimes called the cstatistic
interestingly it can be shown that the area under the roc curve is equivalent to the mann whitney statistic or wilcoxon rank sum test for the median difference between the prediction scores in the two groups hanley and mcneil
for evaluating the contribution of an additional predictor when added to standard model the statistic may not be an informative measure
the new predictor can be very significant in terms of the change in model deviance but show only small increase in the cstatistic
for example removal of the highly significant term george from the model of table results in decrease in the statistic of less than
instead it is useful to examine how the additional predictor changes the classification on an individual sample basis
good discussion of this point appears in cook
prim bump hunting tree based methods for regression partition the feature space into boxshaped regions to try to make the response averages in each box as differ
additive models trees and related methods ent as possible
the splitting rules defining the boxes are related to each through binary tree facilitating their interpretation
the patient rule induction method prim also finds boxes in the feature space but seeks boxes in which the response average is high
hence it looks for maxima in the target function an exercise known as bump hunting
if minima rather than maxima are desired one simply works with the negative response values
prim also differs from tree based partitioning methods in that the box definitions are not described by binary tree
this makes interpretation of the collection of rules more difficult however by removing the binary tree constraint the individual rules are often simpler
the main box construction method in prim works from the top down starting with box containing all of the data
the box is compressed along one face by small amount and the observations then falling outside the box are peeled off
the face chosen for compression is the one resulting in the largest box mean after the compression is performed
then the process is repeated stopping when the current box contains some minimum number of data points
this process is illustrated in figure
there are data points uniformly distributed over the unit square
the color coded plot indicates the response taking the value red when and and zero blue otherwise
the panels shows the successive boxes found by the top down peeling procedure peeling off proportion of the remaining data points at each stage
figure shows the mean of the response values in the box as the box is compressed
after the top down sequence is computed prim reverses the process expanding along any edge if such an expansion increases the box mean
this is called pasting
since the top down procedure is greedy at each step such an expansion is often possible
the result of these steps is sequence of boxes with different numbers of observation in each box
cross validation combined with the judgment of the data analyst is used to choose the optimal box size
denote by the indices of the observations in the box found in step
the prim procedure then removes the observations in from the training set and the two step process top down peeling followed by bottom up pasting is repeated on the remaining dataset
this entire process is repeated several times producing sequence of boxes bk
each box is defined by set of rules involving subset of predictors like and
summary of the prim procedure is given algorithm
prim can handle categorical predictor by considering all partitions of the predictor as in cart
missing values are also handled in manner similar to cart
prim is designed for regression quantitative response
illustration of prim algorithm
there are two classes indicated by the blue class and red class points
the procedure starts with rectangle broken black lines surrounding all of the data and then peels away points along one edge by prespecified amount in order to maximize the mean of the points remaining in the box
starting at the top left panel the sequence of peelings is shown until pure red region is isolated in the bottom right panel
the iteration number is indicated at the top of each panel
box mean as function of number of observations in the box
additive models trees and related methods algorithm patient rule induction method
start with all of the training data and maximal box containing all of the data
consider shrinking the box by compressing one face so as to peel off the proportion of observations having either the highest values of predictor xj or the lowest
choose the peeling that produces the highest response mean in the remaining box
typically or
repeat step until some minimal number of observations say remain in the box
expand the box along any face as long as the resulting box mean increases
steps give sequence of boxes with different numbers of observations in each box
use cross validation to choose member of the sequence
call the box
remove the data in box from the dataset and repeat steps to obtain second box and continue to get as many boxes as desired variable two class outcome can be handled simply by coding it as and
there is no simple way to deal with classes simultaneously one approach is to run prim separately for each class versus baseline class
an advantage of prim over cart is its patience
because of its binary splits cart fragments the data quite quickly
assuming splits of equal size with observations it can only make log splits before running out of data
if prim peels off proportion of training points at each stage it can perform approximately log log peeling steps before running out of data
for example if and then log while log log
taking into account that there must be an integer number of observations at each stage prim in fact can peel only times
in any case the ability of prim to be more patient should help the top down greedy algorithm find better solution
spam example continued we applied prim to the spam data with the response coded as for spam and for email
the first two boxes found by prim are summarized below
mars multivariate adaptive regression splines rule global mean box mean box support training test ch
capave your rule captot edu re ch rule remain mean box mean box support training test bd remove rule george the box support is the proportion of observations falling in the box
the first box is purely spam and contains about of the test data
the second box contains of the test observations of which are spam
together the two boxes contain of the data and are about spam
the next few boxes not shown are quite small containing only about of the data
the predictors are listed in order of importance
interestingly the top splitting variables in the cart tree figure do not appear in prim's first box
mars multivariate adaptive regression splines mars is an adaptive procedure for regression and is well suited for highdimensional problems large number of inputs
it can be viewed as generalization of stepwise linear regression or modification of the cart method to improve the latter's performance in the regression setting
we introduce mars from the first point of view and later make the connection to cart
mars uses expansions in piecewise linear basis functions of the form and
the means positive part so bd bd if if and otherwise otherwise
the basis functions solid orange and broken blue used by mars
as an example the functions and are shown in figure
each function is piecewise linear with knot at the value
in the terminology of chapter these are linear splines
we call the two functions reflected pair in the discussion below
the idea is to form reflected pairs for each input xj with knots at each observed value xij of that input
therefore the collection of basis functions is xj xj xn
if all of the input values are distinct there are basis functions altogether
note that although each basis function depends only on single xj for example xj it is considered as function over the entire input space irp
the model building strategy is like forward stepwise linear regression but instead of using the original inputs we are allowed to use functions from the set and their products
thus the model has the form hm where each hm is function in or product of two or more such functions
given choice for the hm the coefficients are estimated by minimizing the residual sum of squares that is by standard linear regression
the real art however is in the construction of the functions hm
we start with only the constant function in our model and all functions in the set are candidate functions
this is depicted in figure
at each stage we consider as new basis function pair all products of function hm in the model set with one of the reflected pairs in
we add to the model the term of the form xj xj
schematic of the mars forward model building procedure
on the left are the basis functions currently in the model initially this is the constant function
on the right are all candidate basis functions to be considered in building the model
these are pairs of piecewise linear basis functions as in figure with knots at all unique observed values xij of each predictor xj
at each stage we consider all products of candidate pair with basis function in the model
the product that decreases the residual error the most is added into the current model
above we illustrate the first three steps of the procedure with the selected functions shown in red
the function resulting from multiplication of two piecewise linear mars basis functions that produces the largest decrease in training error
here and are coefficients estimated by least squares along with all the other coefficients in the model
then the winning products are added to the model and the process is continued until the model set contains some preset maximum number of terms
for example at the first stage we consider adding to the model function of the form xj xj xij since multiplication by the constant function just produces the function itself
suppose the best choice is
then this pair of basis functions is added to the set and at the next stage we consider including pair of products the form hm xj and hm xj xij where for hm we have the choices or
the third choice produces functions such as depicted in figure
at the end of this process we have large model of the form
this model typically overfits the data and so backward deletion procedure is applied
the term whose removal causes the smallest increase in residual squared error is deleted from the model at each stage producing an estimated best model bb of each size number of terms bb
one could use cross validation to estimate the optimal value of bb but for computational
mars multivariate adaptive regression splines savings the mars procedure instead uses generalized cross validation
this criterion is defined as pn yi bb xi gcv bb
bb the value bb is the effective number of parameters in the model this accounts both for the number of terms in the models plus the number of parameters used in selecting the optimal positions of the knots
some mathematical and simulation results suggest that one should pay price of three parameters for selecting knot in piecewise linear regression
thus if there are linearly independent basis functions in the model and knots were selected in the forward process the formula is bb ck where
when the model is restricted to be additive details belowa penalty of is used
using this we choose the model along the backward sequence that minimizes gcv bb
why these piecewise linear basis functions and why this particular model strategy
key property of the functions of figure is their ability to operate locally they are zero over part of their range
when they are multiplied together as in figure the result is nonzero only over the small part of the feature space where both component functions are nonzero
as result the regression surface is built up parsimoniously using nonzero components locally only where they are needed
this is important since one should spend parameters carefully in high dimensions as they can run out quickly
the use of other basis functions such as polynomials would produce nonzero product everywhere and would not work as well
the second important advantage of the piecewise linear basis function concerns computation
consider the product of function in with each of the reflected pairs for an input xj
this appears to require the fitting of single input linear regression models each of which uses operations making total of operations
however we can exploit the simple form of the piecewise linear function
we first fit the reflected pair with rightmost knot
as the knot is moved successively one position at time to the left the basis functions differ by zero over the left part of the domain and by constant over the right part
hence after each such move we can update the fit in operations
this allows us to try every knot in only operations
the forward modeling strategy in mars is hierarchical in the sense that multiway products are built up from products involving terms already in the model
for example four way product can only be added to the model if one of its three way components is already in the model
the philosophy here is that high order interaction will likely only exist if some of its lowerorder footprints exist as well
this need not be true but is reasonable working assumption and avoids the search over an exponentially growing space of alternatives
spam data test error misclassification rate for the mars procedure as function of the rank number of independent basis functions in the model
there is one restriction put on the formation of model terms each input can appear at most once in product
this prevents the formation of higher order powers of an input which increase or decrease too sharply near the boundaries of the feature space
such powers can be approximated in more stable way with piecewise linear functions
useful option in the mars procedure is to set an upper limit on the order of interaction
for example one can set limit of two allowing pairwise products of piecewise linear functions but not threeor higherway products
this can aid in the interpretation of the final model
an upper limit of one results in an additive model
spam example continued we applied mars to the spam data analyzed earlier in this chapter
to enhance interpretability we restricted mars to second degree interactions
although the target is two class variable we used the squared error loss function nonetheless see section
figure shows the test error misclassification rate as function of the rank number of independent basis functions in the model
the error rate levels off at about which is slightly higher than that of the generalized additive model discussed earlier
gcv chose model size of which is roughly the smallest model giving optimal performance
the leading interactions found by mars involved inputs ch remove ch free and hp captot
however these interactions give no improvement in performance over the generalized additive model
mars multivariate adaptive regression splines example simulated data here we examine the performance of mars in three contrasting scenarios
there are observations and the predictors xp and errors have independent standard normal distributions
scenario the data generation model is
the noise standard deviation was chosen so that the signal tonoise ratio was about
we call this the tensor product scenario the product term gives surface that looks like that of figure
scenario this is the same as scenario but with total predictors that is there are inputs that are independent of the response
scenario this has the structure of neural network
scenarios and are ideally suited for mars while scenario contains high order interactions and may be difficult for mars to approximate
we ran five simulations from each model and recorded the results
in scenario mars typically uncovered the correct model almost perfectly
in scenario it found the correct structure but also found few extraneous terms involving other predictors
let be the true mean of and let mse avex test mse avex test
these represent the mean square error of the constant model and the fitted mars model estimated by averaging at the test values of
table shows the proportional decrease in model error or for each scenario mse mse
mse the values shown are means and standard error over the five simulations
the performance of mars is degraded only slightly by the inclusion of the useless inputs in scenario it performs substantially worse in scenario
additive models trees and related methods table
proportional decrease in model error when mars is applied to three different scenarios
scenario mean
tensor product tensor product neural network other issues mars for classification the mars method and algorithm can be extended to handle classification problems
several strategies have been suggested
for two classes one can code the output as and treat the problem as regression we did this for the spam example
for more than two classes one can use the indicator response approach described in section
one codes the response classes via indicator variables and then performs multi response mars regression
for the latter we use common set of basis functions for all response variables
classification is made to the class with the largest predicted response value
there are however potential masking problems with this approach as described in section
generally superior approach is the optimal scoring method discussed in section
stone et al developed hybrid of mars called polymars specifically designed to handle classification problems
it uses the multiple logistic framework described in section
it grows the model in forward stagewise fashion like mars but at each stage uses quadratic approximation to the multinomial log likelihood to search for the next basis function pair
once found the enlarged model is fit by maximum likelihood and the process is repeated
relationship of mars to cart although they might seem quite different the mars and cart strategies actually have strong similarities
with these changes the mars forward procedure is the same as the cart tree growing algorithm
multiplying step function by pair of reflected
hierarchical mixtures of experts step functions is equivalent to splitting node at the step
the second restriction implies that node may not be split more than once and leads to the attractive binary tree representation of the cart model
on the other hand it is this restriction that makes it difficult for cart to model additive structures
mars forgoes the tree structure and gains the ability to capture additive effects
mixed inputs mars can handle mixed predictors quantitative and qualitative in natural way much like cart does
mars considers all possible binary partitions of the categories for qualitative predictor into two groups
each such partition generates pair of piecewise constant basis functionsindicator functions for the two sets of categories
this basis pair is now treated as any other and is used in forming tensor products with other basis functions already in the model
hierarchical mixtures of experts the hierarchical mixtures of experts hme procedure can be viewed as variant of tree based methods
the main difference is that the tree splits are not hard decisions but rather soft probabilistic ones
at each node an observation goes left or right with probabilities depending on its input values
this has some computational advantages since the resulting parameter optimization problem is smooth unlike the discrete split point search in the tree based approach
the soft splits might also help in prediction accuracy and provide useful alternative description of the data
there are other differences between hmes and the cart implementation of trees
in an hme linear or logistic regression model is fit in each terminal node instead of constant as in cart
the splits can be multiway not just binary and the splits are probabilistic functions of linear combination of inputs rather than single input as in the standard use of cart
however the relative merits of these choices are not clear and most were discussed at the end of section
simple two level hme model in shown in figure
it can be thought of as tree with soft splits at each non terminal node
however the inventors of this methodology use different terminology
the terminal nodes are called experts and the non terminal nodes are called gating networks
the idea is that each expert provides an opinion prediction about the response and these are combined together by the gating networks
as we will see the model is formally mixture model and the two level model in the figure can be extend to multiple levels hence the name hierarchical mixtures of experts
two level hierarchical mixture of experts hme model
consider the regression or classification problem as described earlier in the chapter
the data is xi yi with yi either continuous or binary valued response and xi vector valued input
for ease of notation we assume that the first element of xi is one to account for intercepts
here is how an hme is defined
the top gating network has the output gj pk kt where each is vector of unknown parameters
this represents soft way split in figure
each gj is the probability of assigning an observation with feature vector to the jth branch
notice that with groups if we take the coefficient of one of the elements of to be then we get logistic curve with infinite slope
in this case the gating probabilities are either or corresponding to hard split on that input
at the second level the gating networks have similar form pk jk

hierarchical mixtures of experts this is the probability of assignment to the th branch given assignment to the jth branch at the level above
at each expert terminal node we have model for the response variable of the form pr
this differs according to the problem
regression the gaussian linear regression model is used with and
classification the linear logistic regression model is used pr
denoting the collection of all parameters by the total probability that is pr gj pr
this is mixture model with the mixture probabilities determined by the gating network models
pto estimate the parameters we maximize the log likelihood of the data log pr yi xi over the parameters in
the most convenient method for doing this is the em algorithm which we describe for mixtures in section
we define latent variables all of which are zero except for single one
we interpret these as the branching decisions made by the top level gating network
similarly we define latent variables to describe the gating decisions at the second level
in the step the em algorithm computes the expectations of the and given the current values of the parameters
these expectations are then used as observation weights in the step of the procedure to estimate the parameters in the expert networks
the parameters in the internal nodes are estimated by version of multiple logistic regression
the expectations of the or are probability profiles and these are used as the response vectors for these logistic regressions
the hierarchical mixtures of experts approach is promising competitor to cart trees
by using soft splits rather than hard decision rules it can capture situations where the transition from low to high response is gradual
the log likelihood is smooth function of the unknown weights and hence is amenable to numerical optimization
the model is similar to cart with linear combination splits but the latter is more difficult to optimize
additive models trees and related methods the other hand to our knowledge there are no methods for finding good tree topology for the hme model as there are in cart
typically one uses fixed tree of some depth possibly the output of the cart procedure
the emphasis in the research on hmes has been on prediction rather than interpretation of the final model
close cousin of the hme is the latent class model lin et al which typically has only one layer here the nodes or latent classes are interpreted as groups of subjects that show similar response behavior
missing data it is quite common to have observations with missing values for one or more input features
the usual approach is to impute fill in the missing values in some way
however the first issue in dealing with the problem is determining whether the missing data mechanism has distorted the observed data
roughly speaking data are missing at random if the mechanism resulting in its omission is independent of its unobserved value
more precise definition is given in little and rubin
suppose is the response vector and is the matrix of inputs some of which are missing
denote by xobs the observed entries in and let zobs xobs
finally if is an indicator matrix with ijth entry if xij is missing and zero otherwise then the data is said to be missing at random mar if the distribution of depends on the data only through zobs pr pr zobs
here are any parameters in the distribution of
data are said to be missing completely at random mcar if the distribution of doesn't depend on the observed or missing data pr pr
mcar is stronger assumption than mar most imputation methods rely on mcar for their validity
for example if patient's measurement was not taken because the doctor felt he was too sick that observation would not be mar or mcar
in this case the missing data mechanism causes our observed training data to give distorted picture of the true population and data imputation is dangerous in this instance
often the determination of whether features are mcar must be made from information about the data collection process
for categorical features one way to diagnose this problem is to code missing as an additional class
then we fit our model to the training data and see if class missing is predictive of the response
missing data assuming the features are missing completely at random there are number of ways of proceeding
discard observations with any missing values
rely on the learning algorithm to deal with missing values in its training phase
impute all missing values before training
approach can be used if the relative amount of missing data is small but otherwise should be avoided
regarding cart is one learning algorithm that deals effectively with missing values through surrogate splits section
mars and prim use similar approaches
in generalized additive modeling all observations missing for given input feature are omitted when the partial residuals are smoothed against that feature in the backfitting algorithm and their fitted values are set to zero
since the fitted curves have mean zero when the model includes an intercept this amounts to assigning the average fitted value to the missing observations
for most learning methods the imputation approach is necessary
the simplest tactic is to impute the missing value with the mean or median of the nonmissing values for that feature
note that the above procedure for generalized additive models is analogous to this
if the features have at least some moderate degree of dependence one can do better by estimating predictive model for each feature given the other features and then imputing each missing value by its prediction from the model
in choosing the learning method for imputation of the features one must remember that this choice is distinct from the method used for predicting from
thus flexible adaptive method will often be preferred even for the eventual purpose of carrying out linear regression of on
in addition if there are many missing feature values in the training set the learning method must itself be able to deal with missing feature values
cart therefore is an ideal choice for this imputation engine
after imputation missing values are typically treated as if they were actually observed
this ignores the uncertainty due to the imputation which will itself introduce additional uncertainty into estimates and predictions from the response model
one can measure this additional uncertainty by doing multiple imputations and hence creating many different training sets
the predictive model for can be fit to each training set and the variation across training sets can be assessed
if cart was used for the imputation engine the multiple imputations could be done by sampling from the values in the corresponding terminal nodes
additive models trees and related methods computational considerations with observations and predictors additive model fitting requires some number mp of applications of one dimensional smoother or regression method
the required number of cycles of the backfitting algorithm is usually less than and often less than and depends on the amount of correlation in the inputs
with cubic smoothing splines for example log operations are needed for an initial sort and operations for the spline fit
hence the total operations for an additive model fit is pn log mpn
trees require pn log operations for an initial sort for each predictor and typically another pn log operations for the split computations
if the splits occurred near the edges of the predictor ranges this number could increase to
mars requires pmn operations to add basis function to model with terms already present from pool of predictors
hence to build an term model requires pm computations which can be quite prohibitive if is reasonable fraction of
each of the components of an hme are typically inexpensive to fit at each step for the regressions and for class logistic regression
the em algorithm however can take long time to converge and so sizable hme models are considered costly to fit
bibliographic notes the most comprehensive source for generalized additive models is the text of that name by hastie and tibshirani
different applications of this work in medical problems are discussed in hastie et al and hastie and herman and the software implementation in splus is described in chambers and hastie
green and silverman discuss penalization and spline models in variety of settings
efron and tibshirani give an exposition of modern developments in statistics including generalized additive models for nonmathematical audience
classification and regression trees date back at least as far as morgan and sonquist
we have followed the modern approaches of breiman et al and quinlan
the prim method is due to friedman and fisher while mars is introduced in friedman with an additive precursor in friedman and silverman
hierarchical mixtures of experts were proposed in jordan and jacobs see also jacobs et al
exercises exercises ex
show that smoothing spline fit of yi to xi preserves the linear part of the fit
in other words if yi ri where represents the linear regression fits and is the smoothing matrix then sy sr
show that the same is true for local linear regression section
hence argue that the adjustment step in the second line of in algorithm is unnecessary
let be known matrix be known vector and be an unknown vector
gauss seidel algorithm for solving the linear system of equations az works by successively solving for element zj in the jth equation fixing all other zj at their current guesses
this process is repeated for until convergence golub and van loan consider an additive model with observations and terms with the jth term to be fit by linear smoother sj
consider the following system of equations eb eb eb ec ec ec ec ec ec ec
ec
ec

ed
ed
ed
sp sp sp fp sp here each fj is an vector of evaluations of the jth function at the data points and is an vector of the response values
show that backfitting is blockwise gauss seidel algorithm for solving this system of equations let and be symmetric smoothing operators matrices with eigenvalues in
consider backfitting algorithm with response vector and smoothers
show that with any starting values the algorithm converges and give formula for the final iterates
backfitting equations
consider backfitting procedure with orthogonal projections and let be the overall regression matrix whose columns span lcol lcol lcol sp where lcol denotes the column space of matrix
show that the estimating equations eb eb eb ec ec ec ec ec ec ec
ec
ec
ed
ed
ed
sp sp sp fp sp are equivalent to the least squares normal equations dt dt where is the vector of coefficients
additive models trees and related methods ex
suppose the same smoother is used to estimate both terms in two term additive model both variables are identical
assume that is symmetric with eigenvalues in
show that the backfitting residual converges to and that the residual sum of squares converges upward
can the residual sum of squares converge upward in less structured situations
how does this fit compare to the fit with single term fit by
hint use the eigen decomposition of to help with this comparison
degrees of freedom of tree
given data yi with mean xi and variance and ap fitting operation let's define the degrees of freedom of fit by cov yi
consider fit estimated by regression tree fit to set of predictors xp in terms of the number of terminal nodes give rough formula for the degrees of freedom of the fit generate observations with predictors as independent standard gaussian variates and fix these values generate response values also as standard gaussian independent of the predictors
fit regression trees to the data of fixed size and terminal nodes and hence estimate the degrees of freedom of each fit
do ten simulations of the response and average the results to get good estimate of degrees of freedom compare your estimates of degrees of freedom in and and discuss if the regression tree fit were linear operation we could write sy for some matrix
then the degrees of freedom would be tr
suggest way to compute an approximate matrix for regression tree compute it and compare the resulting degrees of freedom to those in and
consider the ozone data of figure fit an additive model to the cube root of ozone concentration as function of temperature wind speed and radiation
compare your results to those obtained via the trellis display in figure fit trees mars and prim to the same data and compare the results to those found in and in figure
this is page printer opaque this boosting and additive trees boosting methods boosting is one of the most powerful learning ideas introduced in the last twenty years
it was originally designed for classification problems but as will be seen in this chapter it can profitably be extended to regression as well
the motivation for boosting was procedure that combines the outputs of many weak classifiers to produce powerful committee
from this perspective boosting bears resemblance to bagging and other committee based approaches section
however we shall see that the connection is at best superficial and that boosting is fundamentally different
we begin by describing the most popular boosting algorithm due to freund and schapire called adaboost
consider two class problem with the output variable coded as
given vector of predictor variables classifier produces prediction taking one of the two values
the error rate on the training sample is err yi xi and the expected error rate on future predictions is exy
weak classifier is one whose error rate is only slightly better than random guessing
the purpose of boosting is to sequentially apply the weak classification algorithm to repeatedly modified versions of the data thereby producing sequence of weak classifiers gm
schematic of adaboost
classifiers are trained on weighted versions of the dataset and then combined to produce final prediction
the predictions from all of them are then combined through weighted majority vote to produce the final prediction
sign gm
here are computed by the boosting algorithm and weight the contribution of each respective gm
their effect is to give higher influence to the more accurate classifiers in the sequence
figure shows schematic of the adaboost procedure
the data modifications at each boosting step consist of applying weights wn to each of the training observations xi yi
initially all of the weights are set to wi so that the first step simply trains the classifier on the data in the usual manner
for each successive iteration the observation weights are individually modified and the classification algorithm is reapplied to the weighted observations
at step those observations that were misclassified by the classifier gm induced at the previous step have their weights increased whereas the weights are decreased for those that were classified correctly
thus as iterations proceed observations that are difficult to classify correctly receive ever increasing influence
each successive classifier is thereby forced
boosting methods algorithm adaboost
initialize the observation weights wi
for to fit classifier gm to the training data using weights wi compute pn wi yi gm xi errm pn wi compute log errm errm set wi wi exp yi gm xi hp
output sign to concentrate on those training observations that are missed by previous ones in the sequence
algorithm shows the details of the adaboost algorithm
the current classifier gm is induced on the weighted observations at line
the resulting weighted error rate is computed at line
line calculates the weight given to gm in producing the final classifier line
the individual weights of each of the observations are updated for the next iteration at line
observations misclassified by gm have their weights scaled by factor exp increasing their relative influence for inducing the next classifier gm in the sequence
the adaboost algorithm is known as discrete adaboost in friedman et al because the base classifier gm returns discrete class label
if the base classifier instead returns real valued prediction probability mapped to the interval adaboost can be modified appropriately see real adaboost in friedman et al
the power of adaboost to dramatically increase the performance of even very weak classifier is illustrated in figure
the features are standard independent gaussian and the deterministic target is defined by bd if xj otherwise
here is the median of chi squared random variable with degrees of freedom sum of squares of standard gaussians
there are training cases with approximately cases in each class and test observations
here the weak classifier is just stump two terminalnode classification tree
applying this classifier alone to the training data set yields very poor test set error rate of compared to for
simulated data test error rate for boosting with stumps as function of the number of iterations
also shown are the test error rate for single stump and node classification tree random guessing
however as boosting iterations proceed the error rate steadily decreases reaching after iterations
thus boosting this simple very weak classifier reduces its prediction error rate by almost factor of four
it also outperforms single large classification tree error rate
since its introduction much has been written to explain the success of adaboost in producing accurate classifiers
most of this work has centered on using classification trees as the base learner where improvements are often most dramatic
in fact breiman nips workshop referred to adaboost with trees as the best off the shelf classifier in the world see also breiman
this is especially the case for datamining applications as discussed more fully in section later in this chapter
this loss function is
boosting fits an additive model very similar to the negative binomial log likelihood sections
boosting fits an additive model the success of boosting is really not very mysterious
the key lies in expression
boosting is way of fitting an additive expansion in set of elementary basis functions
here the basis functions are the individual classifiers gm
more generally basis function expansions take the form where are the expansion coefficients and ir are usually simple functions of the multivariate argument characterized by set of parameters
we discuss basis expansions in some detail in chapter
boosting and additive trees algorithm forward stagewise additive modeling
initialize
for to compute arg min yi fm xi xi
set fm fm
typically these models are fit by minimizing loss function averaged over the training data such as the squared error or likelihood based loss function
min yi xi
for many loss functions and or basis functions this requires computationally intensive numerical optimization techniques
however simple alternative often can be found when it is feasible to rapidly solve the subproblem of fitting just single basis function min yi xi
forward stagewise additive modeling forward stagewise modeling approximates the solution to by sequentially adding new basis functions to the expansion without adjusting the parameters and coefficients of those that have already been added
this is outlined in algorithm
at each iteration one solves for the optimal basis function and corresponding coefficient to add to the current expansion fm
this produces fm and the process is repeated
previously added terms are not modified
for squared error loss
exponential loss and adaboost one has yi fm xi xi yi fm xi xi rim xi where rim yi fm xi is simply the residual of the current model on the ith observation
thus for squared error loss the term that best fits the current residuals is added to the expansion at each step
this idea is the basis for least squares regression boosting discussed in section
however as we show near the end of the next section squared error loss is generally not good choice for classification hence the need to consider other loss criteria
exponential loss and adaboost we now show that adaboost algorithm is equivalent to forward stagewise additive modeling algorithm using the loss function exp
the appropriateness of this criterion is addressed in the next section
for adaboost the basis functions are the individual classifiers gm
using the exponential loss function one must solve gm arg min exp yi fm xi xi for the classifier gm and corresponding coefficient to be added at each step
this can be expressed as gm arg min wi exp yi xi with wi exp yi fm xi
since each wi depends neither on nor it can be regarded as weight that is applied to each observation
this weight depends on fm xi and so the individual weight values change with each iteration
the solution to can be obtained in two steps
first for any value of the solution to for gm is gm arg min wi yi xi
boosting and additive trees which is the classifier that minimizes the weighted error rate in predicting
this can be easily seen by expressing the criterion in as wi wi yi xi yi xi which in turn can be written as wi yi xi wi
plugging this gm into and solving for one obtains errm log errm where errm is the minimized weighted error rate pn yi gm xi errm ipn
wi the approximation is then updated fm fm gm which causes the weights for the next iteration to be wi wi yi gm xi
using the fact that yi gm xi yi gm xi becomes wi wi yi gm xi where is the quantity defined at line of adaboost algorithm
the factor in multiplies all weights by the same value so it has no effect
thus is equivalent to line of algorithm
one can view line of the adaboost algorithm as method for approximately solving the minimization in and hence
hence we conclude that adaboost minimizes the exponential loss criterion via forward stagewise additive modeling approach
figure shows the training set misclassification error rate and average exponential loss for the simulated data problem of figure
the training set misclassification error decreases to zero at around iterations and remains there but the exponential loss keeps decreasing
notice also in figure that the test set misclassification error continues to improve after iteration
clearly adaboost is not optimizing trainingset misclassification error the exponential loss is more sensitive to changes in the estimated class probabilities
why exponential loss
simulated data boosting with stumps misclassification error rate on the training set and average exponential loss exp
after about iterations the misclassification error is zero while the exponential loss continues to decrease
why exponential loss
the adaboost algorithm was originally motivated from very different perspective than presented in the previous section
its equivalence to forward stagewise additive modeling based on exponential loss was only discovered five years after its inception
by studying the properties of the exponential loss criterion one can gain insight into the procedure and discover ways it might be improved
the principal attraction of exponential loss in the context of additive modeling is computational it leads to the simple modular reweighting adaboost algorithm
however it is of interest to inquire about its statistical properties
what does it estimate and how well is it being estimated
the first question is answered by seeking its population minimizer
it is easy to show friedman et al that pr arg min ey log pr
boosting and additive trees or equivalently pr thus the additive expansion produced by adaboost is estimating onehalf the log odds of
this justifies using its sign as the classification rule in
another loss criterion with the same population minimizer is the binomial negative log likelihood or deviance also known as cross entropy interpreting as the logit transform
let ef pr ef and define
then the binomial log likelihood loss function is log log or equivalently the deviance is log
since the population maximizer of log likelihood is at the true probabilities pr we see from that the population minimizers of the deviance ey and ey are the same
thus using either criterion leads to the same solution at the population level
note that itself is not proper log likelihood since it is not the logarithm of any probability mass function for binary random variable
loss functions and robustness in this section we examine the different loss functions for classification and regression more closely and characterize them in terms of their robustness to extreme data
robust loss functions for classification although both the exponential and binomial deviance yield the same solution when applied to the population joint distribution the same is not true for finite data sets
both criteria are monotone decreasing functions of the margin yf
in classification with response the margin plays role analogous to the residuals in regression
the classification rule sign implies that observations with positive margin yi xi are classified correctly whereas those with negative margin yi xi are misclassified
the decision boundary is defined by
loss functions for two class classification
the response is the prediction is with class prediction sign
the losses are misclassification sign exponential exp yf binomial deviance log exp yf squared error and support vector yf see section
each function has been scaled so that it passes through the point
the goal of the classification algorithm is to produce positive margins as frequently as possible
any loss criterion used for classification should penalize negative margins more heavily than positive ones since positive margin observations are already correctly classified
figure shows both the exponential and binomial deviance criteria as function of the margin
also shown is misclassification loss which gives unit penalty for negative margin values and no penalty at all for positive ones
both the exponential and deviance loss can be viewed as monotone continuous approximations to misclassification loss
they continuously penalize increasingly negative margin values more heavily than they reward increasingly positive ones
the difference between them is in degree
the penalty associated with binomial deviance increases linearly for large increasingly negative margin whereas the exponential criterion increases the influence of such observations exponentially
at any point in the training process the exponential criterion concentrates much more influence on observations with large negative margins
binomial deviance concentrates relatively less influence on such observa
boosting and additive trees tions more evenly spreading the influence among all of the data
it is therefore far more robust in noisy settings where the bayes error rate is not close to zero and especially in situations where there is misspecification of the class labels in the training data
the performance of adaboost has been empirically observed to dramatically degrade in such situations
also shown in the figure is squared error loss
the minimizer of the corresponding risk on the population is arg min ey pr
as before the classification rule is sign
squared error loss is not good surrogate for misclassification error
as seen in figure it is not monotone decreasing function of increasing margin yf
for margin values yi xi it increases quadratically thereby placing increasing influence error on observations that are correctly classified with increasing certainty thereby reducing the relative influence of those incorrectly classified yi xi
thus if class assignment is the goal monotone decreasing criterion serves as better surrogate loss function
figure on page in chapter includes modification of quadratic loss the huberized square hinge loss rosset et al which enjoys the favorable properties of the binomial deviance quadratic loss and the svm hinge loss
it has the same population minimizer as the quadratic is zero for and becomes linear for
since quadratic functions are easier to compute with than exponentials our experience suggests this to be useful alternative to the binomial deviance
with class classification the response takes values in the unordered set gk see sections and
we now seek classifier taking values in
it is sufficient to know the class conditional probabilities pk pr gk for then the bayes classifier is gk where arg max
in principal though we need not learn the pk but simply which one is largest
however in data mining applications the interest is often more in the class probabilities themselves rather than in performing class assignment
as in section the logistic model generalizes naturally to classes efk pk pk fl which ensures that pk and that they sum to one
note that here we have different functions one per class
there is redundancy in the functions fk since adding an arbitrary to each leaves the model unchanged
traditionally one of them is set to zero for example
loss functions and robustness fk as in
here we prefer to retain the symmetry and impose pk the constraint fk
the binomial deviance extends naturally to the class multinomial deviance loss function gk log pk ak
gk fk log
as in the two class case the criterion penalizes incorrect predictions only linearly in their degree of incorrectness
zhu et al generalize the exponential loss for class problems
see exercise for details
robust loss functions for regression in the regression setting analogous to the relationship between exponential loss and binomial log likelihood is the relationship between squared error loss and absolute loss
the population solutions are for squared error loss and median for absolute loss for symmetric error distributions these are the same
however on finite samples squared error loss places much more emphasis on observations with large absolute residuals yi xi during the fitting process
it is thus far less robust and its performance severely degrades for long tailed error distributions and especially for grossly mismeasured values outliers
other more robust criteria such as absolute loss perform much better in these situations
in the statistical robustness literature variety of regression loss criteria have been proposed that provide strong resistance if not absolute immunity to gross outliers while being nearly as efficient as least squares for gaussian errors
they are often better than either for error distributions with moderately heavy tails
one such criterion is the huber loss criterion used for regression huber bd for otherwise
figure compares these three loss functions
these considerations suggest than when robustness is concern as is especially the case in data mining applications see section squarederror loss for regression and exponential loss for classification are not the best criteria from statistical perspective
however they both lead to the elegant modular boosting algorithms in the context of forward stagewise additive modeling
for squared error loss one simply fits the base learner to the residuals from the current model yi fm xi at each step
comparison of three loss functions for regression plotted as function of the margin
the huber loss function combines the good properties of squared error loss near zero and absolute error loss when is large exponential loss one performs weighted fit of the base learner to the output values yi with weights wi exp yi fm xi
using other more robust criteria directly in their place does not give rise to such simple feasible boosting algorithms
however in section we show how one can derive simple elegant boosting algorithms based on any differentiable loss criterion thereby producing highly robust boosting procedures for data mining
off the shelf procedures for data mining predictive learning is an important aspect of data mining
as can be seen from this book wide variety of methods have been developed for predictive learning from data
for each particular method there are situations for which it is particularly well suited and others where it performs badly compared to the best that can be done with that data
we have attempted to characterize appropriate situations in our discussions of each of the respective methods
however it is seldom known in advance which procedure will perform best or even well for any given problem
table summarizes some of the characteristics of number of learning methods
industrial and commercial data mining applications tend to be especially challenging in terms of the requirements placed on learning procedures
data sets are often very large in terms of number of observations and number of variables measured on each of them
thus computational con
off the shelf procedures for data mining table
some characteristics of different learning methods
key good fair and poor
characteristic neural svm trees mars nn nets kernels natural handling of data of mixed type handling of missing values robustness to outliers in input space insensitive to monotone transformations of inputs computational scalability large ability to deal with irrel evant inputs ability to extract linear combinations of features interpretability predictive power siderations play an important role
also the data are usually messy the inputs tend to be mixtures of quantitative binary and categorical variables the latter often with many levels
there are generally many missing values complete observations being rare
distributions of numeric predictor and response variables are often long tailed and highly skewed
this is the case for the spam data section when fitting generalized additive model we first log transformed each of the predictors in order to get reasonable fit
in addition they usually contain substantial fraction of gross mis measurements outliers
the predictor variables are generally measured on very different scales
in data mining applications usually only small fraction of the large number of predictor variables that have been included in the analysis are actually relevant to prediction
also unlike many applications such as pattern recognition there is seldom reliable domain knowledge to help create especially relevant features and or filter out the irrelevant ones the inclusion of which dramatically degrades the performance of many methods
in addition data mining applications generally require interpretable models
it is not enough to simply produce predictions
it is also desirable to have information providing qualitative understanding of the relationship
boosting and additive trees between joint values of the input variables and the resulting predicted response value
thus black box methods such as neural networks which can be quite useful in purely predictive settings such as pattern recognition are far less useful for data mining
these requirements of speed interpretability and the messy nature of the data sharply limit the usefulness of most learning procedures as offthe shelf methods for data mining
an off the shelf method is one that can be directly applied to the data without requiring great deal of timeconsuming data preprocessing or careful tuning of the learning procedure
of all the well known learning methods decision trees come closest to meeting the requirements for serving as an off the shelf procedure for data mining
they are relatively fast to construct and they produce interpretable models if the trees are small
as discussed in section they naturally incorporate mixtures of numeric and categorical predictor variables and missing values
they are invariant under strictly monotone transformations of the individual predictors
as result scaling and or more general transformations are not an issue and they are immune to the effects of predictor outliers
they perform internal feature selection as an integral part of the procedure
they are thereby resistant if not completely immune to the inclusion of many irrelevant predictor variables
these properties of decision trees are largely the reason that they have emerged as the most popular learning method for data mining
trees have one aspect that prevents them from being the ideal tool for predictive learning namely inaccuracy
they seldom provide predictive accuracy comparable to the best that can be achieved with the data at hand
as seen in section boosting decision trees improves their accuracy often dramatically
at the same time it maintains most of their desirable properties for data mining
some advantages of trees that are sacrificed by boosting are speed interpretability and for adaboost robustness against overlapping class distributions and especially mislabeling of the training data
gradient boosted model gbm is generalization of tree boosting that attempts to mitigate these problems so as to produce an accurate and effective off the shelf procedure for data mining
example spam data before we go into the details of gradient boosting we demonstrate its abilities on two class classification problem
the spam data are introduced in chapter and used as an example for many of the procedures in chapter sections and
applying gradient boosting to these data resulted in test error rate of using the same test set as was used in section
by comparison an additive logistic regression achieved cart tree fully grown and
boosting trees pruned by cross validation and mars
the standard error of these estimates is around although gradient boosting is significantly better than all of them using the mcnemar test exercise
in section below we develop relative importance measure for each predictor as well as partial dependence plot describing predictor's contribution to the fitted model
we now illustrate these for the spam data
figure displays the relative importance spectrum for all predictor variables
clearly some predictors are more important than others in separating spam from email
the frequencies of the character strings hp and remove are estimated to be the four most relevant predictor variables
at the other end of the spectrum the character strings table and have virtually no relevance
the quantity being modeled here is the log odds of spam versus email pr spam log pr email see section below
figure shows the partial dependence of the log odds on selected important predictors two positively associated with spam
and remove and two negatively associated edu and hp
these particular dependencies are seen to be essentially monotonic
there is general agreement with the corresponding functions found by the additive logistic regression model see figure on page
running gradient boosted model on these data with terminalnode trees produces purely additive main effects model for the logodds with corresponding error rate of as compared to for the full gradient boosted model with terminal node trees
although not significant this slightly higher error rate suggests that there may be interactions among some of the important predictor variables
this can be diagnosed through two variable partial dependence plots
figure shows one of the several such plots displaying strong interaction effects
one sees that for very low frequencies of hp the log odds of spam are greatly increased
for high frequencies of hp the log odds of spam tend to be much lower and roughly constant as function of
as the frequency of hp decreases the functional relationship with
boosting trees regression and classification trees are discussed in detail in section
they partition the space of all joint predictor variable values into disjoint regions rj as represented by the terminal nodes of the tree
constant is assigned to each such region and the predictive rule is rj
boosting and additive trees addresses labs telnet direct cs table parts credit lab conference report original data project font make address order all hpl technology people pm mail over meeting email internet receive re business will money our you edu captot george capmax your capave free remove hp
predictor variable importance spectrum for the spam data
the variable names are written on the vertical axis
boosting trees partial dependence partial dependence
partial dependence of log odds of spam on four important predictors
the red ticks at the base of the plots are deciles of the input variable
partial dependence of the log odds of spam vs email as function of joint frequencies of hp and the character
boosting and additive trees thus tree can be formally expressed as rj with parameters rj
is usually treated as meta parameter
the parameters are found by minimizing the empirical risk arg min yi
xi rj this is formidable combinatorial optimization problem and we usually settle for approximate suboptimal solutions
it is useful to divide the optimization problem into two parts finding given rj given the rj estimating the is typically trivial and often the mean of the yi falling in region rj
for misclassification loss is the modal class of the observations falling in region rj
finding rj this is the difficult part for which approximate solutions are found
note also that finding the rj entails estimating the as well
typical strategy is to use greedy top down recursive partitioning algorithm to find the rj
in addition it is sometimes necessary to approximate by smoother and more convenient criterion for optimizing the rj arg min yi xi
then given the the can be estimated more precisely using the original criterion
in section we described such strategy for classification trees
the gini index replaced misclassification loss in the growing of the tree identifying the rj
the boosted tree model is sum of such trees fm induced in forward stagewise manner algorithm
at each step in the forward stagewise procedure one must solve arg min yi fm xi xi
boosting trees for the region set and constants rjm jm of the next tree given the current model fm
given the regions rjm finding the optimal constants jm in each region is typically straightforward jm arg min yi fm xi jm
jm xi rjm finding the regions is difficult and even more difficult than for single tree
for few special cases the problem simplifies
for squared error loss the solution to is no harder than for single tree
it is simply the regression tree that best predicts the current residuals yi fm xi and jm is the mean of these residuals in each corresponding region
for two class classification and exponential loss this stagewise approach gives rise to the adaboost method for boosting classification trees algorithm
in particular if the trees are restricted to be scaled classification trees then we showed in section that the solution to pn is the tree that minimizes the weighted error rate wi yi xi with weights wi yi fm xi
by scaled classification tree we mean with the restriction that jm
without this restriction still simplifies for exponential loss to weighted exponential criterion for the new tree arg min wi exp yi xi
it is straightforward to implement greedy recursive partitioning algorithm using this weighted exponential loss as splitting criterion
given the rjm one can show exercise that the solution to is the weighted log odds in each corresponding region xi rjm wi yi jm log
xi rjm wi yi this requires specialized tree growing algorithm in practice we prefer the approximation presented below that uses weighted least squares regression tree
using loss criteria such as the absolute error or the huber loss in place of squared error loss for regression and the deviance in place of exponential loss for classification will serve to robustify boosting trees
unfortunately unlike their nonrobust counterparts these robust criteria do not give rise to simple fast boosting algorithms
for more general loss criteria the solution to given the rjm is typically straightforward since it is simple location estimate
boosting and additive trees absolute loss it is just the median of the residuals in each respective region
for the other criteria fast iterative algorithms exist for solving and usually their faster single step approximations are adequate
the problem is tree induction
simple fast algorithms do not exist for solving for these more general loss criteria and approximations like become essential
numerical optimization via gradient boosting fast approximate algorithms for solving with any differentiable loss criterion can be derived by analogy to numerical optimization
the loss in using to predict on the training data is yi xi
the goal is to minimize with respect to where here is constrained to be sum of trees
ignoring this constraint minimizing can be viewed as numerical optimization arg min where the parameters irn are the values of the approximating function xi at each of the data points xi xn
numerical optimization procedures solve as sum of component vectors fm hm hm irn where is an initial guess and each successive fm is induced based on the current parameter vector fm which is the sum of the previously induced updates
numerical optimization methods differ in their prescriptions for computing each increment vector hm step
steepest descent steepest descent chooses hm gm where is scalar and gm irn is the gradient of evaluated at fm
the components of the gradient gm are yi xi gim xi xi fm xi
numerical optimization via gradient boosting the step length is the solution to arg min fm gm
the current solution is then updated fm fm gm and the process repeated at the next iteration
steepest descent can be viewed as very greedy strategy since gm is the local direction in irn for which is most rapidly decreasing at fm
gradient boosting forward stagewise boosting algorithm is also very greedy strategy
at each step the solution tree is the one that maximally reduces given the current model fm and its fits fm xi
thus the tree predictions xi are analogous to the components of the negative gradient
the principal difference between them is that the tree components tm xn are not independent
they are constrained to be the predictions of jm terminal node decision tree whereas the negative gradient is the unconstrained maximal descent direction
the solution to in the stagewise approach is analogous to the line search in steepest descent
the difference is that performs separate line search for those components of tm that correspond to each separate terminal region xi xi rjm
if minimizing loss on the training data were the only goal steepest descent would be the preferred strategy
the gradient is trivial to calculate for any differentiable loss function whereas solving is difficult for the robust criteria discussed in section
unfortunately the gradient is defined only at the training data points xi whereas the ultimate goal is to generalize fm to new data not represented in the training set
possible resolution to this dilemma is to induce tree at the mth iteration whose predictions tm are as close as possible to the negative gradient
using squared error to measure closeness this leads us to arg min gim xi
that is one fits the tree to the negative gradient values by least squares
as noted in section fast algorithms exist for least squares decision tree induction
although the solution regions jm to will not be identical to the regions rjm that solve it is generally similar enough to serve the same purpose
in any case the forward stagewise
boosting and additive trees table
gradients for commonly used loss functions
setting loss function yi xi xi regression yi xi yi xi regression yi xi sign yi xi regression huber yi xi for yi xi sign yi xi for yi xi where th quantile yi xi classification deviance kth component yi gk pk xi boosting procedure and top down decision tree induction are themselves approximation procedures
after constructing the tree the corresponding constants in each region are given by
table summarizes the gradients for commonly used loss functions
for squared error loss the negative gradient is just the ordinary residual gim yi fm xi so that on its own is equivalent standard least squares boosting
with absolute error loss the negative gradient is the sign of the residual so at each iteration fits the tree to the sign of the current residuals by least squares
for huber regression the negative gradient is compromise between these two see the table
for classification the loss function is the multinomial deviance and least squares trees are constructed at each iteration
each tree tkm is fit to its respective negative gradient vector gkm yi xi xi gikm fkm xi yi gk pk xi with pk given by
although separate trees are built at each iteration they are related through
for binary classification only one tree is needed exercise
implementations of gradient boosting algorithm presents the generic gradient tree boosting algorithm for regression
specific algorithms are obtained by inserting different loss criteria
the first line of the algorithm initializes to the optimal constant model which is just single terminal node tree
the components of the negative gradient computed at line are referred to as generalized or pseudo residuals
gradients for commonly used loss functions are summarized in table
right sized trees for boosting algorithm gradient tree boosting algorithm
initialize arg min yi
for to for compute yi xi rim
xi fm fit regression tree to the targets rim giving terminal regions rjm jm for jm compute jm arg min yi fm xi
xi rjm pjm update fm fm jm rjm
output fm
the algorithm for classification is similar
lines are repeated times at each iteration once for each class using
the result at line is different coupled tree expansions fkm
these produce probabilities via or do classification as in
details are given in exercise
two basic tuning parameters are the number of iterations and the sizes of each of the constituent trees jm
the original implementation of this algorithm was called mart for multiple additive regression trees and was referred to in the first edition of this book
many of the figures in this chapter were produced by mart
gradient boosting as described here is implemented in the gbm package ridgeway gradient boosted models and is freely available
the gbm package is used in section and extensively in chapters and
another implementation of boosting is mboost hothorn and bu hlmann
commercial implementation of gradient boosting mart called treenet is available from salford systems inc
right sized trees for boosting historically boosting was considered to be technique for combining models here trees
as such the tree building algorithm was regarded as
boosting and additive trees primitive that produced models to be combined by the boosting procedure
in this scenario the optimal size of each tree is estimated separately in the usual manner when it is built section
very large oversized tree is first induced and then bottom up procedure is employed to prune it to the estimated optimal number of terminal nodes
this approach assumes implicitly that each tree is the last one in the expansion
except perhaps for the very last tree this is clearly very poor assumption
the result is that trees tend to be much too large especially during the early iterations
this substantially degrades performance and increases computation
the simplest strategy for avoiding this problem is to restrict all trees to be the same size jm
at each iteration terminal node regression tree is induced
thus becomes meta parameter of the entire boosting procedure to be adjusted to maximize estimated performance for the data at hand
one can get an idea of useful values for by considering the properties of the target function arg min exy
here the expected value is over the population joint distribution of
the target function is the one with minimum prediction risk on future data
this is the function we are trying to approximate
one relevant property of is the degree to which the coordinate variables xp interact with one another
this is captured by its anova analysis of variance expansion xj jk xj xk jkl xj xk xl
jk jkl the first sum in is over functions of only single predictor variable xj
the particular functions xj are those that jointly best approximate under the loss criterion being used
each such xj is called the main effect of xj
the second sum is over those two variable functions that when added to the main effects best fit
these are called the second order interactions of each respective variable pair xj xk
the third sum represents third order interactions and so on
for many problems encountered in practice low order interaction effects tend to dominate
when this is the case models that produce strong higher order interaction effects such as large decision trees suffer in accuracy
the interaction level of tree based approximations is limited by the tree size
namely no interaction effects of level greater that are possible
since boosted models are additive in the trees this limit extends to them as well
setting single split decision stump produces boosted models with only main effects no interactions are permitted
with two variable interaction effects are also allowed and
boosting with different sized trees applied to the example used in figure
since the generative model is additive stumps perform the best
the boosting algorithm used the binomial deviance loss in algorithm shown for comparison is the adaboost algorithm so on
this suggests that the value chosen for should reflect the level of dominant interactions of
this is of course generally unknown but in most situations it will tend to be low
figure illustrates the effect of interaction order choice of on the simulation example
the generative function is additive sum of quadratic monomials so boosting models with incurs unnecessary variance and hence the higher test error
figure compares the coordinate functions found by boosted stumps with the true functions
although in many applications will be insufficient it is unlikely that will be required
experience so far indicates that works well in the context of boosting with results being fairly insensitive to particular choices in this range
one can fine tune the value for by trying several different values and choosing the one that produces the lowest risk on validation sample
however this seldom provides significant improvement over using
coordinate functions estimated by boosting stumps for the simulated example used in figure
the true quadratic functions are shown for comparison
regularization besides the size of the constituent trees the other meta parameter of gradient boosting is the number of boosting iterations
each iteration usually reduces the training risk fm so that for large enough this risk can be made arbitrarily small
however fitting the training data too well can lead to overfitting which degrades the risk on future predictions
thus there is an optimal number minimizing future risk that is application dependent
convenient way to estimate is to monitor prediction risk as function of on validation sample
the value of that minimizes this risk is taken to be an estimate of
this is analogous to the early stopping strategy often used with neural networks section
shrinkage controlling the value of is not the only possible regularization strategy
as with ridge regression and neural networks shrinkage techniques can be employed as well see sections and
the simplest implementation of shrinkage in the context of boosting is to scale the contribution of each tree by factor bd when it is added to the current approximation
that is line of algorithm is replaced by fm fm bd jm rjm
the parameter bd can be regarded as controlling the learning rate of the boosting procedure
smaller values of bd more shrinkage result in larger training risk for the same number of iterations
thus both bd and control prediction risk on the training data
however these parameters do
regularization not operate independently
smaller values of bd lead to larger values of for the same training risk so that there is tradeoff between them
empirically it has been found friedman that smaller values of bd favor better test error and require correspondingly larger values of
in fact the best strategy appears to be to set bd to be very small bd and then choose by early stopping
this yields dramatic improvements over no shrinkage bd for regression and for probability estimation
the corresponding improvements in misclassification risk via are less but still substantial
the price paid for these improvements is computational smaller values of bd give rise to larger values of and computation is proportional to the latter
however as seen below many iterations are generally computationally feasible even on very large data sets
this is partly due to the fact that small trees are induced at each step with no pruning
figure shows test error curves for the simulated example of figure
gradient boosted model mart was trained using binomial deviance using either stumps or six terminal node trees and with or without shrinkage
the benefits of shrinkage are evident especially when the binomial deviance is tracked
with shrinkage each test error curve reaches lower value and stays there for many iterations
section draws connection between forward stagewise shrinkage in boosting and the use of an penalty for regularizing model parameters the lasso
we argue that penalties may be superior to the penalties used by methods such as the support vector machine
subsampling we saw in section that bootstrap averaging bagging improves the performance of noisy classifier through averaging
chapter discusses in some detail the variance reduction mechanism of this sampling followed by averaging
we can exploit the same device in gradient boosting both to improve performance and computational efficiency
with stochastic gradient boosting friedman at each iteration we sample fraction of the training observations without replacement and grow the next tree using that subsample
the rest of the algorithm is identical
typical value for can be although for large can be substantially smaller than
not only does the sampling reduce the computing time by the same fraction but in many cases it actually produces more accurate model
figure illustrates the effect of subsampling using the simulated example both as classification and as regression example
we see in both cases that sampling along with shrinkage slightly outperformed the rest
it appears here that subsampling without shrinkage does poorly
test error curves for simulated example of figure using gradient boosting mart
the models were trained using binomial deviance either stumps or six terminal node trees and with or without shrinkage
the left panels report test deviance while the right panels show misclassification error
the beneficial effect of shrinkage can be seen in all cases especially for deviance in the left panels
test error curves for the simulated example showing the effect of stochasticity
for the curves labeled sample different subsample of the training data was used each time tree was grown
in the left panel the models were fit by gbm using binomial deviance loss function in the right hand panel using square error loss
the downside is that we now have four parameters to set bd and
typically some early explorations determine suitable values for bd and leaving as the primary parameter
interpretation single decision trees are highly interpretable
the entire model can be completely represented by simple two dimensional graphic binary tree that is easily visualized
linear combinations of trees lose this important feature and must therefore be interpreted in different way
relative importance of predictor variables in data mining applications the input predictor variables are seldom equally relevant
often only few of them have substantial influence on the response the vast majority are irrelevant and could just as well have not been included
it is often useful to learn the relative importance or contribution of each input variable in predicting the response
boosting and additive trees for single decision tree breiman et al proposed as measure of relevance for each predictor variable
the sum is over the internal nodes of the tree
at each such node one of the input variables xv is used to partition the region associated with that node into two subregions within each separate constant is fit to the response values
the particular variable chosen is the one that gives maximal estimated improvement in squared error risk over that for constant fit over the entire region
the squared relative importance of variable is the sum of such squared improvements over all internal nodes for which it was chosen as the splitting variable
this importance measure is easily generalized to additive tree expansions it is simply averaged over the trees tm
due to the stabilizing effect of averaging this measure turns out to be more reliable than is its counterpart for single tree
also because of shrinkage section the masking of important variables by others with which they are highly correlated is much less of problem
note that and refer to squared relevance the actual relevances are their respective square roots
since these measures are relative it is customary to assign the largest value of and then scale the others accordingly
figure shows the relevant importance of the inputs in predicting spam versus email
for class classification separate models fk are induced each consisting of sum of trees fk tkm
in this case generalizes to tkm
here is the relevance of in separating the class observations from the other classes
the overall relevance of is obtained by averaging over all of the classes

interpretation figures and illustrate the use of these averaged and separate relative importances
partial dependence plots after the most relevant variables have been identified the next step is to attempt to understand the nature of the dependence of the approximation on their joint values
graphical renderings of the as function of its arguments provides comprehensive summary of its dependence on the joint values of the input variables
unfortunately such visualization is limited to low dimensional views
we can easily display functions of one or two arguments either continuous or discrete or mixed in variety of different ways this book is filled with such displays
functions of slightly higher dimensions can be plotted by conditioning on particular sets of values of all but one or two of the arguments producing trellis of plots becker et al for more than two or three variables viewing functions of the corresponding higher dimensional arguments is more difficult
useful alternative can sometimes be to view collection of plots each one of which shows the partial dependence of the approximation on selected small subset of the input variables
although such collection can seldom provide comprehensive depiction of the approximation it can often produce helpful clues especially when is dominated by low order interactions
consider the subvector xs of of the input predictor variables xp indexed by
let be the complement set with
general function will in principle depend on all of the input variables xs xc
one way to define the average or partial dependence of on xs is fs xs exc xs xc
this is marginal average of and can serve as useful description of the effect of the chosen subset on when for example the variables in xs do not have strong interactions with those in xc
partial dependence functions can be used to interpret the results of any black box learning method
they can be estimated by af xs xs xic where xn are the values of xc occurring in the training data
this requires pass over the data for each set of joint values of xs for which af xs is to be evaluated
this can be computationally intensive lattice in
boosting and additive trees even for moderately sized data sets
fortunately with decision trees af xs can be rapidly computed from the tree itself without reference to the data exercise
it is important to note that partial dependence functions defined in represent the effect of xs on after accounting for the average effects of the other variables xc on
they are not the effect of xs on ignoring the effects of xc
the latter is given by the conditional expectation dc xs xs xc xs and is the best least squares approximation to by function of xs alone
the quantities dc xs and af xs will be the same only in the unlikely event that xs and xc are independent
for example if the effect of the chosen variable subset happens to be purely additive xs xc
then produces the xs up to an additive constant
if the effect is purely multiplicative xs xc then produces xs up to multiplicative constant factor
on the other hand will not produce xs in either case
in fact can produce strong effects on variable subsets for which has no dependence at all
viewing plots of the partial dependence of the boosted tree approximation on selected variables subsets can help to provide qualitative description of its properties
illustrations are shown in sections and
owing to the limitations of computer graphics and human perception the size of the subsets xs must be small
there are of course large number of such subsets but only those chosen from among the usually much smaller set of highly relevant predictors are likely to be informative
also those subsets whose effect on is approximately additive or multiplicative will be most revealing
for class classification there are separate models one for each class
each one is related to the respective probabilities through fk log pk log pl
thus each fk is monotone increasing function of its respective probability on logarithmic scale
partial dependence plots of each respective fk on its most relevant predictors can help reveal how the log odds of realizing that class depend on the respective input variables
illustrations illustrations in this section we illustrate gradient boosting on number of larger datasets using different loss functions as appropriate
california housing this data set pace and barry is available from the carnegie mellon statlib repository
it consists of aggregated data from each of neighborhoods census block groups in california
the response variable is the median house value in each neighborhood measured in units of
the predictor variables are demographics such as median income medinc housing density as reflected by the number of houses house and the average occupancy in each house aveoccup
also included as predictors are the location of each neighborhood longitude and latitude and several quantities reflecting the properties of the houses in the neighborhood average number of rooms averooms and bedrooms avebedrms
there are thus total of eight predictors all numeric
we fit gradient boosting model using the mart procedure with terminal nodes learning rate of bd and the huber loss criterion for predicting the numeric response
we randomly divided the dataset into training set and test set
figure shows the average absolute error aae as function for number of iterations on both the training data and test data
the test error is seen to decrease monotonically with increasing more rapidly during the early stages and then leveling off to being nearly constant as iterations increase
thus the choice of particular value of is not critical as long as it is not too small
this tends to be the case in many applications
the shrinkage strategy tends to eliminate the problem of overfitting especially for larger data sets
the value of aae after iterations is
this can be compared to that of the optimal constant predictor median yi which is
in terms of more familiar quantities the squared multiple correlation coefficient of this model is
pace and barry use sophisticated spatial autoregression procedure where prediction for each neighborhood is based on median house values in nearby neighborhoods using the other predictors as covariates
experimenting with transformations they achieved predicting log
using log as the response the corresponding value for gradient boosting was http lib stat cmu edu
average absolute error as function of number of iterations for the california housing data
figure displays the relative variable importances for each of the eight predictor variables
not surprisingly median income in the neighborhood is the most relevant predictor
longitude latitude and average occupancy all have roughly half the relevance of income whereas the others are somewhat less influential
figure shows single variable partial dependence plots on the most relevant nonlocation predictors
note that the plots are not strictly smooth
this is consequence of using tree based models
decision trees produce discontinuous piecewise constant models
this carries over to sums of trees with of course many more pieces
unlike most of the methods discussed in this book there is no smoothness constraint imposed on the result
arbitrarily sharp discontinuities can be modeled
the fact that these curves generally exhibit smooth trend is because that is what is estimated to best predict the response for this problem
this is often the case
the hash marks at the base of each plot delineate the deciles of the data distribution of the corresponding variables
note that here the data density is lower near the edges especially for larger values
this causes the curves to be somewhat less well determined in those regions
the vertical scales of the plots are the same and give visual comparison of the relative importance of the different variables
the partial dependence of median house value on median income is monotonic increasing being nearly linear over the main body of data
house value is generally monotonic decreasing with increasing average occupancy except perhaps for average occupancy rates less than one
median house
relative importance of the predictors for the california housing data value has nonmonotonic partial dependence on average number of rooms
it has minimum at approximately three rooms and is increasing both for smaller and larger values
median house value is seen to have very weak partial dependence on house age that is inconsistent with its importance ranking figure
this suggests that this weak main effect may be masking stronger interaction effects with other variables
figure shows the two variable partial dependence of housing value on joint values of median age and average occupancy
an interaction between these two variables is apparent
for values of average occupancy greater than two house value is nearly independent of median age whereas for values less than two there is strong dependence on age
figure shows the two variable partial dependence of the fitted model on joint values of longitude and latitude displayed as shaded contour plot
there is clearly very strong dependence of median house value on the neighborhood location in california
note that figure is not plot of house value versus location ignoring the effects of the other predictors
like all partial dependence plots it represents the effect of location after accounting for the effects of the other neighborhood and house attributes
it can be viewed as representing an extra premium one pays for location
this premium is seen to be relatively large near the pacific coast especially in the bay area and los angeles san diego re
partial dependence of housing value on the nonlocation variables for the california housing data
the red ticks at the base of the plot are deciles of the input variables
partial dependence of house value on median age and average occupancy
there appears to be strong interaction effect between these two variables
partial dependence of median house value on location in california
one unit is at prices and the values plotted are relative to the overall median of gions
in the northern central valley and southeastern desert regions of california location costs considerably less
new zealand fish plant and animal ecologists use regression models to predict species presence abundance and richness as function of environmental variables
although for many years simple linear and parametric models were popular recent literature shows increasing interest in more sophisticated models such as generalized additive models section gam multivariate adaptive regression splines section mars and boosted regression trees leathwick et al leathwick et al
here we model the
boosting and additive trees presence and abundance of the black oreo dory marine fish found in the oceanic waters around new zealand figure shows the locations of trawls deep water net fishing with maximum depth of km and the red points indicate those trawls for which the black oreo was present one of over hundred species regularly recorded
the catch size in kg for each species was recorded for each trawl
along with the species catch number of environmental measurements are available for each trawl
these include the average depth of the trawl avgdepth and the temperature and salinity of the water
since the latter two are strongly correlated with depth leathwick et al derived instead tempresid and salresid the residuals obtained when these two measures are adjusted for depth via separate non parametric regressions
sstgrad is measure of the gradient of the sea surface temperature and chla is broad indicator of ecosytem productivity via satellite image measurements
suspartmatter provides measure of suspended particulate matter particularly in coastal waters and is also satellite derived
the goal of this analysis is to estimate the probability of finding black oreo in trawl as well as the expected catch size standardized to take into account the effects of variation in trawl speed and distance as well as the mesh size of the trawl net
the authors used logistic regression for estimating the probability
for the catch size it might seem natural to assume poisson distribution and model the log of the mean count but this is often not appropriate because of the excessive number of zeros
although specialized approaches have been developed such as the zeroinflated poisson lambert they chose simpler approach
if is the non negative catch size pr
the second term is estimated by the logistic regression and the first term can be estimated using only the trawls with positive catch
for the logistic regression the authors used gradient boosted model gbm with binomial deviance loss function depth trees and shrinkage factor bd
for the positive catch regression they modeled log using gbm with squared error loss also depth trees but bd and un logged the predictions
in both cases they used fold cross validation for selecting the number of terms as well as the shrinkage factor
the models data and maps shown here were kindly provided by dr john leathwick of the national institute of water and atmospheric research in new zealand and dr jane elith school of botany university of melbourne
the collection of the research trawl data took place from and was funded by the new zealand ministry of fisheries
version of package gbm in ver
map of new zealand and its surrounding exclusive economic zone showing the locations of trawls small blue dots taken between and
the red points indicate trawls for which the species black oreo dory were present
the left panel shows the mean deviance as function of the number of trees for the gbm logistic regression model fit to the presence absence data
shown are fold cross validation on the training data and bars and test deviance on the test data
also shown for comparison is the test deviance using gam model with df for each term
the right panel shows roc curves on the test data for the chosen gbm model vertical line in left plot and the gam model
figure left panel shows the mean binomial deviance for the sequence of gbm models both for fold cv and test data
there is modest improvement over the performance of gam model fit using smoothing splines with degrees of freedom df per term
the right panel shows the roc curves see section for both models which measures predictive performance
from this point of view the performance looks very similar with gbm perhaps having slight edge as summarized by the auc area under the curve
at the point of equal sensitivity specificity gbm achieves and gam
figure summarizes the contributions of the variables in the logistic gbm fit
we see that there is well defined depth range over which black oreo are caught with much more frequent capture in colder waters
we do not give details of the quantitative catch model the important variables were much the same
all the predictors used in these models are available on fine geographical grid in fact they were derived from environmental atlases satellite images and the like see leathwick et al for details
this also means that predictions can be made on this grid and imported into gis mapping systems
figure shows prediction maps for both presence and catch size with both standardized to common set of trawl conditions since the predictors vary in continuous fashion with geographical location so do the predictions
the top left panel shows the relative influence computed from the gbm logistic regression model
the remaining panels show the partial dependence plots for the leading five variables all plotted on the same scale for comparison
because of their ability to model interactions and automatically select variables as well as robustness to outliers and missing data gbm models are rapidly gaining popularity in this data rich and enthusiastic community
demographics data in this section we illustrate gradient boosting on multiclass classification problem using mart
the data come from questionnaires filled out by shopping mall customers in the san francisco bay area impact resources inc columbus oh
among the questions are concerning demographics
for this illustration the goal is to predict occupation using the other variables as predictors and hence identify demographic variables that discriminate between different occupational categories
we randomly divided the data into training set and test set and used node trees with learning rate bd
figure shows the occupation class values along with their corresponding error rates
the overall error rate is which can be compared to the null rate of obtained by predicting the most numerous
geological prediction maps of the presence probability left map and catch size right map obtained from the gradient boosted models class prof man professional managerial
the four best predicted classes are seen to be retired student prof man and homemaker
figure shows the relative predictor variable importances as averaged over all classes
figure displays the individual relative importance distributions for each of the four best predicted classes
one sees that the most relevant predictors are generally different for each respective class
an exception is age which is among the three most relevant for predicting retired student and prof man
figure shows the partial dependence of the log odds on age for these three classes
the abscissa values are ordered codes for respective equally spaced age intervals
one sees that after accounting for the contributions of the other variables the odds of being retired are higher for older people whereas the opposite is the case for being student
the odds of being professional managerial are highest for middle aged people
these results are of course not surprising
they illustrate that inspecting partial dependences separately for each class can lead to sensible results
bibliographic notes schapire developed the first simple boosting procedure in the pac learning framework valiant kearns and vazirani
relative importance of the predictors as averaged over all classes for the demographics data
predictor variable importances separately for each of the four classes with lowest error rate for the demographics data
partial dependence of the odds of three different occupations on age for the demographics data showed that weak learner could always improve its performance by training two additional classifiers on filtered versions of the input data stream
weak learner is an algorithm for producing two class classifier with performance guaranteed with high probability to be significantly better than coin flip
schapire's strength of weak learnability theorem proves that gb has improved performance over
freund proposed boost by majority variation which combined many weak learners simultaneously and improved the performance of the simple boosting algorithm of schapire
the theory supporting both of these
boosting and additive trees algorithms requires the weak learner to produce classifier with fixed error rate
this led to the more adaptive and realistic adaboost freund and schapire and its offspring where this assumption was dropped
freund and schapire and schapire and singer provide some theory to support their algorithms in the form of upper bounds on generalization error
this theory has evolved in the computational learning community initially based on the concepts of pac learning
other theories attempting to explain boosting come from game theory freund and schapire breiman breiman and vc theory schapire et al
the bounds and the theory associated with the adaboost algorithms are interesting but tend to be too loose to be of practical importance
in practice boosting achieves results far more impressive than the bounds would imply
schapire and meir and ra tsch give useful overviews more recent than the first edition of this book
friedman et al and friedman form the basis for our exposition in this chapter
friedman et al analyze adaboost statistically derive the exponential criterion and show that it estimates the log odds of the class probability
they propose additive tree models the right sized trees and anova representation of section and the multiclass logit formulation
friedman developed gradient boosting and shrinkage for classification and regression while friedman explored stochastic variants of boosting
mason et al also embraced gradient approach to boosting
as the published discussions of friedman et al shows there is some controversy about how and why boosting works
since the publication of the first edition of this book these debates have continued and spread into the statistical community with series of papers on consistency of boosting jiang lugosi and vayatis zhang and yu bartlett and traskin
mease and wyner through series of simulation examples challenge some of our interpretations of boosting our response friedman et al puts most of these objections to rest
recent survey by bu hlmann and hothorn supports our approach to boosting
exercises ex
derive expression for the update parameter in adaboost
prove result that is the minimizer of the population version of the adaboost criterion is one half of the log odds
show that the marginal average recovers additive and multiplicative functions and while the conditional expectation does not
exercises ex
write program implementing adaboost with trees redo the computations for the example of figure
plot the training error as well as test error and discuss its behavior investigate the number of iterations needed to make the test error finally start to rise change the setup of this example as follows define two classes with the features in class being standard independent gaussian variates
in class the features are also standard independent gaussian but conditioned on the event
now the classes have significant overlap in feature space
repeat the adaboost experiments as in figure and discuss the results
multiclass exponential loss zhu et al
for class classification problem consider the coding yk with bd if gk yk otherwise
pk let fk with fk and define exp
using lagrange multipliers derive the population minimizer of subject to the zero sum constraint and relate these to the class probabilities show that multiclass boosting using this loss function leads to reweighting algorithm similar to adaboost as in section
mcnemar test agresti
we report the test error rates on the spam data to be for generalized additive model gam and for gradient boosting gbm with test sample of size show that the standard error of these estimates is about
since the same test data are used for both methods the error rates are correlated and we cannot perform two sample test
we can compare the methods directly on each test observation leading to the summary gbm gam correct error correct error
boosting and additive trees the mcnemar test focuses on the discordant errors vs
conduct test to show that gam makes significantly more errors than gradient boosting with two sided value of
derive expression
consider class problem where the targets yik are coded as if observation is in class and zero otherwise
suppose we have current model fk with fk see in section
we wish to update the model for observations in region in predictor space by adding constants fk with write down the multinomial log likelihood for this problem and its first and second derivatives using only the diagonal of the hessian matrix in and starting from show that one step approximate newton update for is yik pik xi xi pik pik pk where pik exp fk xi xi we prefer our update to sum to zero as the current model does
using symmetry arguments show that is an appropriate update where is defined as in for all
consider class problem where the targets yik are coded as if observation is in class and zero otherwise
using the multinomial deviance loss function and the symmetric logistic transform use the arguments leading to the gradient boosting algorithm to derive algorithm
hint see exercise for step iii
show that for class classification only one tree needs to be grown at each gradient boosting iteration
show how to compute the partial dependence function fs xs in efficiently
referring to let and with
assume and are bivariate gaussian each with mean zero variance one and
show that even though is not function of
exercises algorithm gradient boosting for class classification
initialize fk
for to set efk pk pk for to
compute rikm yik pk xi ii
fit regression tree to the targets rikm giving terminal regions rjkm jm iii
compute xi rjkm rikm jkm jm
xi rjkm rikm rikm pj iv
update fkm fk jkm rjkm
output fkm
boosting and additive trees
this is page printer opaque this neural networks introduction in this chapter we describe class of learning methods that was developed separately in different fields statistics and artificial intelligence based on essentially identical models
the central idea is to extract linear combinations of the inputs as derived features and then model the target as nonlinear function of these features
the result is powerful learning method with widespread applications in many fields
we first discuss the projection pursuit model which evolved in the domain of semiparametric statistics and smoothing
the rest of the chapter is devoted to neural network models
projection pursuit regression as in our generic supervised learning problem assume we have an input vector with components and target
let be unit vectors of unknown parameters
the projection pursuit regression ppr model has the form gm
this is an additive model but in the derived features vm rather than the inputs themselves
the functions gm are unspecified and are esti
perspective plots of two ridge functions left exp where
right sin where mated along with the directions using some flexible smoothing method see below
the function gm is called ridge function in irp
it varies only in the direction defined by the vector
the scalar variable vm is the projection of onto the unit vector and we seek so that the model fits well hence the name projection pursuit
figure shows some examples of ridge functions
in the example on the left so that the function only varies in the direction
in the example on the right
the ppr model is very general since the operation of forming nonlinear functions of linear combinations generates surprisingly large class of models
for example the product can be written as and higher order products can be represented similarly
in fact if is taken arbitrarily large for appropriate choice of gm the ppr model can approximate any continuous function in irp arbitrarily well
such class of models is called universal approximator
however this generality comes at price
interpretation of the fitted model is usually difficult because each input enters into the model in complex and multifaceted way
as result the ppr model is most useful for prediction and not very useful for producing an understandable model for the data
the model known as the single index model in econometrics is an exception
it is slightly more general than the linear regression model and offers similar interpretation
how do we fit ppr model given training data xi yi
we seek the approximate minimizers of the error function yi gm xi
projection pursuit regression over functions gm and direction vectors
as in other smoothing problems we need either explicitly or implicitly to impose complexity constraints on the gm to avoid overfit solutions
consider just one term and drop the subscript
given the direction vector we form the derived variables vi xi
then we have one dimensional smoothing problem and we can apply any scatterplot smoother such as smoothing spline to obtain an estimate of
on the other hand given we want to minimize over
gauss newton search is convenient for this task
this is quasi newton method in which the part of the hessian involving the second derivative of is discarded
it can be simply derived as follows
let old be the current estimate for
we write xi old xi old xi old xi to give xn yi oldt xi yi xi old xi old xi xi old to minimize the right hand side we carry out least squares regression with target old xi yi oldt xi old xi on the input xi with weights old xi and no intercept bias term
this produces the updated coefficient vector new
these two steps estimation of and are iterated until convergence
with more than one term in the ppr model the model is built in forward stage wise manner adding pair gm at each stage
there are number of implementation details
local regression and smoothing splines are convenient
while this may lead ultimately to fewer terms it is not clear whether it improves prediction performance
the model building stops when the next term does not appreciably improve the fit of the model
cross validation can also be used to determine
neural networks there are many other applications such as density estimation friedman et al friedman where the projection pursuit idea can be used
in particular see the discussion of ica in section and its relationship with exploratory projection pursuit
however the projection pursuit regression model has not been widely used in the field of statistics perhaps because at the time of its introduction its computational demands exceeded the capabilities of most readily available computers
but it does represent an important intellectual advance one that has blossomed in its reincarnation in the field of neural networks the topic of the rest of this chapter
neural networks the term neural network has evolved to encompass large class of models and learning methods
here we describe the most widely used vanilla neural net sometimes called the single hidden layer back propagation network or single layer perceptron
there has been great deal of hype surrounding neural networks making them seem magical and mysterious
as we make clear in this section they are just nonlinear statistical models much like the projection pursuit regression model discussed above
neural network is two stage regression or classification model typically represented by network diagram as in figure
this network applies both to regression or classification
for regression typically and there is only one output unit at the top
however these networks can handle multiple quantitative responses in seamless fashion so we will deal with the general case
for class classification there are units at the top with the kth unit modeling the probability of class
there are target measurements yk each being coded as variable for the kth class
derived features zm are created from linear combinations of the inputs and then the target yk is modeled as function of linear combinations of the zm zm tk kt fk gk where zm and tk
the activation function is usually chosen to be the sigmoid see figure for plot of
sometimes gaussian radial basis functions chapter are used for the producing what is known as radial basis function network
neural network diagrams like figure are sometimes drawn with an additional bias unit feeding into every unit in the hidden and output layers
schematic of single hidden layer feed forward neural network
thinking of the constant as an additional input feature this bias unit captures the intercepts and in model
the output function gk allows final transformation of the vector of outputs
for regression we typically choose the identity function gk tk
early work in class classification also used the identity function but this was later abandoned in favor of the softmax function etk gk pk
this is of course exactly the transformation used in the multilogit model section and produces positive estimates that sum to one
in section we discuss other problems with linear activation functions in particular potentially severe masking effects
the units in the middle of the network computing the derived features zm are called hidden units because the values zm are not directly observed
in general there can be more than one hidden layer as illustrated in the example at the end of this chapter
we can think of the zm as basis expansion of the original inputs the neural network is then standard linear model or linear multilogit model using these transformations as inputs
there is however an important enhancement over the basisexpansion techniques discussed in chapter here the parameters of the basis functions are learned from the data
plot of the sigmoid function exp red curve commonly used in the hidden layer of neural network
included are sv for blue curve and purple curve
the scale parameter controls the activation rate and we can see that large amounts to hard activation at
note that shifts the activation threshold from to
notice that if is the identity function then the entire model collapses to linear model in the inputs
hence neural network can be thought of as nonlinear generalization of the linear model both for regression and classification
by introducing the nonlinear transformation it greatly enlarges the class of linear models
in figure we see that the rate of activation of the sigmoid depends on the norm of and if is very small the unit will indeed be operating in the linear part of its activation function
notice also that the neural network model with one hidden layer has exactly the same form as the projection pursuit model described above
the difference is that the ppr model uses nonparametric functions gm while the neural network uses far simpler function based on with three free parameters in its argument
in detail viewing the neural network model as ppr model we identify gm where is the mth unit vector
since sv has lower complexity than more general nonparametric it is not surprising that neural network might use or such functions while the ppr model typically uses fewer terms or for example
finally we note that the name neural networks derives from the fact that they were first developed as models for the human brain
each unit represents neuron and the connections links in figure represent synapses
in early models the neurons fired when the total signal passed to that unit exceeded certain threshold
in the model above this corresponds
fitting neural networks to use of step function for and gm
later the neural network was recognized as useful tool for nonlinear statistical modeling and for this purpose the step function is not smooth enough for optimization
hence the step function was replaced by smoother threshold function the sigmoid in figure
fitting neural networks the neural network model has unknown parameters often called weights and we seek values for them that make the model fit the training data well
we denote the complete set of weights by which consists of weights weights
for regression we use sum of squared errors as our measure of fit error function yik fk xi
for classification we use either squared error or cross entropy deviance yik log fk xi and the corresponding classifier is argmaxk fk
with the softmax activation function and the cross entropy error function the neural network model is exactly linear logistic regression model in the hidden units and all the parameters are estimated by maximum likelihood
typically we don't want the global minimizer of as this is likely to be an overfit solution
instead some regularization is needed this is achieved directly through penalty term or indirectly by early stopping
details are given in the next section
the generic approach to minimizing is by gradient descent called back propagation in this setting
because of the compositional form of the model the gradient can be easily derived using the chain rule for differentiation
this can be computed by forward and backward sweep over the network keeping track only of quantities local to each unit
neural networks here is back propagation in detail for squared error loss
let zmi xi from and let zi zm
then we have ri yik fk xi with derivatives ri yik fk xi gk kt zi zmi km xk ri yik fk xi gk kt zi km xi xi
given these derivatives gradient descent update at the st iteration has the form ri km km km ri where is the learning rate discussed below
now write as ri ki zmi km ri smi xi
the quantities ki and smi are errors from the current model at the output and hidden layer units respectively
from their definitions these errors satisfy smi xi km ki known as the back propagation equations
using this the updates in can be implemented with two pass algorithm
in the forward pass the current weights are fixed and the predicted values xi are computed from formula
in the backward pass the errors ki are computed and then back propagated via to give the errors smi
both sets of errors are then used to compute the gradients for the updates in via
some issues in training neural networks this two pass procedure is what is known as back propagation
it has also been called the delta rule widrow and hoff
the computational components for cross entropy have the same form as those for the sum of squares error function and are derived in exercise
the advantages of back propagation are its simple local nature
in the back propagation algorithm each hidden unit passes and receives information only to and from units that share connection
hence it can be implemented efficiently on parallel architecture computer
the updates in are kind of batch learning with the parameter updates being sum over all of the training cases
learning can also be carried out online processing each observation one at time updating the gradient after each training case and cycling through the training cases many times
in this case the sums in equations are replaced by single summand
training epoch refers to one sweep through the entire training set
online training allows the network to handle very large training sets and also to update the weights as new observations come in
the learning rate for batch learning is usually taken to be constant and can also be optimized by line search that minimizes the error function at each update
with online learning should decrease to zero as the iteration
this learning is form of stochastic approximation robbins and munro results in this field ensure convergence if and satisfied for example by
back propagation can be very slow and for that reason is usually not the method of choice
second order techniques such as newton's method are not attractive here because the second derivative matrix of the hessian can be very large
better approaches to fitting include conjugate gradients and variable metric methods
these avoid explicit computation of the second derivative matrix while still providing faster convergence
some issues in training neural networks there is quite an art in training neural networks
the model is generally overparametrized and the optimization problem is nonconvex and unstable unless certain guidelines are followed
in this section we summarize some of the important issues
starting values note that if the weights are near zero then the operative part of the sigmoid figure is roughly linear and hence the neural network collapses into an approximately linear model exercise
usually starting values for weights are chosen to be random values near zero
hence the model starts out nearly linear and becomes nonlinear as the weights increase
neural networks units localize to directions and introduce nonlinearities where needed
use of exact zero weights leads to zero derivatives and perfect symmetry and the algorithm never moves
starting instead with large weights often leads to poor solutions
overfitting often neural networks have too many weights and will overfit the data at the global minimum of
in early developments of neural networks either by design or by accident an early stopping rule was used to avoid overfitting
here we train the model only for while and stop well before we approach the global minimum
since the weights start at highly regularized linear solution this has the effect of shrinking the final model toward linear model
validation dataset is useful for determining when to stop since we expect the validation error to start increasing
more explicit method for regularization is weight decay which is analogous to ridge regression used for linear models section
we add penalty to the error function bb where km km and bb is tuning parameter
larger values of bb will tend to shrink the weights toward zero typically cross validation is used to estimate bb
the effect of the penalty is to simply add terms km and to the respective gradient expressions
other forms for the penalty have been proposed for example km km km known as the weight elimination penalty
this has the effect of shrinking smaller weights more than does
figure shows the result of training neural network with ten hidden units without weight decay upper panel and with weight decay lower panel to the mixture example of chapter
weight decay has clearly improved the prediction
figure shows heat maps of the estimated weights from the training grayscale versions of these are called hinton diagrams
we see that weight decay has dampened the weights in both layers the resulting weights are spread fairly evenly over the ten hidden units
scaling of the inputs since the scaling of the inputs determines the effective scaling of the weights in the bottom layer it can have large effect on the quality of the final
error
error
neural network on the mixture example of chapter
the upper panel uses no weight decay and overfits the training data
the lower panel uses weight decay and achieves close to the bayes error rate broken purple boundary
both use the softmax activation function and cross entropy error
heat maps of the estimated weights from the training of neural networks from figure
the display ranges from bright green negative to bright red positive solution
at the outset it is best to standardize all inputs to have mean zero and standard deviation one
this ensures all inputs are treated equally in the regularization process and allows one to choose meaningful range for the random starting weights
with standardized inputs it is typical to take random uniform weights over the range
number of hidden units and layers generally speaking it is better to have too many hidden units than too few
with too few hidden units the model might not have enough flexibility to capture the nonlinearities in the data with too many hidden units the extra weights can be shrunk toward zero if appropriate regularization is used
typically the number of hidden units is somewhere in the range of to with the number increasing with the number of inputs and number of training cases
it is most common to put down reasonably large number of units and train them with regularization
some researchers use cross validation to estimate the optimal number but this seems unnecessary if cross validation is used to estimate the regularization parameter
choice of the number of hidden layers is guided by background knowledge and experimentation
each layer extracts features of the input for regression or classification
use of multiple hidden layers allows construction of hierarchical features at different levels of resolution
an example of the effective use of multiple layers is given in section
multiple minima the error function is nonconvex possessing many local minima
as result the final solution obtained is quite dependent on the choice of start
example simulated data ing weights
one must at least try number of random starting configurations and choose the solution giving lowest penalized error
probably better approach is to use the average predictions over the collection of networks as the final prediction ripley
this is preferable to averaging the weights since the nonlinearity of the model implies that this averaged solution could be quite poor
another approach is via bagging which averages the predictions of networks training from randomly perturbed versions of the training data
this is described in section
example simulated data we generated data from two additive error models sum of sigmoids at at radial xm here xp each xj being standard gaussian variate with in the first model and in the second
for the sigmoid model for the radial model exp
both and are gaussian errors with variance chosen so that the signal to noise ratio var var var var is in both models
we took training sample of size and test sample of size
we fit neural networks with weight decay and various numbers of hidden units and recorded the average test error etest for each of random starting weights
only one training set was generated but the results are typical for an average training set
the test errors are shown in figure
note that the zero hidden unit model refers to linear least squares regression
the neural network is perfectly suited to the sum of sigmoids model and the two unit model does perform the best achieving an error close to the bayes rate
recall that the bayes rate for regression with squared error is the error variance in the figures we report test error relative to the bayes error
notice however that with more hidden units overfitting quickly creeps in and with some starting weights the model does worse than the linear model zero hidden unit model
even with two hidden units two of the ten starting weight configurations produced results no better than the linear model confirming the importance of multiple starting values
radial function is in sense the most difficult for the neural net as it is spherically symmetric and with no preferred directions
we see in the right
boxplots of test error for simulated data example relative to the bayes error broken horizontal line
true function is sum of two sigmoids on the left and radial function is on the right
the test error is displayed for different starting weights for single hidden layer neural network with the number of units as indicated panel of figure that it does poorly in this case with the test error staying well above the bayes error note the different vertical scale from the left panel
in fact since constant fit such as the sample average achieves relative error of when the snr is we see that the neural networks perform increasingly worse than the mean
in this example we used fixed weight decay parameter of representing mild amount of regularization
the results in the left panel of figure suggest that more regularization is needed with greater numbers of hidden units
in figure we repeated the experiment for the sum of sigmoids model with no weight decay in the left panel and stronger weight decay bb in the right panel
with no weight decay overfitting becomes even more severe for larger numbers of hidden units
the weight decay value bb produces good results for all numbers of hidden units and there does not appear to be overfitting as the number of units increase
finally figure shows the test error for ten hidden unit network varying the weight decay parameter over wide range
the value is approximately optimal
in summary there are two free parameters to select the weight decay bb and number of hidden units
as learning strategy one could fix either parameter at the value corresponding to the least constrained model to ensure that the model is rich enough and use cross validation to choose the other parameter
here the least constrained values are zero weight decay and ten hidden units
comparing the left panel of figure to figure we see that the test error is less sensitive to the value of the weight
boxplots of test error for simulated data example relative to the bayes error
true function is sum of two sigmoids
the test error is displayed for ten different starting weights for single hidden layer neural network with the number units as indicated
the two panels represent no weight decay left and strong weight decay bb right
boxplots of test error for simulated data example
true function is sum of two sigmoids
the test error is displayed for ten different starting weights for single hidden layer neural network with ten hidden units and weight decay parameter value as indicated
examples of training cases from zip code data
each image is bit grayscale representation of handwritten digit decay parameter and hence cross validation of this parameter would be preferred
example zip code data this example is character recognition task classification of handwritten numerals
this problem captured the attention of the machine learning and neural network community for many years and has remained benchmark problem in the field
figure shows some examples of normalized handwritten digits automatically scanned from envelopes by the
postal service
the original scanned digits are binary and of different sizes and orientations the images shown here have been deslanted and size normalized resulting in grayscale images le cun et al
these pixel values are used as inputs to the neural network classifier
black box neural network is not ideally suited to this pattern recognition task partly because the pixel representation of the images lack certain invariances such as small rotations of the image
consequently early attempts with neural networks yielded misclassification rates around on various examples of the problem
in this section we show some of the pioneering efforts to handcraft the neural network to overcome some these deficiencies le cun which ultimately led to the state of the art in neural network performance le cun et al
although current digit datasets have tens of thousands of training and test examples the sample size here is deliberately modest in order to em the figures and tables in this example were recreated from le cun
architecture of the five networks used in the zip code example phasize the effects
the examples were obtained by scanning some actual hand drawn digits and then generating additional images by random horizontal shifts
details may be found in le cun
there are digits in the training set and in the test set
five different networks were fit to the data net no hidden layer equivalent to multinomial logistic regression
net one hidden layer hidden units fully connected
net two hidden layers locally connected
net two hidden layers locally connected with weight sharing
net two hidden layers locally connected two levels of weight sharing
these are depicted in figure
net for example has inputs one each for the input pixels and ten output units for each of the digits
the predicted value represents the estimated probability that an image has digit class for
test performance curves as function of the number of training epochs for the five networks of table applied to the zip code data
le cun the networks all have sigmoidal output units and were all fit with the sum of squares error function
the first network has no hidden layer and hence is nearly equivalent to linear multinomial regression model exercise
net is single hidden layer network with hidden units of the kind described above
the training set error for all of the networks was since in all cases there are more parameters than training observations
the evolution of the test error during the training epochs is shown in figure
the linear network net starts to overfit fairly quickly while test performance of the others level off at successively superior values
the other three networks have additional features which demonstrate the power and flexibility of the neural network paradigm
they introduce constraints on the network natural for the problem at hand which allow for more complex connectivity but fewer parameters
net uses local connectivity this means that each hidden unit is connected to only small patch of units in the layer below
in the first hidden layer an array each unit takes inputs from patch of the input layer for units in the first hidden layer that are one unit apart their receptive fields overlap by one row or column and hence are two pixels apart
in the second hidden layer inputs are from patch and again units that are one unit apart have receptive fields that are two units apart
the weights for all other connections are set to zero
local connectivity makes each unit responsible for extracting local features from the layer below and
example zip code data table
test set performance of five different neural networks on handwritten digit classification example le cun
network architecture links weights correct net single layer network net two layer network net locally connected net constrained network net constrained network reduces considerably the total number of weights
with many more hidden units than net net has fewer links and hence weights vs and achieves similar performance
net and net have local connectivity with shared weights
all units in local feature map perform the same operation on different parts of the image achieved by sharing the same weights
the first hidden layer of net has two arrays and each unit takes input from patch just like in net
however each of the units in single feature map share the same set of nine weights but have their own bias parameter
this forces the extracted features in different parts of the image to be computed by the same linear functional and consequently these networks are sometimes known as convolutional networks
the second hidden layer of net has no weight sharing and is the same as in net
the gradient of the error function with respect to shared weight is the sum of the gradients of with respect to each connection controlled by the weights in question
table gives the number of links the number of weights and the optimal test performance for each of the networks
we see that net has more links but fewer weights than net and superior test performance
net has four feature maps in the second hidden layer each unit connected to local patch in the layer below
weights are shared in each of these feature maps
we see that net does the best having errors of only compared to for the vanilla network net
the clever design of network net motivated by the fact that features of handwriting style should appear in more than one part of digit was the result of many person years of experimentation
this and similar networks gave better performance on zip code problems than any other learning method at that time early
this example also shows that neural networks are not fully automatic tool as they are sometimes advertised
as with all statistical models subject matter knowledge can and should be used to improve their performance
this network was later outperformed by the tangent distance approach simard et al described in section which explicitly incorporates natural affine invariances
at this point the digit recognition datasets become test beds for every new learning procedure and researchers worked
neural networks hard to drive down the error rates
boosting is described in chapter
lenet is predecessor of lenet
le cun et al report much larger table of performance results and it is evident that many groups have been working very hard to bring these test error rates down
they report standard error of on the error estimates which is based on binomial average with and
this implies that error rates within of one another are statistically equivalent
realistically the standard error is even higher since the test data has been implicitly used in the tuning of the various procedures
discussion both projection pursuit regression and neural networks take nonlinear functions of linear combinations derived features of the inputs
this is powerful and very general approach for regression and classification and has been shown to compete well with the best learning methods on many problems
these tools are especially effective in problems with high signal to noise ratio and settings where prediction without interpretation is the goal
they are less effective for problems where the goal is to describe the physical process that generated the data and the roles of individual inputs
each input enters into the model in many places in nonlinear fashion
some authors hinton plot diagram of the estimated weights into each hidden unit to try to understand the feature that each unit is extracting
this is limited however by the lack of identifiability of the parameter vectors
often there are solutions with spanning the same linear space as the ones found during training giving predicted values that the national institute of standards and technology maintain large databases in cluding handwritten character databases http www nist gov srd
bayesian neural nets and the nips challenge are roughly the same
some authors suggest carrying out principal component analysis of these weights to try to find an interpretable solution
in general the difficulty of interpreting these models has limited their use in fields like medicine where interpretation of the model is very important
there has been great deal of research on the training of neural networks
unlike methods like cart and mars neural networks are smooth functions of real valued parameters
this facilitates the development of bayesian inference for these models
the next sections discusses successful bayesian implementation of neural networks
bayesian neural nets and the nips challenge classification competition was held in in which five labeled training datasets were provided to participants
it was organized for neural information processing systems nips workshop
each of the data sets constituted two class classification problems with different sizes and from variety of domains see table
feature measurements for validation dataset were also available
participants developed and applied statistical learning procedures to make predictions on the datasets and could submit predictions to website on the validation set for period of weeks
with this feedback participants were then asked to submit predictions for separate test set and they received their results
finally the class labels for the validation set were released and participants had one week to train their algorithms on the combined training and validation sets and submit their final predictions to the competition website
total of groups participated with and eventually making submissions on the validation and test sets respectively
there was an emphasis on feature extraction in the competition
artificial probes were added to the data these are noise features with distributions resembling the real features but independent of the class labels
the percentage of probes that were added to each dataset relative to the total set of features is shown on table
thus each learning algorithm had to figure out way of identifying the probes and downweighting or eliminating them
number of metrics were used to evaluate the entries including the percentage correct on the test set the area under the roc curve and combined score that compared each pair of classifiers head to head
the results of the competition are very interesting and are detailed in guyon et al
the most notable result the entries of neal and zhang were the clear overall winners
in the final competition they finished first
neural networks table
nips challenge data sets
the column labeled is the number of features
for the dorothea dataset the features are binary
ntr nval and nte are the number of training validation and test cases respectively dataset domain feature percent ntr nval nte type probes arcene mass spectrometry dense dexter text classification sparse dorothea drug discovery sparse gisette digit recognition dense madelon artificial dense in three of the five datasets and were th and th on the remaining two datasets
in their winning entries neal and zhang used series of preprocessing feature selection steps followed by bayesian neural networks dirichlet diffusion trees and combinations of these methods
here we focus only on the bayesian neural network approach and try to discern which aspects of their approach were important for its success
we rerun their programs and compare the results to boosted neural networks and boosted trees and other related methods
bayes boosting and bagging let us first review briefly the bayesian approach to inference and its application to neural networks
given training data xtr ytr we assume sampling model with parameters neal and zhang use two hiddenlayer neural network with output nodes the class probabilities pr for the binary outcomes
given prior distribution pr the posterior distribution for the parameters is pr pr ytr xtr pr xtr ytr pr pr ytr xtr for test case with features xnew the predictive distribution for the label ynew is pr ynew xnew xtr ytr pr ynew xnew pr xtr ytr equation
since the integral in is intractable sophisticated markov chain monte carlo mcmc methods are used to sample from the posterior distribution pr ynew xnew xtr ytr
few hundred values are generated and then simple average of these values estimates the integral
neal and zhang use diffuse gaussian priors for all of the parameters
the particular mcmc approach that was used is called hybrid monte carlo and may be important for the success of the method
it includes an auxiliary momentum vector and implements hamiltonian dynamics in which the potential function is the target density
this is done to avoid
bayesian neural nets and the nips challenge random walk behavior the successive candidates move across the sample space in larger steps
they tend to be less correlated and hence converge to the target distribution more rapidly
neal and zhang also tried different forms of pre processing of the features univariate screening using tests and automatic relevance determination
in the latter method ard the weights coefficients for the jth feature to each of the first hidden layer units all share common prior variance and prior mean zero
the posterior distributions for each variance are computed and the features whose posterior variance concentrates on small values are discarded
there are thus three main features of this approach that could be important for its success the feature selection and pre processing the neural network model and the bayesian inference for the model using mcmc
according to neal and zhang feature screening in is carried out purely for computational efficiency the mcmc procedure is slow with large number of features
there is no need to use feature selection to avoid overfitting
the posterior average takes care of this automatically
we would like to understand the reasons for the success of the bayesian method
in our view power of modern bayesian methods does not lie in their use as formal inference procedure most people would not believe that the priors in high dimensional complex neural network model are actually correct
rather the bayesian mcmc approach gives an efficient way of sampling the relevant parts of model space and then averaging the predictions for the high probability models
bagging and boosting are non bayesian procedures that have some similarity to mcmc in bayesian model
the bayesian approach fixes the data and perturbs the parameters according to current estimate of the posterior distribution
bagging perturbs the data in an fashion and then re estimates the model to give new set of model parameters
at the end simple average of the model predictions from different bagged samples is computed
boosting is similar to bagging but fits model that is additive in the models of each individual base learner which are learned using non samples
we can write all of these models in the form xnew ynew xnew
neural networks in all cases the are large collection of model parameters
for the bayesian model the and the average estimates the posterior mean by sampling from the posterior distribution
for bagging as well and the are the parameters refit to bootstrap resamples of the training data
for boosting the weights are all equal to but the are typically chosen in nonrandom sequential fashion to constantly improve the fit
performance comparisons based on the similarities above we decided to compare bayesian neural networks to boosted trees boosted neural networks random forests and bagged neural networks on the five datasets in table
bagging and boosting of neural networks are not methods that we have previously used in our work
we decided to try them here because of the success of bayesian neural networks in this competition and the good performance of bagging and boosting with trees
we also felt that by bagging and boosting neural nets we could assess both the choice of model as well as the model search strategy
here are the details of the learning methods that were compared bayesian neural nets
the results here are taken from neal and zhang using their bayesian approach to fitting neural networks
the models had two hidden layers of and units
we re ran some networks for timing purposes only
boosted trees
we used the gbm package version in the language
tree depth and shrinkage factors varied from dataset to dataset
we consistently bagged of the data at each boosting iteration the default is
shrinkage was between and
tree depth was between and
boosted neural networks
since boosting is typically most effective with weak learners we boosted single hidden layer neural network with two or four units fit with the nnet package version in
random forests
we used the package randomforest version with default settings for the parameters
bagged neural networks
we used the same architecture as in the bayesian neural network above two hidden layers of and units fit using both neal's language package flexible bayesian modeling release and matlab neural net toolbox version
performance of different learning methods on five problems using both univariate screening of features top panel and reduced feature set from automatic relevance determination
the error bars at the top of each plot have width equal to one standard error of the difference between two error rates
on most of the problems several competitors are within this error bound
this analysis was carried out by nicholas johnson and full details may be found in johnson
the results are shown in figure and table
the figure and table show bayesian boosted and bagged neural networks boosted trees and random forests using both the screened and reduced features sets
the error bars at the top of each plot indicate one standard error of the difference between two error rates
bayesian neural networks again emerge as the winner although for some datasets the differences between the test error rates is not statistically significant
random forests performs the best among the competitors using the selected feature set while the boosted neural networks perform best with the reduced feature set and nearly match the bayesian neural net
the superiority of boosted neural networks over boosted trees suggest that the neural network model is better suited to these particular problems
specifically individual features might not be good predictors here we also thank isabelle guyon for help in preparing the results of this section
neural networks table
performance of different methods
values are average rank of test error across the five problems low is good and mean computation time and standard error of the mean in minutes
screened features ard reduced features method average average average average rank time rank time bayesian neural networks boosted trees boosted neural networks random forests bagged neural networks and linear combinations of features work better
however the impressive performance of random forests is at odds with this explanation and came as surprise to us
since the reduced feature sets come from the bayesian neural network approach only the methods that use the screened features are legitimate self contained procedures
however this does suggest that better methods for internal feature selection might help the overall performance of boosted neural networks
the table also shows the approximate training time required for each method
here the non bayesian methods show clear advantage
overall the superior performance of bayesian neural networks here may be due to the fact that the neural network model is well suited to these five problems and the mcmc approach provides an efficient way of exploring the important part of the parameter space and then averaging the resulting models according to their quality
the bayesian approach works well for smoothly parametrized models like neural nets it is not yet clear that it works as well for non smooth models like trees
computational considerations with observations predictors hidden units and training epochs neural network fit typically requires pm operations
there are many packages available for fitting neural networks probably many more than exist for mainstream statistical methods
because the available software varies widely in quality and the learning problem for neural networks is sensitive to issues such as input scaling such software should be carefully chosen and tested
exercises bibliographic notes projection pursuit was proposed by friedman and tukey and specialized to regression by friedman and stuetzle
huber gives scholarly overview and roosen and hastie present formulation using smoothing splines
the motivation for neural networks dates back to mcculloch and pitts widrow and hoff reprinted in anderson and rosenfeld and rosenblatt
hebb heavily influenced the development of learning algorithms
the resurgence of neural networks in the mid was due to werbos parker and rumelhart et al who proposed the back propagation algorithm
today there are many books written on the topic for broad range of audiences
for readers of this book hertz et al
bishop and ripley may be the most informative
bayesian learning for neural networks is described in neal
the zip code example was taken from le cun see also le cun et al and le cun et al
we do not discuss theoretical topics such as approximation properties of neural networks such as the work of barron girosi et al and jones
some of these results are summarized by ripley
exercises ex
establish the exact correspondence between the projection pursuit regression model and the neural network
in particular show that the single layer regression network is equivalent to ppr model with gm sm where is the mth unit vector
establish similar equivalence for classification network
consider neural network for quantitative outcome as in using squared error loss and identity output function gk
suppose that the weights from the input to hidden layer are nearly zero
show that the resulting model is nearly linear in the inputs
derive the forward and backward propagation equations for the cross entropy loss function
consider neural network for class outcome that uses crossentropy loss
if the network has no hidden layer show that the model is equivalent to the multinomial logistic model described in chapter
write program to fit single hidden layer neural network ten hidden units via back propagation and weight decay
neural networks apply it to observations from the model at at where is the sigmoid function is standard normal each xj being independent standard normal and
generate test sample of size and plot the training and test error curves as function of the number of training epochs for different values of the weight decay parameter
discuss the overfitting behavior in each case vary the number of hidden units in the network from up to and determine the minimum number needed to perform well for this task
write program to carry out projection pursuit regression using cubic smoothing splines with fixed degrees of freedom
fit it to the data from the previous exercise for various values of the smoothing parameter and number of model terms
find the minimum number of model terms necessary for the model to perform well and compare this to the number of hidden units from the previous exercise
fit neural network to the spam data of section and compare the results to those for the additive model given in that chapter
compare both the classification performance and interpretability of the final model
this is page printer opaque this support vector machines and flexible discriminants introduction in this chapter we describe generalizations of linear decision boundaries for classification
optimal separating hyperplanes are introduced in chapter for the case when two classes are linearly separable
here we cover extensions to the nonseparable case where the classes overlap
these techniques are then generalized to what is known as the support vector machine which produces nonlinear boundaries by constructing linear boundary in large transformed version of the feature space
the second set of methods generalize fisher's linear discriminant analysis lda
the generalizations include flexible discriminant analysis which facilitates construction of nonlinear boundaries in manner very similar to the support vector machines penalized discriminant analysis for problems such as signal and image classification where the large number of features are highly correlated and mixture discriminant analysis for irregularly shaped classes
the support vector classifier in chapter we discussed technique for constructing an optimal separating hyperplane between two perfectly separated classes
we review this and generalize to the nonseparable case where the classes may not be separable by linear boundary
support vector classifiers
the left panel shows the separable case
the decision boundary is the solid line while broken lines bound the shaded maximal margin of width
the right panel shows the nonseparable overlap case
the points labeled be are on the wrong side of their margin by an amount be be points on thepcorrect side have be
the margin is maximized subject to total budget be constant
hence be is the total distance of points on the wrong side of their margin
our training data consists of pairs xn yn with xi irp and yi
define hyperplane by xt where is unit vector
classification rule induced by is sign xt
the geometry of hyperplanes is reviewed in section where we show that in gives the signed distance from point to the hyperplane xt
since the classes are separable we can find function xt with yi xi
hence we are able to find the hyperplane that creates the biggest margin between the training points for class and see figure
the optimization problem max subject to yi xti captures this concept
the band in the figure is units away from the hyperplane on either side and hence units wide
it is called the margin
we showed that this problem can be more conveniently rephrased as min subject to yi xti
the support vector classifier where we have dropped the norm constraint on
note that
expression is the usual way of writing the support vector criterion for separated data
this is convex optimization problem quadratic criterion linear inequality constraints and the solution is characterized in section
suppose now that the classes overlap in feature space
one way to deal with the overlap is to still maximize but allow for some points to be on the wrong side of the margin
define the slack variables be be be be
there are two natural ways to modify the constraint in yi xti be or yi xti be pn be be constant
the two choices lead to different solutions
the first choice seems more natural since it measures overlap in actual distance from the margin the second choice measures the overlap in relative distance which changes with the width of the margin
however the first choice results in nonconvex optimization problem while the second is convex thus leads to the standard support vector classifier which we use from here on
here is the idea of the formulation
the value be in the constraint yi xti be is the proportional amount by which the prediction xti is on the wrong side of its margin
hence by bounding the xi sum be we bound the total proportional amount by which predictions side of their margin
misclassifications occur when be fall on the wrong so bounding be at value say bounds the total number of training misclassifications at
as in in section we can drop the norm constraint on define and write in the equivalent form yi xti be min subject to be be constant
this is the usual way the support vector classifier is defined for the nonseparable case
however we find confusing the presence of the fixed scale in the constraint yi xti be and prefer to start with
the right panel of figure illustrates this overlapping case
by the nature of the criterion we see that points well inside their class boundary do not play big role in shaping the boundary
this seems like an attractive property and one that differentiates it from linear discriminant analysis section
in lda the decision boundary is determined by the covariance of the class distributions and the positions of the class centroids
we will see in section that logistic regression is more similar to the support vector classifier in this regard
flexible discriminants computing the support vector classifier the problem is quadratic with linear inequality constraints hence it is convex optimization problem
we describe quadratic programming solution using lagrange multipliers
computationally it is convenient to re express in the equivalent form xn min be subject to be yi xti be where the cost parameter replaces the constant in the separable case corresponds to
the lagrange primal function is xn xn xn lp be yi xti be i be which we minimize and be
setting the respective derivatives to zero we get yi xi yi i as well as the positivity constraints i be
by substituting into we obtain the lagrangian wolfe dual objective function xx ld yi yi xti xi which gives lower bound on the objective function for any feasible pn point
we maximize ld subject to and yi
in addition to the karush kuhn tucker conditions include the constraints yi xti be i be yi xti be for
together these equations uniquely characterize the solution to the primal and dual problem
the support vector classifier from we see that the solution for has the form yi xi with nonzero coefficients only for those observations for which the constraints in are exactly met due to
these observations are called the support vectors since is represented in terms of them alone
among these support points some will lie on the edge of the margin be and hence from and will be characterized by the remainder be have
from we can see that any of these margin points be can be used to solve for and we typically use an average of all the solutions for numerical stability
maximizing the dual is simpler convex quadratic programming problem than the primal and can be solved with standard techniques murray et al for example
given the solutions and the decision function can be written as sign sign xt
the tuning parameter of this procedure is the cost parameter
mixture example continued figure shows the support vector boundary for the mixture example of figure on page with two overlapping classes for two different values of the cost parameter
the classifiers are rather similar in their performance
points on the wrong side of the boundary are support vectors
in addition points on the correct side of the boundary but close to it in the margin are also support vectors
the margin is larger for than it is for
hence larger values of focus attention more on correctly classified points near the decision boundary while smaller values involve data further away
either way misclassified points are given weight no matter how far away
in this example the procedure is not very sensitive to choices of because of the rigidity of linear boundary
the optimal value for can be estimated by cross validation as discussed in chapter
interestingly the leave one out cross validation error can be bounded above by the proportion of support points in the data
the reason is that leaving out an observation that is not support vector will not change the solution
hence these observations being classified correctly by the original boundary will be classified correctly in the cross validation process
however this bound tends to be too high and not generally useful for choosing and respectively in our examples
error
the linear support vector boundary for the mixture data example with two overlapping classes for two different values of
the broken lines indicate the margins where
the support points are all the points on the wrong side of their margin
the black solid dots are those support points falling exactly on the margin be
in the upper panel of the observations are support points while in the lower panel are
the broken purple curve in the background is the bayes decision boundary
support vector machines and kernels support vector machines and kernels the support vector classifier described so far finds linear boundaries in the input feature space
as with other linear methods we can make the procedure more flexible by enlarging the feature space using basis expansions such as polynomials or splines chapter
generally linear boundaries in the enlarged space achieve better training class separation and translate to nonlinear boundaries in the original space
once the basis functions hm are selected the procedure is the same as before
we fit the sv classifier using input features xi xi xi hm xi and produce the nonlinear function
the classifier is sign as before
the support vector machine classifier is an extension of this idea where the dimension of the enlarged space is allowed to get very large infinite in some cases
it might seem that the computations would become prohibitive
it would also seem that with sufficient basis functions the data would be separable and overfitting would occur
we first show how the svm technology deals with these issues
we then see that in fact the svm classifier is solving function fitting problem using particular criterion and form of regularization and is part of much bigger class of problems that includes the smoothing splines of chapter
the reader may wish to consult section which provides background material and overlaps somewhat with the next two sections
computing the svm for classification we can represent the optimization problem and its solution in special way that only involves the input features via inner products
we do this directly for the transformed feature vectors xi
we then see that for particular choices of these inner products can be computed very cheaply
the lagrange dual function has the form xx ld yi yi hh xi xi
from we see that the solution function can be written yi hh xi
as before given can be determined by solving yi xi in for any or all xi for which
flexible discriminants so both and involve only through inner products
in fact we need not specify the transformation at all but require only knowledge of the kernel function hh that computes inner products in the transformed space
should be symmetric positive semi definite function see section
three popular choices for in the svm literature are dth degree polynomial hx radial basis exp kx neural network tanh ba hx ba
consider for example feature space with two inputs and and polynomial kernel of degree
then hx
then and if we choose vh and then hh
from we see that the solution can be written yi xi
the role of the parameter is clearer in an enlarged feature space since perfect separation is often achievable there
large value of will discourage any positive be and lead to an overfit wiggly boundary in the original feature space small value of will encourage small value of which in turn causes and hence the boundary to be smoother
figure show two nonlinear support vector machines applied to the mixture example of chapter
the regularization parameter was chosen in both cases to achieve good test error
the radial basis kernel produces boundary quite similar to the bayes optimal boundary for this example compare figure
in the early literature on support vectors there were claims that the kernel property of the support vector machine is unique to it and allows one to finesse the curse of dimensionality
neither of these claims is true and we go into both of these issues in the next three subsections
two nonlinear svms for the mixture data
the upper plot uses th degree polynomial kernel the lower radial basis kernel with
in each case was tuned to approximately achieve the best test error performance and worked well in both cases
the radial basis kernel performs the best close to bayes optimal as might be expected given the data arise from mixtures of gaussians
the broken purple curve in the background is the bayes decision boundary
the support vector loss function hinge loss compared to the negative log likelihood loss binomial deviance for logistic regression squared error loss and huberized version of the squared hinge loss
all are shown as function of yf rather than because of the symmetry between the and case
the deviance and huber have the same asymptotes as the svm loss but are rounded in the interior
all are scaled to have the limiting left tail slope of
the svm as penalization method with consider the optimization problem bb min yi xi where the subscript indicates positive part
this has the form loss penalty which is familiar paradigm in function estimation
it is easy to show exercise that the solution to with bb is the same as that for
examination of the hinge loss function yf shows that it is reasonable for two class classification when compared to other more traditional loss functions
figure compares it to the log likelihood loss for logistic regression as well as squared error loss and variant thereof
the negative log likelihood or binomial deviance has similar tails as the svm loss giving zero penalty to points well inside their margin and
support vector machines and kernels table
the population minimizers for the different loss functions in figure
logistic regression uses the binomial log likelihood or deviance
linear discriminant analysis exercise uses squared error loss
the svm hinge loss estimates the mode of the posterior class probabilities whereas the others estimate linear transformation of these probabilities
loss function minimizing function binomial pr deviance log yf log pr svm hinge yf sign pr loss squared yf pr error huberised yf yf pr square hinge loss yf otherwise linear penalty to points on the wrong side and far away
squared error on the other hand gives quadratic penalty and points well inside their own margin have strong influence on the model as well
the squared hinge loss yf is like the quadratic except it is zero for points inside their margin
it still rises quadratically in the left tail and will be less robust than hinge or deviance to misclassified observations
recently rosset and zhu proposed huberized version of the squared hinge loss which converts smoothly to linear loss at yf
we can characterize these loss functions in terms of what they are estimating at the population level
we consider minimizing el
table summarizes the results
whereas the hinge loss estimates the classifier itself all the others estimate transformation of the class posterior probabilities
the huberized square hinge loss shares attractive properties of logistic regression smooth loss function estimates probabilities as well as the svm hinge loss support points
formulation casts the svm as regularized function estimation problem where the coefficients of the linear expansion are shrunk toward zero excluding the constant
if represents hierarchical basis having some ordered structure such as ordered in roughness
flexible discriminants then the uniform shrinkage makes more sense if the rougher elements hj in the vector have smaller norm
all the loss function in table except squared error are so called margin maximizing loss functions rosset et al
this means that if the data are separable then the limit of bb in as bb defines the optimal separating hyperplane
function estimation and reproducing kernels here we describe svms in terms of function estimation in reproducing kernel hilbert spaces where the kernel property abounds
this material is discussed in some detail in section
this provides another view of the support vector classifier and helps to clarify how it works
suppose the basis arises from the possibly finite eigen expansion of positive definite kernel and hm
then with we can write as xn bb min yi xi
now is identical in form to on page in section and the theory of reproducing kernel hilbert spaces described there guarantees finite dimensional solution of the form xi
in particular we see there an equivalent version of the optimization criterion equation in section see also wahba et al
bb min yi xi where is the matrix of kernel evaluations for all pairs of training features exercise
these models are quite general and include for example the entire family of smoothing splines additive and interaction spline models discussed for logistic regression with separable data diverges but converges to bb bb bb the optimal separating direction
support vector machines and kernels in chapters and and in more detail in wahba and hastie and tibshirani
they can be expressed more generally as min yi xi bb where is the structured space of functions and an appropriate regularizer on that space
pp for example suppose is the space of additive functions fj xj and xj dxj
then the pp cubic spline and has kernel representasolution to is an additive tion with kj xj
each of the kj is the kernel appropriate for the univariate smoothing spline in xj wahba
conversely this discussion also shows that for example any of the kernels described in above can be used with any convex loss function and will also lead to finite dimensional representation of the form
figure uses the same kernel functions as in figure except using the binomial log likelihood as loss function
the fitted function is hence an estimate of the log odds log xi or conversely we get an estimate of the class probabilities pn
xi the fitted models are quite similar in shape and performance
examples and more details are given in section
it does happen that for svms sizable fraction of the values of can be zero the nonsupport points
in the two examples in figure these fractions are and respectively
this is consequence of the piecewise linear nature of the first part of the criterion
the lower the class overlap on the training data the greater this fraction will be
reducing bb will generally reduce the overlap allowing more flexible
small number of support points means that can be evaluated more quickly which is important at lookup time
of course reducing the overlap too much can lead to poor generalization
ji zhu assisted in the preparation of these examples
the logistic regression versions of the svm models in figure using the identical kernels and hence penalties but the log likelihood loss instead of the svm loss function
the two broken contours correspond to posterior probabilities of and for the class or vice versa
the broken purple curve in the background is the bayes decision boundary
support vector machines and kernels table
skin of the orange shown are mean standard error of the mean of the test error over simulations
bruto fits an additive spline model adaptively while mars fits low order interaction model adaptively
test error se method no noise features six noise features sv classifier svm poly svm poly svm poly bruto mars bayes svms and the curse of dimensionality in this section we address the question of whether svms have some edge on the curse of dimensionality
notice that in expression we are not allowed fully general inner product in the space of powers and products
for example all terms of the form xj xj are given equal weight and the kernel cannot adapt itself to concentrate on subspaces
if the number of features were large but the class separation occurred only in the linear subspace spanned by say and this kernel would not easily find the structure and would suffer from having many dimensions to search over
one would have to build knowledge about the subspace into the kernel that is tell it to ignore all but the first two inputs
if such knowledge were available priori much of statistical learning would be made much easier
major goal of adaptive methods is to discover such structure
we support these statements with an illustrative example
we generated observations in each of two classes
the first class has four standard has normal independent features
the second class also four standard normal independent features but conditioned on xj
this is relatively easy problem
as second harder problem we augmented the features with an additional six standard gaussian noise features
hence the second class almost completely surrounds the first like the skin surrounding the orange in four dimensional subspace
the bayes error rate for this problem is irrespective of dimension
we generated test observations to compare different procedures
the average test errors over simulations with and without noise features are shown in table
line uses the support vector classifier in the original feature space
lines refer to the support vector machine with and dimensional polynomial kernel
for all support vector procedures we chose the cost parameter to minimize the test error to be as fair as possible to the
test error curves as function of the cost parameter for the radial kernel svm classifier on the mixture data
at the top of each plot is the scale parameter for the radial kernel exp
the optimal value for depends quite strongly on the scale of the kernel
the bayes error rate is indicated by the broken horizontal lines method
line fits an additive spline model to the response by least squares using the bruto algorithm for additive models described in hastie and tibshirani
line uses mars multivariate adaptive regression splines allowing interaction of all orders as described in chapter as such it is comparable with the svm poly
both bruto and mars have the ability to ignore redundant variables
test error was not used to choose the smoothing parameters in either of lines or
in the original feature space hyperplane cannot separate the classes and the support vector classifier line does poorly
the polynomial support vector machine makes substantial improvement in test error rate but is adversely affected by the six noise features
it is also very sensitive to the choice of kernel the second degree polynomial kernel line does best since the true decision boundary is second degree polynomial
however higher degree polynomial kernels lines and do much worse
bruto performs well since the boundary is additive
bruto and mars adapt well their performance does not deteriorate much in the presence of noise
path algorithm for the svm classifier the regularization parameter for the svm classifier is the cost parameter or its inverse bb in
common usage is to set high leading often to somewhat overfit classifiers
figure shows the test error on the mixture data as function of using different radial kernel parameters
when narrow peaked kernels the heaviest regularization small is called for
with
simple example illustrates the svm path algorithm left panel this plot illustrates the state of the model at bb
the points are orange the blue
bb and the width of the soft margin is
two blue points are misclassified while the two orange points are correctly classified but on the wrong side of their margin each of these has yi xi
the three square shaped points are exactly on their margins right panel this plot shows the piecewise linear profiles bb
the horizontal broken line at bb indicates the state of the for the model in the left plot the value used in figure an intermediate value of is required
clearly in situations such as these we need to determine good choice for perhaps by cross validation
here we describe path algorithm in the spirit of section for efficiently fitting the entire sequence of svm models obtained by varying
it is convenient to use the loss penalty formulation along with figure
this leads to solution for at given value of bb bb yi xi
bb the are again lagrange multipliers but in this case they all lie in
figure illustrates the setup
it can be shown that the kkt optimality conditions imply that the labeled points xi yi fall into three distinct groups
they have yi xi and lagrange multipliers
examples are the orange points and and the blue points and
examples are the orange and the blue and
examples are the blue and and the orange and
the idea for the path algorithm is as follows
initially bb is large the margin bb is wide and all points are inside their margin and have
as bb decreases bb decreases and the margin gets narrower
some points will move from inside their margins to outside their margins and their will change from to
by continuity of the bb these points will linger on the margin during this transition
from we see that the points with make fixed contributions to bb and those with make no contribution
so all that changes as bb decreases are the of those small number of points on the margin
since all these points have yi xi this results in small set of linear equations that prescribe how bb and hence bb changes during these transitions
this results in piecewise linear paths for each of the bb
the breaks occur when points cross the margin
figure right panel shows the bb profiles for the small example in the left panel
although we have described this for linear svms exactly the same idea works for nonlinear models in which is replaced by bb yi xi
bb details can be found in hastie et al
an package svmpath is available on cran for fitting these models
support vector machines for regression in this section we show how svms can be adapted for regression with quantitative response in ways that inherit some of the properties of the svm classifier
we first discuss the linear regression model xt and then handle nonlinear generalizations
to estimate we consider minimization of bb yi xi
the left panel shows the insensitive error function used by the support vector regression machine
the right panel shows the error function used in huber's robust regression blue curve
beyond the function changes from quadratic to linear where if vo otherwise
this is an insensitive error measure ignoring errors of size less than left panel of figure
there is rough analogy with the support vector classification setup where points on the correct side of the decision boundary and far away from it are ignored in the optimization
in regression these low error points are the ones with small residuals
it is interesting to contrast this with error measures used in robust regression in statistics
the most popular due to huber has the form if vh shown in the right panel of figure
this function reduces from quadratic to linear the contributions of observations with absolute residual greater than prechosen constant
this makes the fitting less sensitive to outliers
the support vector error measure also has linear tails beyond but in addition it flattens the contributions of those cases with small residuals
if are the minimizers of the solution function can be shown to have the form xi hx xi
flexible discriminants where are positive and solve the quadratic programming problem min yi hxi xi subject to the constraints bb
due to the nature of these constraints typically only subset of the solution values are nonzero and the associated data values are called the support vectors
as was the case in the classification setting the solution depends on the input values only through the inner products hxi xi
thus we can generalize the methods to richer spaces by defining an appropriate inner product for example one of those defined in
note that there are parameters and bb associated with the criterion
these seem to play different roles is parameter of the loss function vo just like is for vh
note that both vo and vh depend on the scale of and hence
if we scale our response and hence use vh and vo instead then we might consider using preset values for and the value achieves efficiency for the gaussian
the quantity bb is more traditional regularization parameter and can be estimated for example by cross validation
regression and kernels as discussed in section this kernel property is not unique to support vector machines
suppose we consider approximation of the regression function in terms of set of basis functions hm hm
to estimate and we minimize bb yi xi for some general error measure
for any choice of the solution hm has the form xi
support vector machines and kernels pm with hm hm
notice that this has the same form as both the radial basis function expansion and regularization estimate discussed in chapters and
for concreteness let's work out the case
let be the basis matrix with imth element hm xi and suppose that is large
for simplicity we assume that or that the constant is absorbed in see exercise for an alternative
we estimate by minimizing the penalized least squares criterion bb
the solution is with determined by ht bb
from this it appears that we need to evaluate the matrix of inner products in the transformed space
however we can premultiply by to give hht bb hht
the matrix hht consists of inner products between pairs of observations that is the evaluation of an inner product kernel hht xi xi
it is easy to show directly in this case that the predicted values at an arbitrary satisfy xi where hht bb
as in the support vector machine we need not specify or evaluate the large set of functions hm
only the inner product kernel xi xi need be evaluated at the training points for each and at points for predictions there
careful choice of hm such as the eigenfunctions of particular easy to evaluate kernels means for example that hht can be computed at cost of evaluations of rather than the direct cost
note however that this property depends on the choice of squared norm in the penalty
it does not hold for example for the norm which may lead to superior model
flexible discriminants discussion the support vector machine can be extended to multiclass problems essentially by solving many two class problems
classifier is built for each pair of classes and the final classifier is the one that dominates the most kressel friedman hastie and tibshirani
alternatively one could use the multinomial loss function along with suitable kernel as in section
svms have applications in many other supervised and unsupervised learning problems
at the time of this writing empirical evidence suggests that it performs well in many real learning problems
finally we mention the connection of the support vector machine and structural risk minimization
suppose the training points or their basis expansion are contained in sphere of radius and let sign sign as in
then one can show that the class of functions has vc dimension satisfying
if separates the training data optimally for then with probability at least over training sets vapnik page log log error test
the support vector classifier was one of the first practical learning procedures for which useful bounds on the vc dimension could be obtained and hence the srm program could be carried out
however in the derivation balls are put around the data points process that depends on the observed values of the features
hence in strict sense the vc complexity of the class is not fixed priori before seeing the features
the regularization parameter controls an upper bound on the vc dimension of the classifier
following the srm paradigm we could choose by minimizing the upper bound on the test error given in
however it is not clear that this has any advantage over the use of cross validation for choice of
generalizing linear discriminant analysis in section we discussed linear discriminant analysis lda fundamental tool for classification
for the remainder of this chapter we discuss class of techniques that produce better classifiers than lda by directly generalizing lda
new observation is classified to the class with closest centroid
slight twist is that distance is measured in the mahalanobis metric using pooled covariance estimate
since this assumption is unlikely to be true this might not seem to be much of virtue
for example figure is an informative two dimensional view of data in dimensions with ten classes
lda was among the top three classifiers for of the datasets studied in the statlog project michie et al
when is large it is possible to estimate more complex decision boundaries
quadratic discriminant analysis qda is often useful here and allows for quadratic decision boundaries
more generally we would like to be able to model irregular decision boundaries
lda uses single prototype class centroid plus common covariance matrix to describe the spread of the data in each class
in many situations several prototypes are more appropriate
in this case lda uses too many parameters which are estimated with high variance and its performance suffers
in cases such as this we need to restrict or regularize lda even further
in the remainder of this chapter we describe class of techniques that attend to all these issues by generalizing the lda model
this is achieved largely by three different ideas
the first idea is to recast the lda problem as linear regression problem
many techniques exist for generalizing linear regression to more flexible nonparametric forms of regression
this in turn leads to more flexible forms of discriminant analysis which we call fda
in most cases of interest the this study predated the emergence of svms
flexible discriminants regression procedures can be seen to identify an enlarged set of predictors via basis expansions
fda amounts to lda in this enlarged space the same paradigm used in svms
in the case of too many predictors such as the pixels of digitized image we do not want to expand the set it is already too large
the second idea is to fit an lda model but penalize its coefficients to be smooth or otherwise coherent in the spatial domain that is as an image
we call this procedure penalized discriminant analysis or pda
with fda itself the expanded basis set is often so large that regularization is also required again as in svms
both of these can be achieved via suitably regularized regression in the context of the fda model
the third idea is to model each class by mixture of two or more gaussians with different centroids but with every component gaussian both within and between classes sharing the same covariance matrix
this allows for more complex decision boundaries and allows for subspace reduction as in lda
we call this extension mixture discriminant analysis or mda
all three of these generalizations use common framework by exploiting their connection with lda
flexible discriminant analysis in this section we describe method for performing lda using linear regression on derived responses
this in turn leads to nonparametric and flexible alternatives to lda
as in chapter we assume we have observations with quantitative response falling into one of classes each having measured features
suppose ir is function that assigns scores to the classes such that the transformed class labels are optimally predicted by linear regression on if our training sample has the form gi xi then we solve min gi xti with restrictions on to avoid trivial solution mean zero and unit variance over the training data
this produces one dimensional separation between the classes
more generally we can find up to sets of independent scorings for the class labels and corresponding linear maps chosen to be optimal for multiple regression in irp
the scores and the maps are chosen to minimize the average squared residual asr gi xi

flexible discriminant analysis the set of scores are assumed to be mutually orthogonal and normalized with respect to an appropriate inner product to prevent trivial zero solutions
why are we going down this road
it can be shown that the sequence of discriminant canonical vectors bd derived in section are identical to the sequence up to constant mardia et al hastie et al
moreover the mahalanobis distance of test point to the kth class centroid is given by where is the mean of the xi in the kth class and does not depend on
here are coordinate weights that are defined in terms of the mean squared residual of the th optimally scored fit
in section we saw that these canonical distances are all that is needed for classification in the gaussian setup with equal covariances in each class
to summarize lda can be performed by sequence of linear regressions followed by classification to the closest class centroid in the space of fits
the analogy applies both to the reduced rank version or the full rank case when
the real power of this result is in the generalizations that it invites
we can replace the linear regression fits xt by far more flexible nonparametric fits and by analogy achieve more flexible classifier than lda
we have in mind generalized additive fits spline functions mars models and the like
in this more general form the regression problems are defined via the criterion asr gi xi bb where is regularizer appropriate for some forms of nonparametric regression such as smoothing splines additive splines and lower order anova spline models
also included are the classes of functions and associated penalties generated by kernels as in section
before we describe the computations involved in this generalization let us consider very simple example
suppose we use degree polynomial regression for each
the decision boundaries implied by the will be quadratic surfaces since each of the fitted functions is quadratic and as
the data consist of points generated from each of and
the solid black ellipse is the decision boundary found by fda using degree two polynomial regression
the dashed purple circle is the bayes decision boundary in lda their squares cancel out when comparing distances
we could have achieved identical quadratic boundaries in more conventional way by augmenting our original predictors with their squares and cross products
in the enlarged space one performs an lda and the linear boundaries in the enlarged space map down to quadratic boundaries in the original space
classic example is pair of multivariate gaussians centered at the origin one having covariance matrix and the other ci for figure illustrates
the bayes decision boundary is the sphere kxk pc log which is linear boundary in the enlarged space
many nonparametric regression procedures operate by generating basis expansion of derived variables and then performing linear regression in the enlarged space
the mars procedure chapter is exactly of this form
smoothing splines and additive spline models generate an extremely large basis set basis functions for additive splines but then perform penalized regression fit in the enlarged space
svms do as well see also the kernel based regression example in section
fda in this case can be shown to perform penalized linear discriminant analysis in the enlarged space
we elaborate in section
linear boundaries in the enlarged space map down to nonlinear boundaries in the reduced space
this is exactly the same paradigm that is used with support vector machines section
we illustrate fda on the speech recognition example used in chapter with classes and predictors
the classes correspond to
the left plot shows the first two lda canonical variates for the vowel training data
the right plot shows the corresponding projection when fda bruto is used to fit the model plotted are the fitted regression functions xi and xi
notice the improved separation
the colors represent the eleven different vowel sounds vowel sounds each contained in different words
here are the words preceded by the symbols that represent them vowel word vowel word vowel word vowel word heed hod hid hoard head hood had who'd hard heard hud each of eight speakers spoke each word six times in the training set and likewise seven speakers in the test set
the ten predictors are derived from the digitized speech in rather complicated way but standard in the speech recognition world
there are thus training observations and test observations
figure shows two dimensional projections produced by lda and fda
the fda model used adaptive additive spline regression functions to model the and the points plotted in the right plot have coordinates xi and xi
the routine used in plus is called bruto hence the heading on the plot and in table
we see that flexible modeling has helped to separate the classes in this case
table shows training and test error rates for number of classification techniques
fda mars refers to friedman's multivariate adaptive regression splines degree means pairwise products are permitted
notice that for fda mars the best classification results are obtained in reduced rank subspace
flexible discriminants table
vowel recognition data performance results
the results for neural networks are the best among much larger set taken from neural network archive
the notation fda bruto refers to the regression method used with fda
technique error rates training test lda softmax qda cart cart linear combination splits single layer perceptron multi layer perceptron hidden units gaussian node network hidden units nearest neighbor fda bruto softmax fda mars degree best reduced dimension softmax fda mars degree best reduced dimension softmax computing the fda estimates the computations for the fda coordinates can be simplified in many important cases in particular when the nonparametric regression procedure can be represented as linear operator
we will denote this operator by bb that is bb where is the vector of responses and the vector of fits
additive splines have this property if the smoothing parameters are fixed as does mars once the basis functions are selected
the subscript bb denotes the entire set of smoothing parameters
in this case optimal scoring is equivalent to canonical correlation problem and the solution can be computed by single eigen decomposition
this is pursued in exercise and the resulting algorithm is presented here
we create an indicator response matrix from the responses gi such that yik if gi otherwise yik
for five class problem might look like the following
flexible discriminant analysis bb cc bb cc bb cc bb

gn here are the computational steps
multivariate nonparametric regression
fit multiresponse adaptive nonparametric regression of on giving fitted values
let bb be the linear operator that fits the final chosen model and be the vector of fitted regression functions
optimal scores
compute the eigen decomposition of yt yt bb where the eigenvectors are normalized
here yt is diagonal matrix of the estimated class prior probabilities
update the model from step using the optimal scores
the first of the functions in is the constant functiona trivial solution the remaining functions are the discriminant functions
the constant function along with the normalization causes all the remaining functions to be centered
again bb can correspond to any regression method
when bb hx the linear regression projection operator then fda is linear discriminant analysis
the software that we reference in the computational considerations section on page makes good use of this modularity the fda function has method argument that allows one to supply any regression function as long as it follows some natural conventions
the regression functions we provide allow for polynomial regression adaptive additive models and mars
they all efficiently handle multiple responses so step is single call to regression routine
the eigen decomposition in step simultaneously computes all the optimal scoring functions
in section we discussed the pitfalls of using linear regression on an indicator response matrix as method for classification
in particular severe masking can occur with three or more classes
fda uses the fits from such regression in step but then transforms them further to produce useful discriminant functions that are devoid of these pitfalls
exercise takes another view of this phenomenon
flexible discriminants penalized discriminant analysis although fda is motivated by generalizing optimal scoring it can also be viewed directly as form of regularized discriminant analysis
suppose the regression procedure used in fda amounts to linear regression onto basis expansion with quadratic penalty on the coefficients asr gi xi bb
the choice of depends on the problem
if is an expansion on spline basis functions might constrain to be smooth over irp
in the case of additive splines there are spline basis functions for each coordinate resulting in total of basis functions in in this case is and block diagonal
loosely speaking the penalized mahalanobis distance tends to give less weight to rough coordinates and more weight to smooth ones since the penalty is not diagonal the same applies to linear combinations that are rough or smooth
for some classes of problems the first step involving the basis expansion is not needed we already have far too many correlated predictors
the images appear in pairs and represent the nine discriminant coefficient functions for the digit recognition problem
the left member of each pair is the lda coefficient while the right member is the pda coefficient regularized to enforce spatial smoothness
it is also intuitively clear in these cases why regularization is needed
take the digitized image as an example
neighboring pixel values will tend to be correlated being often almost the same
this implies that the pair of corresponding lda coefficients for these pixels can be wildly different and opposite in sign and thus cancel when applied to similar pixel values
positively correlated predictors lead to noisy negatively correlated coefficient estimates and this noise results in unwanted sampling variance
reasonable strategy is to regularize the coefficients to be smooth over the spatial domain as with images
this is what pda does
the computations proceed just as for fda except that an appropriate penalized regression method is used
here ht and is chosen so that penalizes roughness in when viewed as an image
figure on page shows some examples of handwritten digits
figure shows the discriminant variates using lda and pda
those produced by lda appear as salt and pepper images while those produced by pda are smooth images
the first smooth image can be seen as the coefficients of linear contrast functional for separating images with dark central vertical strip ones possibly sevens from images that are hollow in the middle zeros some fours
figure supports this interpretation and with more difficulty allows an interpretation of the second coordinate
this and other
the first two penalized canonical variates evaluated for the test data
the circles indicate the class centroids
the first coordinate contrasts mainly and while the second contrasts and
mixture discriminant analysis examples are discussed in more detail in hastie et al who also show that the regularization improves the classification performance of lda on independent test data by factor of around in the cases they tried
mixture discriminant analysis linear discriminant analysis can be viewed as prototype classifier
each class is represented by its centroid and we classify to the closest using an appropriate metric
in many situations single prototype is not sufficient to represent inhomogeneous classes and mixture models are more appropriate
in this section we review gaussian mixture models and show how they can be generalized via the fda and pda methods discussed earlier
gaussian mixture model for the kth class has density rk kr kr where the mixing proportions kr sum to one
this has rk prototypes for the kth class and in our specification the same covariance matrix is used as the metric throughout
given such model for each class the class posterior probabilities are given by prk kr kr pk pr where represent the class prior probabilities
we saw these calculations for the special case of two components in chapter
as in lda we estimate the parameters by maximum likelihood using the joint log likelihood based on xk log kr xi kr
gi the sum within the log makes this rather messy optimization problem if tackled directly
the classical and natural method for computing the maximum likelihood estimates mles for mixture distributions is the em algorithm dempster et al which is known to possess good convergence properties
em alternates between the two steps
flexible discriminants step given the current parameters compute the responsibility of subclass ckr within class for each of the class observations gi kr xi kr ckr xi gi prk
xi k step compute the weighted mles for the parameters of each of the component gaussians within each of the classes using the weights from the step
in the step the algorithm apportions the unit weight of an observation in class to the various subclasses assigned to that class
if it is close to the centroid of particular subclass and far from the others it will receive mass close to one for that subclass
on the other hand observations halfway between two subclasses will get approximately equal weight for both
in the step an observation in class is used rk times to estimate the parameters in each of the rk component densities with different weight for each
the em algorithm is studied in detail in chapter
the algorithm requires initialization which can have an impact since mixture likelihoods are generally multimodal
our software referenced in the computational considerations on page allows several strategies here we describe the default
the user supplies the number rk of subclasses per class
within class means clustering model with multiple random starts is fitted to the data
this partitions the observations into rk disjoint groups from which an initial weight matrix consisting of zeros and ones is created
our assumption of an equal component covariance matrix throughout buys an additional simplicity we can incorporate rank restrictions in the mixture formulation just like in lda
to understand this we review littleknown fact about lda
the rank lda fit section is equivalent to the maximum likelihood fit of gaussian model where the different mean vectors in each class are confined to rank subspace of irp exercise
we can inherit this property for the mixture model and maximize the loglikelihood subject to rank constraints on all the rk centroids rank k
again the em algorithm is available and the step turns out to be weighted version of lda with rk classes
furthermore we can use optimal scoring as before to solve the weighted lda problem which allows us to use weighted version of fda or pda at this stage
one would expect in addition to an increase in the number of classes similar increase in the number of observations in the kth class by factor of rk
it turns out that this is not the case if linear operators are used for the optimal scoring regression
the enlarged indicator matrix collapses in this case to blurred response matrix which is intuitively pleasing
for example suppose there are classes and rk subclasses per class
then might be
mixture discriminant analysis eb ec ec ec ec ec ec ec ec
ec

ed
gn where the entries in class row correspond to ckr gi
the remaining steps are the same fc sz fd zt step of mda
mda substitutes subclasses for classes and then allows us to look at low dimensional views of the subspace spanned by these subclass centroids
this subspace will often be an important one for discrimination
for example we can fit mda models to digitized analog signals and images with smoothness constraints built in
figure compares fda and mda on the mixture example
example waveform data we now illustrate some of these ideas on popular simulated example taken from breiman et al pages and used in hastie and tibshirani and elsewhere
it is three class problem with variables and is considered to be difficult pattern recognition problem
the predictors are defined by xj oj class xj oj class xj oj class where is uniform on oj are standard normal variates and the are the shifted triangular waveforms max
fda and mda on the mixture data
the upper plot uses fda with mars as the regression procedure
the lower plot uses mda with five mixture centers per class indicated
the mda solution is close to bayes optimal as might be expected given the data arise from mixtures of gaussians
the broken purple curve in the background is the bayes decision boundary
some examples of the waveforms generated from model before the gaussian noise is added and
figure shows some example waveforms from each class
table shows the results of mda applied to the waveform data as well as several other methods from this and other chapters
each training sample has observations and equal priors were used so there are roughly observations in each class
we used test samples of size
the two mda models are described in the caption
figure shows the leading canonical variates for the penalized mda model evaluated at the test data
as we might have guessed the classes appear to lie on the edges of triangle
this is because the hj are represented by three points in space thereby forming vertices of triangle and each class is represented as convex combination of pair of vertices and hence lie on an edge
also it is clear visually that all the information lies in the first two dimensions the percentage of variance explained by the first two coordinates is and we would lose nothing by truncating the solution there
the bayes risk for this problem has been estimated to be about breiman et al
mda comes close to the optimal rate which is not surprising since the structure of the mda model is similar to the generating model
flexible discriminants table
results for waveform data
the values are averages over ten simulations with the standard error of the average in parentheses
the five entries above the line are taken from hastie et al
the first model below the line is mda with three subclasses per class
the next line is the same except that the discriminant coefficients are penalized via roughness penalty to effectively df
the third is the corresponding penalized lda or pda model
some two dimensional views of the mda model fitted to sample of the waveform model
the points are independent test data projected on to the leading two canonical coordinates left panel and the third and fourth right panel
the subclass centers are indicated
exercises computational considerations with training cases predictors and support vectors the support vector machine requires mn mpn operations assuming
they do not scale well with although computational shortcuts are available platt
since these are evolving rapidly the reader is urged to search the web for the latest technology
lda requires operations as does pda
the complexity of fda depends on the regression method used
many techniques are linear in such as additive models and mars
general splines and kernel based regression methods will typically require operations
software is available for fitting fda pda and mda models in the package mda which is also available in plus
bibliographic notes the theory behind support vector machines is due to vapnik and is described in vapnik
there is burgeoning literature on svms an online bibliography created and maintained by alex smola and bernhard scho lkopf can be found at http www kernel machines org
our treatment is based on wahba et al and evgeniou et al and the tutorial by burges burges
linear discriminant analysis is due to fisher and rao
the connection with optimal scoring dates back at least to breiman and ihaka and in simple form to fisher
there are strong connections with correspondence analysis greenacre
the description of flexible penalized and mixture discriminant analysis is taken from hastie et al
hastie et al and hastie and tibshirani and all three are summarized in hastie et al see also ripley
exercises ex
show that the criteria and are equivalent
show that the solution to is the same as the solution to for particular kernel
consider modification to where you do not penalize the constant
formulate the problem and characterize its solution
suppose you perform reduced subspace linear discriminant analysis for group problem
you compute the canonical variables of di
flexible discriminants mension given by ut where is the matrix of discriminant coefficients and is the dimension of if show that kz kz kx kw kx kw where kw denotes mahalanobis distance with respect to the covariance if show that the same expression on the left measures the difference in mahalanobis squared distances for the distributions projected onto the subspace spanned by
the data in phoneme subset available from this book's website http www stat stanford edu elemstatlearn consists of digitized log periodograms for phonemes uttered by speakers each speaker having produced phonemes from each of five classes
it is appropriate to plot each vector of features against the frequencies produce separate plot of all the phoneme curves against frequency for each class you plan to use nearest prototype classification scheme to classify the curves into phoneme classes
in particular you will use means clustering algorithm in each class kmeans in and then classify observations to the class of the closest cluster center
the curves are high dimensional and you have rather small sample size to variables ratio
you decide to restrict all the prototypes to be smooth functions of frequency
in particular you decide to represent each prototype as where is matrix of natural spline basis functions with knots uniformly chosen in and boundary knots at and
describe how to proceed analytically and in particular how to avoid costly high dimensional fitting procedures
hint it may help to restrict to be orthogonal implement your procedure on the phoneme data and try it out
divide the data into training set and test set making sure that speakers are not split across sets why
use centers per class and for each use knots taking care to start the means procedure at the same starting values for each value of and compare the results
suppose that the regression procedure used in fda section is linear expansion of basis functions hm
let yt be the diagonal matrix of class proportions
exercises show that the optimal scoring problem can be written in vector notation as min ky where is vector of real numbers and is the matrix of evaluations hj xi suppose that the normalization on is and
interpret these normalizations in terms of the original scored gi show that with this normalization can be partially optimized
and leads to max subject to the normalization constraints where is the projection operator corresponding to the basis matrix suppose that the hj include the constant function
show that the largest eigenvalue of is let be matrix of scores in columns and suppose the normalization is
show that the solution to is given by the complete set of eigenvectors of the first eigenvector is trivial and takes care of the centering of the scores
the remainder characterize the optimal scoring solution
derive the solution to the penalized optimal scoring problem
show that coefficients found by optimal scoring are proportional to the discriminant directions bd found by linear discriminant analysis
let xb be the fitted indicator response matrix after linear regression on the matrix where
consider the reduced features xi
show that lda using is equivalent to lda in the original space
kernels and linear discriminant analysis
suppose you wish to carry out linear discriminant analysis two classes using vector of transformations of the input variables
since is high dimensional you will use regularized within class covariance matrix wh
show that the model can be estimated using only the inner products xi xi hh xi xi
hence the kernel property of support vector machines is also shared by regularized linear discriminant analysis
the mda procedure models each class as mixture of gaussians
hence each mixture center belongs to one and only one class
more general model allows each mixture center to be shared by all classes
we take the joint density of labels and features to be
flexible discriminants pr mixture of joint densities
furthermore we assume pr pr r
this model consists of regions centered at r and for each there is class profile pr
the posterior class distribution is given by pr pr r pr r where the denominator is the marginal distribution show that this model called mda can be viewed as generalization of mda since pr pr r pr pr pr where rk pr pr corresponds to the mixing proportions for the kth class derive the em algorithm for mda show that if the initial weight matrix is constructed as in mda involving separate means clustering in each class then the algorithm for mda is identical to the original mda procedure
this is page printer opaque this prototype methods and nearest neighbors introduction in this chapter we discuss some simple and essentially model free methods for classification and pattern recognition
because they are highly unstructured they typically are not useful for understanding the nature of the relationship between the features and class outcome
however as black box prediction engines they can be very effective and are often among the best performers in real data problems
the nearest neighbor technique can also be used in regression this was touched on in chapter and works reasonably well for low dimensional problems
however with high dimensional features the bias variance tradeoff does not work as favorably for nearestneighbor regression as it does for classification
prototype methods throughout this chapter our training data consists of the pairs xn gn where gi is class label taking values in
prototype methods represent the training data by set of points in feature space
these prototypes are typically not examples from the training sample except in the case of nearest neighbor classification discussed later
each prototype has an associated class label and classification of query point is made to the class of the closest prototype
closest is usually defined by euclidean distance in the feature space after each feature has
prototypes and nearest neighbors been standardized to have overall mean and variance in the training sample
euclidean distance is appropriate for quantitative features
we discuss distance measures between qualitative and other kinds of feature values in chapter
these methods can be very effective if the prototypes are well positioned to capture the distribution of each class
irregular class boundaries can be represented with enough prototypes in the right places in feature space
the main challenge is to figure out how many prototypes to use and where to put them
methods differ according to the number and way in which prototypes are selected
means clustering means clustering is method for finding clusters and cluster centers in set of unlabeled data
these two steps are iterated until convergence
typically the initial centers are randomly chosen observations from the training data
details of the means procedure as well as generalizations allowing for different variable types and more general distance measures are given in chapter
figure upper panel shows simulated example with three classes and two features
we used prototypes per class and show the classification regions and the decision boundary
notice that number of the the in means refers to the number of cluster centers
since we have already reserved to denote the number of classes we denote the number of clusters by
simulated example with three classes and five prototypes per class
the data in each class are generated from mixture of gaussians
in the upper panel the prototypes were found by applying the means clustering algorithm separately in each class
in the lower panel the lvq algorithm starting from the means solution moves the prototypes away from the decision boundary
the broken purple curve in the background is the bayes decision boundary
prototypes and nearest neighbors algorithm learning vector quantization lvq
choose initial prototypes for each class mr for example by sampling training points at random from each class
sample training point xi randomly with replacement and let index the closest prototype mj to xi if gi they are in the same class move the prototype towards the training point mj mj xi mj where is the learning rate if gi they are in different classes move the prototype away from the training point mj mj xi mj
repeat step decreasing the learning rate with each iteration towards zero prototypes are near the class boundaries leading to potential misclassification errors for points near these boundaries
this results from an obvious shortcoming with this method for each class the other classes do not have say in the positioning of the prototypes for that class
better approach discussed next uses all of the data to position all prototypes
learning vector quantization in this technique due to kohonen prototypes are placed strategically with respect to the decision boundaries in an ad hoc way
lvq is an online algorithm observations are processed one at time
the idea is that the training points attract prototypes of the correct class and repel other prototypes
when the iterations settle down prototypes should be close to the training points in their class
the learning rate is decreased to zero with each iteration following the guidelines for stochastic approximation learning rates section
figure lower panel shows the result of lvq using the means solution as starting values
the prototypes have tended to move away from the decision boundaries and away from prototypes of competing classes
the procedure just described is actually called lvq
modifications lvq lvq etc have been proposed that can sometimes improve performance
drawback of learning vector quantization methods is the fact
nearest neighbor classifiers that they are defined by algorithms rather than optimization of some fixed criteria this makes it difficult to understand their properties
gaussian mixtures the gaussian mixture model can also be thought of as prototype method similar in spirit to means and lvq
we discuss gaussian mixtures in some detail in sections and
each cluster is described in terms of gaussian density which has centroid as in means and covariance matrix
the comparison becomes crisper if we restrict the component gaussians to have scalar covariance matrix exercise
observations close to the center of cluster will most likely get weight for that cluster and weight for every other cluster
observations half way between two clusters divide their weight accordingly
as consequence the gaussian mixture model is often referred to as soft clustering method while means is hard
similarly when gaussian mixture models are used to represent the feature density in each class it produces smooth posterior probabilities for classifying see on page
often this is interpreted as soft classification while in fact the classification rule is arg maxk
figure compares the results of means and gaussian mixtures on the simulated mixture problem of chapter
we see that although the decision boundaries are roughly similar those for the mixture model are smoother although the prototypes are in approximately the same positions
we also see that while both procedures devote blue prototype incorrectly to region in the northwest the gaussian mixture classifier can ultimately ignore this region while means cannot
lvq gave very similar results to means on this example and is not shown
nearest neighbor classifiers these classifiers are memory based and require no model to be fit
given query point we find the training points closest in distance to and then classify using majority vote among the neighbors
error
the upper panel shows the means classifier applied to the mixture data example
the decision boundary is piecewise linear
the lower panel shows gaussian mixture model with common covariance for all component gaussians
the em algorithm for the mixture model was started at the means solution
the broken purple curve in the background is the bayes decision boundary
nearest neighbor classifiers ties are broken at random
for simplicity we will assume that the features are real valued and we use euclidean distance in feature space
typically we first standardize each of the features to have mean zero and variance since it is possible that they are measured in different units
in chapter we discuss distance measures appropriate for qualitative and ordinal features and how to combine them for mixed data
adaptively chosen distance metrics are discussed later in this chapter
despite its simplicity nearest neighbors has been successful in large number of classification problems including handwritten digits satellite image scenes and ekg patterns
it is often successful where each class has many possible prototypes and the decision boundary is very irregular
figure upper panel shows the decision boundary of nearestneighbor classifier applied to the three class simulated data
the decision boundary is fairly smooth compared to the lower panel where nearestneighbor classifier was used
there is close relationship between nearestneighbor and prototype methods in nearest neighbor classification each training point is prototype
figure shows the training test and tenfold cross validation errors as function of the neighborhood size for the two class mixture problem
since the tenfold cv errors are averages of ten numbers we can estimate standard error
because it uses only the training point closest to the query point the bias of the nearest neighbor estimate is often low but the variance is high
famous result of cover and hart shows that asymptotically the error rate of the nearest neighbor classifier is never more than twice the bayes rate
the rough idea of the proof is as follows using squared error loss
we assume that the query point coincides with one of the training points so that the bias is zero
this is true asymptotically if the dimension of the feature space is fixed and the training data fills up the space in dense fashion
then the error of the bayes rule is just the variance of bernoulli random variate the target at the query point while the error of nearest neighbor rule is twice the variance of bernoulli random variate one contribution each for the training and query targets
we now give more detail for misclassification loss
at let be the dominant class and pk the true conditional probability for class
then bayes error pk nearest neighbor error pk pk pk
the asymptotic nearest neighbor error rate is that of random rule we pick both the classification and the test point at random with probabili
the broken purple curve in the background is the bayes decision boundary
the upper panel shows the misclassification errors as function of neighborhood size
standard error bars are included for fold cross validation
the lower panel shows the decision boundary for nearest neighbors which appears to be optimal for minimizing test error
the broken purple curve in the background is the bayes decision boundary
prototypes and nearest neighbors ties pk
for the nearest neighbor error rate is pk pk pk twice the bayes error rate
more generally one can show exercise pk pk pk pk
many additional results of this kind have been derived ripley summarizes number of them
this result can provide rough idea about the best performance that is possible in given problem
for example if the nearest neighbor rule has error rate then asymptotically the bayes error rate is at least
the kicker here is the asymptotic part which assumes the bias of the nearest neighbor rule is zero
in real problems the bias can be substantial
the adaptive nearest neighbor rules described later in this chapter are an attempt to alleviate this bias
for simple nearest neighbors the bias and variance characteristics can dictate the optimal number of near neighbors for given problem
this is illustrated in the next example
example comparative study we tested the nearest neighbors means and lvq classifiers on two simulated problems
there are independent features xj each uniformly distributed on
the two class target variable is defined as follows problem easy eb fc fd ed sign xj problem difficult
fe hence in the first problem the two classes are separated by the hyperplane in the second problem the two classes form checkerboard pattern in the hypercube defined by the first three features
the bayes error rate is zero in both problems
there were training and test observations
figure shows the mean and standard error of the misclassification error for nearest neighbors means and lvq over ten realizations as the tuning parameters are varied
we see that means and lvq give nearly identical results
for the best choices of their tuning parameters means and lvq outperform nearest neighbors for the first problem and they perform similarly for the second problem
notice that the best value of each tuning parameter is clearly situation dependent
for example nearest neighbors outperforms nearest neighbor by factor of in the
mean one standard error of misclassification error for nearest neighbors means blue and lvq red over ten realizations for two simulated problems easy and difficult described in the text
the first four panels are landsat images for an agricultural area in four spectral bands depicted by heatmap shading
the remaining two panels give the actual land usage color coded and the predicted land usage using five nearest neighbor rule described in the text first problem while nearest neighbor is best in the second problem by factor of
these results underline the importance of using an objective data based method like cross validation to estimate the best value of tuning parameter see figure and chapter
example nearest neighbors and image scene classification the statlog project michie et al used part of landsat image as benchmark for classification pixels
figure shows four heat map images two in the visible spectrum and two in the infrared for an area of agricultural land in australia
each pixel has class label from the element set red soil cotton vegetation stubble mixture gray soil damp gray soil very damp gray soil determined manually by research assistants surveying the area
the lower middle panel shows the actual land usage shaded by different colors to indicate the classes
the objective is to classify the land usage at pixel based on the information in the four spectral bands
five nearest neighbors produced the predicted map shown in the bottom right panel and was computed as follows
for each pixel we extracted an neighbor feature map the pixel itself and its immediate neighbors
pixel and its neighbor feature map see figure
this is done separately in the four spectral bands giving input features per pixel
then five nearest neighbors classification was carried out in this dimensional feature space
the resulting test error rate was about see figure
of all the methods used in the statlog project including lvq cart neural networks linear discriminant analysis and many others nearest neighbors performed best on this task
hence it is likely that the decision boundaries in ir are quite irregular
invariant metrics and tangent distance in some problems the training features are invariant under certain natural transformations
the nearest neighbor classifier can exploit such invariances by incorporating them into the metric used to measure the distances between objects
here we give an example where this idea was used with great success and the resulting classifier outperformed all others at the time of its development simard et al
the problem is handwritten digit recognition as discussed is chapter and section
the inputs are grayscale images with pixels some examples are shown in figure
at the top of figure is shown in its actual orientation middle and rotated and in either direction
such rotations can often occur in real handwriting and it is obvious to our eye that this is still after small rotations
hence we want our nearest neighbor classifier to consider these two to be close together similar
however the grayscale pixel values for rotated will look quite different from those in the original image and hence the two objects can be far apart in euclidean distance in ir
we wish to remove the effect of rotation in measuring distances between two digits of the same class
consider the set of pixel values consisting of the original and its rotated versions
this is one dimensional curve in ir depicted by the green curve passing through the in figure
figure shows stylized version of ir with two images indicated by xi and xi
these might be two different for example
through each image we have drawn the curve of rotated versions of that image called
test error performance for number of classifiers as reported by the statlog project
the entry dann is variant of nearest neighbors using an adaptive metric section
examples of grayscale images of handwritten digits
nearest neighbor classifiers transformations of tangent pixel space linear equation for images above
the top row shows in its original orientation middle and rotated versions of it
the green curve in the middle of the figure depicts this set of rotated in dimensional space
the red line is the tangent line to the curve at the original image with some on this tangent line and its equation shown at the bottom of the figure invariance manifolds in this context
now rather than using the usual euclidean distance between the two images we use the shortest distance between the two curves
in other words the distance between the two images is taken to be the shortest euclidean distance between any rotated version of first image and any rotated version of the second image
this distance is called an invariant metric
in principle one could carry out nearest neighbor classification using this invariant metric
however there are two problems with it
first it is very difficult to calculate for real images
second it allows large transformations that can lead to poor performance
for example would be considered close to after rotation of
we need to restrict attention to small rotations
the use of tangent distance solves both of these problems
as shown in figure we can approximate the invariance manifold of the image by its tangent at the original image
this tangent can be computed by estimating the direction vector from small rotations of the image or by more sophisticated spatial smoothing methods exercise
for large rotations the tangent image no longer looks like so the problem with large transformations is alleviated
tangent distance computation for two images xi and xi
rather than using the euclidean distance between xi and xi or the shortest distance between the two curves we use the shortest distance between the two tangent lines
the idea then is to compute the invariant tangent line for each training image
for query image to be classified we compute its invariant tangent line and find the closest line to it among the lines in the training set
the class digit corresponding to this closest line is our predicted class for the query image
in figure the two tangent lines intersect but this is only because we have been forced to draw two dimensional representation of the actual dimensional situation
in ir the probability of two such lines intersecting is effectively zero
now simpler way to achieve this invariance would be to add into the training set number of rotated versions of each training image and then just use standard nearest neighbor classifier
this idea is called hints in abu mostafa and works well when the space of invariances is small
so far we have presented simplified version of the problem
in addition to rotation there are six other types of transformations under which we would like our classifier to be invariant
there are translation two directions scaling two directions sheer and character thickness
hence the curves and tangent lines in figures and are actually dimensional manifolds and hyperplanes
it is infeasible to add transformed versions of each training image to capture all of these possibilities
the tangent manifolds provide an elegant way of capturing the invariances
table shows the test misclassification error for problem with training images and test digits the
postal services database for carefully constructed neural network and simple nearest neighbor and
adaptive nearest neighbor methods table
test error rates for the handwritten zip code problem
method error rate neural net nearest neighbor euclidean distance nearest neighbor tangent distance tangent distance nearest neighbor rules
the tangent distance nearestneighbor classifier works remarkably well with test error rates near those for the human eye this is notoriously difficult test set
in practice it turned out that nearest neighbors are too slow for online classification in this application see section and neural network classifiers were subsequently developed to mimic it
adaptive nearest neighbor methods when nearest neighbor classification is carried out in high dimensional feature space the nearest neighbors of point can be very far away causing bias and degrading the performance of the rule
to quantify this consider data points uniformly distributed in the unit cube
let be the radius of nearest neighborhood centered at the origin
then median vp where vp rp is the volume of the sphere of radius in dimensions
figure shows the median radius for various training sample sizes and dimensions
we see that median radius quickly approaches the distance to the edge of the cube
what can be done about this problem
consider the two class situation in figure
there are two features and nearest neighborhood at query point is depicted by the circular region
implicit in near neighbor classification is the assumption that the class probabilities are roughly constant in the neighborhood and hence simple averages give good estimates
however in this example the class probabilities vary only in the horizontal direction
if we knew this we would stretch the neighborhood in the vertical direction as shown by the tall rectangular region
this will reduce the bias of our estimate and leave the variance the same
in general this calls for adapting the metric used in nearest neighbor classification so that the resulting neighborhoods stretch out in directions for which the class probabilities don't change much
in high dimensional feature space the class probabilities might change only low dimensional subspace and hence there can be considerable advantage to adapting the metric
median radius of nearest neighborhood for uniform data with observations in dimensions
the points are uniform in the cube with the vertical line separating class red and green
the vertical strip denotes the nearest neighbor region using only the horizontal coordinate to find the nearest neighbors for the target point solid dot
the sphere shows the nearest neighbor region using both coordinates and we see in this case it has extended into the class red region and is dominated by the wrong class in this instance
adaptive nearest neighbor methods friedman proposed method in which rectangular neighborhoods are found adaptively by successively carving away edges of box containing the training data
here we describe the discriminant adaptive nearest neighbor dann rule of hastie and tibshirani
earlier related proposals appear in short and fukunaga and myles and hand
at each query point neighborhood of say points is formed and the class distribution among the points is used to decide how to deform the neighborhood that is to adapt the metric
the adapted metric is then used in nearest neighbor rule at the query point
thus at each query point potentially different metric is used
in figure it is clear that the neighborhood should be stretched in the direction orthogonal to line joining the class centroids
this direction also coincides with the linear discriminant boundary and is the direction in which the class probabilities change the least
in general this direction of maximum change will not be orthogonal to the line joining the class centroids see figure on page
assuming local discriminant model the information contained in the local withinand between class covariance matrices is all that is needed to determine the optimal shape of the neighborhood
the discriminant adaptive nearest neighbor dann metric at query point is defined by where bw oi oi
pk here is the pooled within class covariance matrix wk and pk is the between class covariance matrix with and computed using only the nearest neighbors around
after computation of the metric it is used in nearest neighbor rule at
this complicated formula is actually quite simple in its operation
it first spheres the data with respect to and then stretches the neighborhood in the zero eigenvalue directions of the between matrix for the sphered data
this makes sense since locally the observed class means do not differ in these directions
the parameter rounds the neighborhood from an infinite strip to an ellipsoid to avoid using points far away from the query point
the value of seems to work well in general
figure shows the resulting neighborhoods for problem where the classes form two concentric circles
notice how the neighborhoods stretch out orthogonally to the decision boundaries when both classes are present in the neighborhood
in the pure regions with only one class the neighborhoods remain circular
neighborhoods found by the dann procedure at various query points centers of the crosses
there are two classes in the data with one class surrounding the other nearest neighbors were used to estimate the local metrics
shown are the resulting metrics used to form nearest neighborhoods in these cases the between matrix and the in is the identity matrix
example here we generate two class data in ten dimensions analogous to the twodimensional example of figure
all ten predictors in class are independent standard normal conditioned on the radius being greater than and less than while the predictors in class are independent standard normal without the restriction
there are observations in each class
hence the first class almost completely surrounds the second class in the full ten dimensional space
in this example there are no pure noise variables the kind that nearestneighbor subset selection rule might be able to weed out
at any given point in the feature space the class discrimination occurs along only one direction
however this direction changes as we move across the feature space and all variables are important somewhere in the space
figure shows boxplots of the test error rates over ten realizations for standard nearest neighbors lvq and discriminant adaptive nearest neighbors
we used prototypes per class for lvq to make it comparable to nearest neighbors since
the adaptive metric significantly reduces the error rate compared to lvq or standard nearest neighbors
ten dimensional simulated example boxplots of the test error rates over ten realizations for standard nearest neighbors lvq with centers and discriminant adaptive nearest neighbors global dimension reduction for nearest neighbors the discriminant adaptive nearest neighbor method carries out local dimension reduction that is dimension reduction separately at each query point
in many problems we can also benefit from global dimension reduction that is apply nearest neighbor rule in some optimally chosen subspace of the original feature space
for example suppose that the two classes form two nested spheres in four dimensions of feature space and there are an additional six noise features whose distribution is independent of class
then we would like to discover the important four dimensional subspace and carry out nearest neighbor classification in that reduced subspace
hastie and tibshirani discuss variation of the discriminantadaptive nearest neighbor method for this purpose
at each training point xi the between centroids sum of squares matrix bi is computed and then these matrices are averaged over all training points bi
let ep be the eigenvectors of the matrix ordered from largest to smallest eigenvalue
then these eigenvectors span the optimal subspaces for global subspace reduction
the derivation plis based on the fact that the best rank approximation to et solves the least squares problem min trace bi
rank since each bi contains information on the local discriminant subspace and the strength of discrimination in that subspace can be seen
prototypes and nearest neighbors as way of finding the best approximating subspace of dimension to series of subspaces by weighted least squares exercise
in the four dimensional sphere example mentioned above and examined in hastie and tibshirani four of the eigenvalues turn out to be large having eigenvectors nearly spanning the interesting subspace and the remaining six are near zero
operationally we project the data into the leading four dimensional subspace and then carry out nearest neighbor classification
in the satellite image classification example in section the technique labeled dann in figure used nearest neighbors in globally reduced subspace
there are also connections of this technique with the sliced inverse regression proposal of duan and li
these authors use similar ideas in the regression setting but do global rather than local computations
they assume and exploit spherical symmetry of the feature distribution to estimate interesting subspaces
computational considerations one drawback of nearest neighbor rules in general is the computational load both in finding the neighbors and storing the entire training set
with observations and predictors nearest neighbor classification requires operations to find the neighbors per query point
there are fast algorithms for finding nearest neighbors friedman et al friedman et al which can reduce this load somewhat
hastie and simard reduce the computations for tangent distance by developing analogs of means clustering in the context of this invariant metric
reducing the storage requirements is more difficult and various editing and condensing procedures have been proposed
the idea is to isolate subset of the training set that suffices for nearest neighbor predictions and throw away the remaining training data
intuitively it seems important to keep the training points that are near the decision boundaries and on the correct side of those boundaries while some points far from the boundaries could be discarded
the multi edit algorithm of devijver and kittler divides the data cyclically into training and test sets computing nearest neighbor rule on the training set and deleting test points that are misclassified
the idea is to keep homogeneous clusters of training observations
the condensing procedure of hart goes further trying to keep only important exterior points of these clusters
starting with single randomly chosen observation as the training set each additional data item is processed one at time adding it to the training set only if it is misclassified by nearest neighbor rule computed on the current training set
these procedures are surveyed in dasarathy and ripley
they can also be applied to other learning procedures besides nearest
exercises neighbors
while such methods are sometimes useful we have not had much practical experience with them nor have we found any systematic comparison of their performance in the literature
bibliographic notes the nearest neighbor method goes back at least to fix and hodges
the extensive literature on the topic is reviewed by dasarathy chapter of ripley contains good summary
means clustering is due to lloyd and macqueen
kohonen introduced learning vector quantization
the tangent distance method is due to simard et al
hastie and tibshirani proposed the discriminant adaptive nearest neighbor technique
exercises ex
consider gaussian mixture model where the covariance matrices are assumed to be scalar and is fixed parameter
discuss the analogy between the means clustering algorithm and the em algorithm for fitting this mixture model in detail
show that in the limit the two methods coincide
derive formula for the median radius of the nearestneighborhood
let be the error rate of the bayes rule in class problem where the true class probabilities are given by pk
assuming the test point and training point have identical features prove pk pk pk pk
where arg maxk pk
hence argue that the error rate of the nearest neighbor rule converges in as the size of the training set increases to value bounded above by
this statement of the theorem of cover and hart is taken from chapter of ripley where short proof is also given
prototypes and nearest neighbors ex
consider an image to be function ir ir over the twodimensional spatial domain paper coordinates
then represents an affine transformation of the image where is matrix
decompose via in such way that parameters identifying the four affine transformations two scale shear and rotation are clearly identified
using the chain rule show that the derivative of each of these parameters can be represented in terms of the two spatial derivatives of
using two dimensional kernel smoother chapter describe how to implement this procedure when the images are quantized to pixels be square positive semi definite maex
let bi
trices pp and let bi
write the eigen decomposition of as with
show that the best rank approximation for the bi min trace bi rank pl pn is given by
hint write trace bi as trace bi trace ex
here we consider the problem of shape averaging
in particular li are each matrices of points in ir each sampled from corresponding positions of handwritten cursive letters
we seek an affine invariant average also vt of the letters li with the following property minimizes min klj vaj
aj characterize the solution
this solution can suffer if some of the letters are big and dominate the average
an alternative approach is to minimize instead min lj
aj derive the solution to this problem
how do the criteria differ
use the svd of the lj to simplify the comparison of the two approaches
exercises ex
consider the application of nearest neighbors to the easy and hard problems in the left panel of figure
replicate the results in the left panel of figure
estimate the misclassification errors using fivefold cross validation and compare the error rate curves to those in
consider an aic like penalization of the training set misclassification error
specifically add to the training set misclassification error where is the approximate number of parameters being the number of nearest neighbors
compare plots of the resulting penalized misclassification error to those in and
which method gives better estimate of the optimal number of nearest neighbors cross validation or aic
generate data in two classes with two features
these features are all independent gaussian variates with standard deviation
their mean vectors are in class and in class
to each feature vector apply random rotation of angle chosen uniformly from to
generate observations from each class to form the training set and in each class as the test set
apply four different classifiers
nearest neighbors
nearest neighbors with hints ten randomly rotated versions of each data point are added to the training set before applying nearestneighbors
invariant metric nearest neighbors using euclidean distance invariant to rotations about the origin
tangent distance nearest neighbors
in each case choose the number of neighbors by tenfold cross validation
compare the results
prototypes and nearest neighbors
this is page printer opaque this unsupervised learning introduction the previous chapters have been concerned with predicting the values of one or more outputs or response variables ym for given set of input or predictor variables xp
denote by xti xi xip the inputs for the ith training case and let yi be response measurement
the predictions are based on the training sample xn yn of previously solved cases where the joint values of all of the variables are known
this is called supervised learning or learning with teacher
under this metaphor the student presents an answer for each xi in the training sample and the supervisor or teacher provides either the correct answer and or an error associated with the student's answer
this is usually characterized by some loss function for example
if one supposes that are random variables represented by some joint probability density pr then supervised learning can be formally characterized as density estimation problem where one is concerned with determining properties of the conditional density pr
usually the properties of interest are the location parameters that minimize the expected error at each argmin ey

unsupervised learning conditioning one has pr pr pr where pr is the joint marginal density of the values alone
in supervised learning pr is typically of no direct concern
one is interested mainly in the properties of the conditional density pr
since is often of low dimension usually one and only its location is of interest the problem is greatly simplified
as discussed in the previous chapters there are many approaches for successfully addressing supervised learning in variety of contexts
in this chapter we address unsupervised learning or learning without teacher
in this case one has set of observations xn of random vector having joint density pr
the goal is to directly infer the properties of this probability density without the help of supervisor or teacher providing correct answers or degree of error for each observation
the dimension of is sometimes much higher than in supervised learning and the properties of interest are often more complicated than simple location estimates
these factors are somewhat mitigated by the fact that represents all of the variables under consideration one is not required to infer how the properties of pr change conditioned on the changing values of another set of variables
in low dimensional problems say there are variety of effective nonparametric methods for directly estimating the density pr itself at all values and representing it graphically silverman
owing to the curse of dimensionality these methods fail in high dimensions
one must settle for estimating rather crude global models such as gaussian mixtures or various simple descriptive statistics that characterize pr
generally these descriptive statistics attempt to characterize values or collections of such values where pr is relatively large
principal components multidimensional scaling self organizing maps and principal curves for example attempt to identify low dimensional manifolds within the space that represent high data density
this provides information about the associations among the variables and whether or not they can be considered as functions of smaller set of latent variables
cluster analysis attempts to find multiple convex regions of the space that contain modes of pr
this can tell whether or not pr can be represented by mixture of simpler densities representing distinct types or classes of observations
mixture modeling has similar goal
association rules attempt to construct simple descriptions conjunctive rules that describe regions of high density in the special case of very high dimensional binary valued data
with supervised learning there is clear measure of success or lack thereof that can be used to judge adequacy in particular situations and to compare the effectiveness of different methods over various situations
association rules lack of success is directly measured by expected loss over the joint distribution pr
this can be estimated in variety of ways including cross validation
in the context of unsupervised learning there is no such direct measure of success
it is difficult to ascertain the validity of inferences drawn from the output of most unsupervised learning algorithms
one must resort to heuristic arguments not only for motivating the algorithms as is often the case in supervised learning as well but also for judgments as to the quality of the results
this uncomfortable situation has led to heavy proliferation of proposed methods since effectiveness is matter of opinion and cannot be verified directly
in this chapter we present those unsupervised learning techniques that are among the most commonly used in practice and additionally few others that are favored by the authors
association rules association rule analysis has emerged as popular tool for mining commercial data bases
the goal is to find joint values of the variables xp that appear most frequently in the data base
it is most often applied to binary valued data xj where it is referred to as market basket analysis
in this context the observations are sales transactions such as those occurring at the checkout counter of store
the variables represent all of the items sold in the store
for observation each variable xj is assigned one of two values xij if the jth item is purchased as part of the transaction whereas xij if it was not purchased
those variables that frequently have joint values of one represent items that are frequently purchased together
this information can be quite useful for stocking shelves cross marketing in sales promotions catalog design and consumer segmentation based on buying patterns
more generally the basic goal of association rule analysis is to find collection of prototype values vl for the feature vector such that the probability density pr vl evaluated at each of those values is relatively large
in this general framework the problem can be viewed as mode finding or bump hunting
as formulated this problem is impossibly difficult
natural estimator for each pr vl is the fraction of observations for which vl
for problems that involve more than small number of variables each of which can assume more than small number of values the number of observations for which vl will nearly always be too small for reliable estimation
in order to have tractable problem both the goals of the analysis and the generality of the data to which it is applied must be greatly simplified
the first simplification modifies the goal
instead of seeking values where pr is large one seeks regions of the space with high probability
unsupervised learning content relative to their size or support
let sj represent the set of all possible values of the jth variable its support and let sj sj be subset of these values
the modified goal can be stated as attempting to find subsets of variable values sp such that the probability of each of the variables simultaneously assuming value within its respective subset ee pr xj sj fb is relatively large
the intersection of subsets pj xj sj is called conjunctive rule
for quantitative variables the subsets sj are contiguous intervals for categorical variables the subsets are delineated explicitly
note that if the subset sj is in fact the entire set of values sj sj as is often the case the variable xj is said not to appear in the rule
market basket analysis general approaches to solving are discussed in section
these can be quite useful in many applications
however they are not feasible for the very large commercial data bases to which market basket analysis is often applied
several further simplifications of are required
first only two types of subsets are considered either sj consists of single value of xj sj or it consists of the entire set of values that xj can assume sj sj
this simplifies the problem to finding subsets of the integers and corresponding values such that ee pr xj fb is large
figure illustrates this assumption
one can apply the technique of dummy variables to turn into problem involving only binary valued variables
here we assume that the support sj is finite for each variable xj
specifically new set of variables zk is created one such variable for each of the values vlj attainable by each of the original variables xp
the number of dummy variables is xp sj where sj is the number of distinct values attainable by xj
each dummy variable is assigned the value zk if the variable with which it is associated takes on the corresponding value to which zk is assigned and zk otherwise
this transforms to finding subset of the integers such that
simplifications for association rules
here there are two inputs and taking four and six distinct values respectively
the red squares indicate areas of high density
to simplify the computations we assume that the derived subset corresponds to either single value of an input or all values
with this assumption we could find either the middle or right pattern but not the left one
pr zk pr zk is large
this is the standard formulation of the market basket problem
the set is called an item set
the number of variables zk in the item set is called its size note that the size is no bigger than
the estimated value of is taken to be the fraction of observations in the data base for which the conjunction in is true xy pr zk zik
here zik is the value of zk for this ith case
this is called the support or prevalence of the item set
an observation for which zik is said to contain the item set
in association rule mining lower support bound is specified and one seeks all item sets kl that can be formed from the variables zk with support in the data base greater than this lower bound kl kl
the apriori algorithm the solution to this problem can be obtained with feasible computation for very large data bases provided the threshold is adjusted so that consists of only small fraction of all possible item sets
the apriori algorithm agrawal et al exploits several aspects of the
unsupervised learning curse of dimensionality to solve with small number of passes over the data
the first pass over the data computes the support of all single item sets
those whose support is less than the threshold are discarded
the second pass computes the support of all item sets of size two that can be formed from pairs of the single items surviving the first pass
in other words to generate all frequent itemsets with we need to consider only candidates such that all of their ancestral item sets of size are frequent
those size two item sets with support less than the threshold are discarded
each successive pass over the data considers only those item sets that can be formed by combining those that survived the previous pass with those retained from the first pass
passes over the data continue until all candidate rules from the previous pass have support less than the specified threshold
the apriori algorithm requires only one pass over the data for each value of which is crucial since we assume the data cannot be fitted into computer's main memory
if the data are sufficiently sparse or if the threshold is high enough then the process will terminate in reasonable time even for huge data sets
there are many additional tricks that can be used as part of this strategy to increase speed and convergence agrawal et al
the apriori algorithm represents one of the major advances in data mining technology
each high support item set returned by the apriori algorithm is cast into set of association rules
the items zk are partitioned into two disjoint subsets and written
the first item subset is called the antecedent and the second the consequent
association rules are defined to have several properties based on the prevalence of the antecedent and consequent item sets in the data base
the support of the rule is the fraction of observations in the union of the antecedent and consequent which is just the support of the item set from which they were derived
it can be viewed as an estimate of the probability of simultaneously observing both item sets pr and in randomly selected market basket
the confidence or predictability of the rule is its support divided by the support of the antecedent which can be viewed as an estimate of pr
the notation pr the probability of an item set occurring in basket is an abbreviation for
association rules pr zk
the expected confidence is defined as the support of the consequent which is an estimate of the unconditional probability pr
finally the lift of the rule is defined as the confidence divided by the expected confidence
this is an estimate of the association measure pr and pr pr
as an example suppose the item set peanut butter jelly bread and consider the rule peanut butter jelly bread
support value of for this rule means that peanut butter jelly and bread appeared together in of the market baskets
confidence of for this rule implies that when peanut butter and jelly were purchased of the time bread was also purchased
if bread appeared in of all market baskets then the rule peanut butter jelly bread would have lift of
the goal of this analysis is to produce association rules with both high values of support and confidence
the apriori algorithm returns all item sets with high support as defined by the support threshold
confidence threshold is set and all rules that can be formed from those item sets with confidence greater than this value are reported
for each item set of size there are rules of the form
agrawal et al present variant of the apriori algorithm that can rapidly determine which rules survive the confidence threshold from all possible rules that can be formed from the solution item sets
the output of the entire analysis is collection of association rules that satisfy the constraints and
these are generally stored in data base that can be queried by the user
typical requests might be to display the rules in sorted order of confidence lift or support
more specifically one might request such list conditioned on particular items in the antecedent or especially the consequent
for example request might be the following display all transactions in which ice skates are the consequent that have confidence over and support of more than
this could provide information on those items antecedent that predicate sales of ice skates
focusing on particular consequent casts the problem into the framework of supervised learning
association rules have become popular tool for analyzing very large commercial data bases in settings where market basket is relevant
that is
unsupervised learning when the data can be cast in the form of multidimensional contingency table
the output is in the form of conjunctive rules that are easily understood and interpreted
the apriori algorithm allows this analysis to be applied to huge data bases much larger that are amenable to other types of analyses
association rules are among data mining's biggest successes
besides the restrictive form of the data to which they can be applied association rules have other limitations
critical to computational feasibility is the support threshold
the number of solution item sets their size and the number of passes required over the data can grow exponentially with decreasing size of this lower bound
thus rules with high confidence or lift but low support will not be discovered
for example high confidence rule such as vodka caviar will not be uncovered owing to the low sales volume of the consequent caviar
example market basket analysis we illustrate the use of apriori on moderately sized demographics data base
this data set consists of questionnaires filled out by shopping mall customers in the san francisco bay area impact resources inc columbus oh
here we use answers to the first questions relating to demographics for illustration
these questions are listed in table
the data are seen to consist of mixture of ordinal and unordered categorical variables many of the latter having more than few values
there are many missing values
we used freeware implementation of the apriori algorithm due to christian borgelt
after removing observations with missing values each ordinal predictor was cut at its median and coded by two dummy variables each categorical predictor with categories was coded by dummy variables
this resulted in matrix of observations on dummy variables
the algorithm found total of association rules involving predictors with support of at least
understanding this large set of rules is itself challenging data analysis task
we will not attempt this here but only illustrate in figure the relative frequency of each dummy variable in the data top and the association rules bottom
prevalent categories tend to appear more often in the rules for example the first category in language english
however others such as occupation are under represented with the exception of the first and fifth level
here are three examples of association rules found by the apriori algorithm association rule support confidence and lift
see http fuzzy cs uni magdeburg de borgelt
market basket analysis relative frequency of each dummy varirelative frequency in association rules relative frequency in data income income sex sex marstat marstat age age educ educ occup occup yrs bay yrs bay attribute attribute dualinc dualinc perhous perhous association rules peryoung peryoung house house typehome typehome ethnic ethnic language language
unsupervised learning table
inputs for the demographic data
feature demographic values type sex categorical marital status categorical age ordinal education ordinal occupation categorical income ordinal years in bay area ordinal dual incomes categorical number in household ordinal number of children ordinal householder status categorical type of home categorical ethnic classification categorical language in home categorical number in household number of children language in home english association rule support confidence and lift
ee language in home english householder status own fb occupation professional managerial income association rule support confidence and lift
ee language in home english ef income fa ef fa marital status not married fb number of children college graduate graduate study education
association rules we chose the first and third rules based on their high support
the second rule is an association rule with high income consequent and could be used to try to target high income individuals
as stated above we created dummy variables for each category of the input predictors for example income and income for below and above the median income
if we were interested only in finding associations with the high income category we would include but not
this is often the case in actual market basket problems where we are interested in finding associations with the presence of relatively rare item but not associations with its absence
unsupervised as supervised learning here we discuss technique for transforming the density estimation problem into one of supervised function approximation
this forms the basis for the generalized association rules described in the next section
let be the unknown data probability density to be estimated and be specified probability density function used for reference
for example might be the uniform density over the range of the variables
other possibilities are discussed below
the data set xn is presumed to be an random sample drawn from
sample of size can be drawn from using monte carlo methods
pooling these two data sets and assigning mass to those drawn from and to those drawn from results in random sample drawn from the mixture density
if one assigns the value to each sample point drawn from and those drawn from then can be estimated by supervised learning using the combined sample yn xn as training data
the resulting estimate can be inverted to provide an estimate for
generalized versions of logistic regression section are especially well suited for this application since the log odds log are estimated directly
in this case one has
density estimation via classification
left panel training set of data points
right panel training set plus reference data points generated uniformly over the rectangle containing the training data
the training sample was labeled as class and the reference sample class and semiparametric logistic regression model was fit to the data
some contours for are shown
ef
an example is shown in figure
we generated training set of size shown in the left panel
the right panel shows the reference data blue generated uniformly over the rectangle containing the training data
the training sample was labeled as class and the reference sample class and logistic regression model using tensor product of natural splines section was fit to the data
some probability contours of are shown in the right panel these are also the contours of the density estimate since is monotone function
the contours roughly capture the data density
in principle any reference density can be used for in
in practice the accuracy of the estimate can depend greatly on particular choices
good choices will depend on the data density and the procedure used to estimate or
if accuracy is the goal should be chosen so that the resulting functions or are approximated easily by the method being used
however accuracy is not always the primary goal
both and are monotonic functions of the density ratio
they can thus be viewed as contrast statistics that provide information concerning departures of the data density from the chosen reference density
therefore in data analytic settings choice for is dictated by types of departures that are deemed most interesting in the context of the specific problem at hand
for example if departures from uniformity are of interest might be the uniform density over the range of the variables
if departures from joint normality
association rules are of interest good choice for would be gaussian distribution with the same mean vector and covariance matrix as the data
departures from independence could be investigated by using gj xj where gj xj is the marginal data density of xj the jth coordinate of
sample from this independent density is easily generated from the data itself by applying different random permutation to the data values of each of the variables
as discussed above unsupervised learning is concerned with revealing properties of the data density
each technique focuses on particular property or set of properties
although this approach of transforming the problem to one of supervised learning seems to have been part of the statistics folklore for some time it does not appear to have had much impact despite its potential to bring well developed supervised learning methodology to bear on unsupervised learning problems
one reason may be that the problem must be enlarged with simulated data set generated by monte carlo techniques
since the size of this data set should be at least as large as the data sample the computation and memory requirements of the estimation procedure are at least doubled
also substantial computation may be required to generate the monte carlo sample itself
although perhaps deterrent in the past these increased computational requirements are becoming much less of burden as increased resources become routinely available
we illustrate the use of supervising learning methods for unsupervised learning in the next section
generalized association rules the more general problem of finding high density regions in the data space can be addressed using the supervised learning approach described above
although not applicable to the huge data bases for which market basket analysis is feasible useful information can be obtained from moderately sized data sets
the problem can be formulated as finding subsets of the integers and corresponding value subsets sj for the corresponding variables xj such that eb eb xn ed pr xj sj ed xij sj is large
following the nomenclature of association rule analysis xj sj will be called generalized item set
the subsets sj corresponding to quantitative variables are taken to be contiguous intervals within
unsupervised learning their range of values and subsets for categorical variables can involve more than single value
the ambitious nature of this formulation precludes thorough search for all generalized item sets with support greater than specified minimum threshold as was possible in the more restrictive setting of market basket analysis
heuristic search methods must be employed and the most one can hope for is to find useful collection of such generalized item sets
both market basket analysis and the generalized formulation implicitly reference the uniform probability distribution
one seeks item sets that are more frequent than would be expected if all joint data values xn were uniformly distributed
this favors the discovery of item sets whose marginal constituents xj sj are individually frequent that is the quantity xij sj is large
conjunctions of frequent subsets will tend to appear more often among item sets of high support than conjunctions of marginally less frequent subsets
this is why the rule vodka caviar is not likely to be discovered in spite of high association lift neither item has high marginal support so that their joint support is especially small
reference to the uniform distribution can cause highly frequent item sets with low associations among their constituents to dominate the collection of highest support item sets
highly frequent subsets sj are formed as disjunctions of the most frequent xj values
using the product of the variable marginal data densities as reference distribution removes the preference for highly frequent values of the individual variables in the discovered item sets
this is because the density ratio is uniform if there are no associations among the variables complete independence regardless of the frequency distribution of the individual variable values
rules like vodka caviar would have chance to emerge
it is not clear however how to incorporate reference distributions other than the uniform into the apriori algorithm
as explained in section it is straightforward to generate sample from the product density given the original data set
after choosing reference distribution and drawing sample from it as in one has supervised learning problem with binary valued output variable
the goal is to use this training data to find regions xj sj for which the target function is relatively large
in addition one might wish to require that the data support of these regions
association rules dx not be too small
choice of supervised learning method the regions are defined by conjunctive rules
hence supervised methods that learn such rules would be most appropriate in this context
the terminal nodes of cart decision tree are defined by rules precisely of the form
applying cart to the pooled data will produce decision tree that attempts to model the target over the entire data space by disjoint set of regions terminal nodes
each region is defined by rule of the form
those terminal nodes with high average values ave yi xi are candidates for high support generalized item sets
the actual data support is given by nt where nt is the number of pooled observations within the region represented by the terminal node
by examining the resulting decision tree one might discover interesting generalized item sets of relatively high support
these can then be partitioned into antecedents and consequents in search for generalized association rules of high confidence and or lift
another natural learning method for this purpose is the patient rule induction method prim described in section
prim also produces rules precisely of the form but it is especially designed for finding high support regions that maximize the average target value within them rather than trying to model the target function over the entire data space
it also provides more control over the support average target value tradeoff
exercise addresses an issue that arises with either of these methods when we generate random data from the product of the marginal distributions
example market basket analysis continued we illustrate the use of prim on the demographics data of table
three of the high support generalized item sets emerging from the prim analysis were the following item set support
unsupervised learning ee marital status married householder status own fb type of home apartment item set support
ee age ef marital status living together not married single fa ef fa occupation professional homemaker retired fb householder status rent live with family item set support
ee householder status rent ef type of home house fa ef fa ef number in household fa ef fa ef number of children fa ef fa occupation homemaker student unemployed fb income generalized association rules derived from these item sets with confidence greater than are the following association rule support confidence and lift
marital status married householder status own type of home apartment association rule support confidence and lift
ee age occupation professional homemaker retired fb householder status rent live with family marital status single living together not married association rule support confidence and lift
householder status own type of home apartment marital status married
cluster analysis association rule support confidence and lift
ee householder status rent ef type of home house fa ef fa ef number in household fa ef fa occupation homemaker student unemployed fb income number of children there are no great surprises among these particular rules
for the most part they verify intuition
in other contexts where there is less prior information available unexpected results have greater chance to emerge
these results do illustrate the type of information generalized association rules can provide and that the supervised learning approach coupled with ruled induction method such as cart or prim can uncover item sets exhibiting high associations among their constituents
how do these generalized association rules compare to those found earlier by the apriori algorithm
since the apriori procedure gives thousands of rules it is difficult to compare them
however some general points can be made
the apriori algorithm is exhaustive it finds all rules with support greater than specified amount
in contrast prim is greedy algorithm and is not guaranteed to give an optimal set of rules
on the other hand the apriori algorithm can deal only with dummy variables and hence could not find some of the above rules
for example since type of home is categorical input with dummy variable for each level apriori could not find rule involving the set type of home apartment
to find this set we would have to code dummy variable for apartment versus the other categories of type of home
it will not generally be feasible to precode all such potentially interesting comparisons
cluster analysis cluster analysis also called data segmentation has variety of goals
all relate to grouping or segmenting collection of objects into subsets or clusters such that those within each cluster are more closely related to one another than objects assigned to different clusters
an object can be described by set of measurements or by its relation to other objects
in addition the goal is sometimes to arrange the clusters into natural hierarchy
this involves successively grouping the clusters themselves so
simulated data in the plane clustered into three classes represented by orange blue and green by the means clustering algorithm that at each level of the hierarchy clusters within the same group are more similar to each other than those in different groups
cluster analysis is also used to form descriptive statistics to ascertain whether or not the data consists of set distinct subgroups each group representing objects with substantially different properties
this latter goal requires an assessment of the degree of difference between the objects assigned to the respective clusters
central to all of the goals of cluster analysis is the notion of the degree of similarity or dissimilarity between the individual objects being clustered
clustering method attempts to group the objects based on the definition of similarity supplied to it
this can only come from subject matter considerations
the situation is somewhat similar to the specification of loss or cost function in prediction problems supervised learning
there the cost associated with an inaccurate prediction depends on considerations outside the data
figure shows some simulated data clustered into three groups via the popular means algorithm
in this case two of the clusters are not well separated so that segmentation more accurately describes the part of this process than clustering
means clustering starts with guesses for the three cluster centers
we describe means clustering in more detail later including the problem of how to choose the number of clusters three in this example
kmeans clustering is top down procedure while other cluster approaches that we discuss are bottom up
fundamental to all clustering techniques is the choice of distance or dissimilarity measure between two objects
we first discuss distance measures before describing variety of algorithms for clustering
proximity matrices sometimes the data is represented directly in terms of the proximity alikeness or affinity between pairs of objects
these can be either similarities or dissimilarities difference or lack of affinity
for example in social science experiments participants are asked to judge by how much certain objects differ from one another
dissimilarities can then be computed by averaging over the collection of such judgments
this type of data can be represented by an matrix where is the number of objects and each element dii records the proximity between the ith and th objects
this matrix is then provided as input to the clustering algorithm
most algorithms presume matrix of dissimilarities with nonnegative entries and zero diagonal elements dii
if the original data were collected as similarities suitable monotone decreasing function can be used to convert them to dissimilarities
also most algorithms assume symmetric dissimilarity matrices so if the original matrix is not symmetric it must be replaced by dt
subjectively judged dissimilarities are seldom distances in the strict sense since the triangle inequality dii dik di for all does not hold
thus some algorithms that assume distances cannot be used with such data
dissimilarities based on attributes most often we have measurements xij for on variables also called attributes
since most of the popular clustering algorithms take dissimilarity matrix as their input we must first construct pairwise dissimilarities between the observations
in the most common case we define dissimilarity dj xij xi between values of the jth attribute and then define xi xi dj xij xi as the dissimilarity between objects and
by far the most common choice is squared distance
unsupervised learning dj xij xi xij xi
however other choices are possible and can lead to potentially different results
for nonquantitative attributes categorical data squared distance may not be appropriate
in addition it is sometimes desirable to weigh attributes differently rather than giving them equal weight as in
we first discuss alternatives in terms of the attribute type quantitative variables
measurements of this type of variable or attribute are represented by continuous real valued numbers
it is natural to define the error between them as monotone increasing function of their absolute difference xi xi xi xi
besides squared error loss xi xi common choice is the identity absolute error
the former places more emphasis on larger differences than smaller ones
alternatively clustering can be based on the correlation xij xi xi xi qp ij with xij
note that this is averaged over variables pnot observations
if the observations are first standardized then xij xi xi xi
hence clustering based on correlation similarity is equivalent to that based on squared distance dissimilarity
ordinal variables
the values of this type of variable are often represented as contiguous integers and the realizable values are considered to be an ordered set
examples are academic grades degree of preference can't stand dislike ok like terrific
rank data are special kind of ordinal data
error measures for ordinal variables are generally defined by replacing their original values with in the prescribed order of their original values
they are then treated as quantitative variables on this scale
categorical variables
with unordered categorical also called nominal variables the degree of difference between pairs of values must be delineated explicitly
if the variable assumes distinct values these can be arranged in symmetric matrix with elements lrr lr lrr lrr
the most common choice is lrr for all while unequal losses can be used to emphasize some errors more than others
cluster analysis object dissimilarity next we define procedure for combining the individual attribute dissimilarities dj xij xi into single overall measure of dissimilarity xi xi between two objects or observations xi xi possessing the respective attribute values
this is nearly always done by means of weighted average convex combination xi xi wj dj xij xi wj
here wj is weight assigned to the jth attribute regulating the relative influence of that variable in determining the overall dissimilarity between objects
this choice should be based on subject matter considerations
it is important to realize that setting the weight wj to the same value for each variable say wj does not necessarily give all attributes equal influence
the influence of the jth attribute xj on object dissimilarity xi xi depends upon its relative contribution to the average object dissimilarity measure over all pairs of observations in the data set xx xi xi wj af with xx af dj xij xi being the average dissimilarity on the jth attribute
thus the relative influence of the jth variable is wj af and setting wj af would give all attributes equal influence in characterizing overall dissimilarity between objects
for example with quantitative variables and squared error distance used for each coordinate then becomes the weighted squared euclidean distance di xi xi wj xij xi between pairs of points in an irp with the quantitative variables as axes
in this case becomes xx af xij xi varj where varj is the sample estimate of var xj
thus the relative importance of each such variable is proportional to its variance over the data
simulated data on the left means clustering with has been applied to the raw data
the two colors indicate the cluster memberships
on the right the features were first standardized before clustering
this is equivalent to using feature weights var xj
the standardization has obscured the two well separated groups
note that each plot uses the same units in the horizontal and vertical axes set
in general setting wj af for all attributes irrespective of type will cause each one of them to equally influence the overall dissimilarity between pairs of objects xi xi
although this may seem reasonable and is often recommended it can be highly counterproductive
if the goal is to segment the data into groups of similar objects all attributes may not contribute equally to the problem dependent notion of dissimilarity between objects
some attribute value differences may reflect greater actual object dissimilarity in the context of the problem domain
if the goal is to discover natural groupings in the data some attributes may exhibit more of grouping tendency than others
variables that are more relevant in separating the groups should be assigned higher influence in defining object dissimilarity
giving all attributes equal influence in this case will tend to obscure the groups to the point where clustering algorithm cannot uncover them
figure shows an example
although simple generic prescriptions for choosing the individual attribute dissimilarities dj xij xi and their weights wj can be comforting there is no substitute for careful thought in the context of each individual problem
specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than choice of clustering algorithm
this aspect of the problem is emphasized less in the clustering literature than the algorithms themselves since it depends on domain knowledge specifics and is less amenable to general research
cluster analysis finally often observations have missing values in one or more of the attributes
the most common method of incorporating missing values in dissimilarity calculations is to omit each observation pair xij xi having at least one value missing when computing the dissimilarity between observations xi and
this method can fail in the circumstance when both observations have no measured values in common
in this case both observations could be deleted from the analysis
alternatively the missing values could be imputed using the mean or median of each attribute over the nonmissing data
for categorical variables one could consider the value missing as just another categorical value if it were reasonable to consider two objects as being similar if they both have missing values on the same variables
clustering algorithms the goal of cluster analysis is to partition the observations into groups clusters so that the pairwise dissimilarities between those assigned to the same cluster tend to be smaller than those in different clusters
clustering algorithms fall into three distinct types combinatorial algorithms mixture modeling and mode seeking
combinatorial algorithms work directly on the observed data with no direct reference to an underlying probability model
mixture modeling supposes that the data is an sample from some population described by probability density function
this density function is characterized by parameterized model taken to be mixture of component density functions each component density describes one of the clusters
this model is then fit to the data by maximum likelihood or corresponding bayesian approaches
mode seekers bump hunters take nonparametric perspective attempting to directly estimate distinct modes of the probability density function
observations closest to each respective mode then define the individual clusters
mixture modeling is described in section
the prim algorithm discussed in sections and is an example of mode seeking or bump hunting
we discuss combinatorial algorithms next
combinatorial algorithms the most popular clustering algorithms directly assign each observation to group or cluster without regard to probability model describing the data
each observation is uniquely labeled by an integer
prespecified number of clusters is postulated and each one is labeled by an integer
each observation is assigned to one and only one cluster
these assignments can be characterized by manyto one mapping or encoder that assigns the ith observation to the kth cluster
one seeks the particular encoder that achieves the
unsupervised learning required goal details below based on the dissimilarities xi xi between every pair of observations
these are specified by the user as described above
generally the encoder is explicitly delineated by giving its value cluster assignment for each observation
thus the parameters of the procedure are the individual cluster assignments for each of the observations
these are adjusted so as to minimize loss function that characterizes the degree to which the clustering goal is not met
one approach is to directly specify mathematical loss function and attempt to minimize it through some combinatorial optimization algorithm
since the goal is to assign close points to the same cluster natural loss or energy function would be xi xi
this criterion characterizes the extent to which observations assigned to the same cluster tend to be close to one another
it is sometimes referred to as the within cluster point scatter since eb xn xk ed dii dii dii or where dii xi xi
here is the total point scatter which is constant given the data independent of cluster assignment
the quantity dii is the between cluster point scatter
this will tend to be large when observations assigned to different clusters are far apart
thus one has and minimizing is equivalent to maximizing
cluster analysis by combinatorial optimization is straightforward in principle
one simply minimizes or equivalently maximizes over all possible assignments of the data points to clusters
unfortunately such optimization by complete enumeration is feasible only for very small data sets
the number of distinct assignments is jain and dubes kn
for example which is quite feasible
but grows very rapidly with increasing values of its arguments
already
cluster analysis and most clustering problems involve much larger data sets than
for this reason practical clustering algorithms are able to examine only very small fraction of all possible encoders
the goal is to identify small subset that is likely to contain the optimal one or at least good suboptimal partition
such feasible strategies are based on iterative greedy descent
an initial partition is specified
at each iterative step the cluster assignments are changed in such way that the value of the criterion is improved from its previous value
clustering algorithms of this type differ in their prescriptions for modifying the cluster assignments at each iteration
when the prescription is unable to provide an improvement the algorithm terminates with the current assignments as its solution
since the assignment of observations to clusters at any iteration is perturbation of that for the previous iteration only very small fraction of all possible assignments are examined
however these algorithms converge to local optima which may be highly suboptimal when compared to the global optimum
means the means algorithm is one of the most popular iterative descent clustering methods
it is intended for situations in which all variables are of the quantitative type and squared euclidean distance xi xi xij xi xi xi is chosen as the dissimilarity measure
note that weighted euclidean distance can be used by redefining the xij values exercise
the within point scatter can be written as xi xi nk xi where pk is the mean vector associated with the kth cluspn ter and nk
thus the criterion is minimized by assigning the observations to the clusters in such way that within each cluster the average dissimilarity of the observations from the cluster mean as defined by the points in that cluster is minimized
an iterative descent algorithm for solving
unsupervised learning algorithm means clustering
for given cluster assignment the total cluster variance is minimized with respect to mk yielding the means of the currently assigned clusters
given current set of means mk is minimized by assigning each observation to the closest current cluster mean
that is argmin xi mk

steps and are iterated until the assignments do not change
min nk xi can be obtained by noting that for any set of observations argmin xi
hence we can obtain by solving the enlarged optimization problem min nk xi mk
mk this can be minimized by an alternating optimization procedure given in algorithm
each of steps and reduces the value of the criterion so that convergence is assured
however the result may represent suboptimal local minimum
the algorithm of hartigan and wong goes further and ensures that there is no single switch of an observation from one group to another group that will decrease the objective
in addition one should start the algorithm with many different random choices for the starting means and choose the solution having smallest value of the objective function
figure shows some of the means iterations for the simulated data of figure
the centroids are depicted by
the straight lines show the partitioning of points each sector being the set of points closest to each centroid
this partitioning is called the voronoi tessellation
after iterations the procedure has converged
gaussian mixtures as soft means clustering the means clustering procedure is closely related to the em algorithm for estimating certain gaussian mixture model
sections and
successive iterations of the means clustering algorithm for the simulated data of figure
left panels two gaussian densities and blue and orange on the real line and single data point green dot at
the colored squares are plotted at and the means of each density
right panels the relative densities and called the responsibilities of each cluster for this data point
in the top panels the gaussian standard deviation in the bottom panels
the em algorithm uses these responsibilities to make soft assignment of each data point to each of the two clusters
when is fairly large the responsibilities can be near they are and in the top right panel
as the responsibilities for the cluster center closest to the target point and for all other clusters
this hard assignment is seen in the bottom right panel
the step of the em algorithm assigns responsibilities for each data point based in its relative density under each mixture component while the step recomputes the component density parameters based on the current responsibilities
suppose we specify mixture components each with gaussian density having scalar covariance matrix
then the relative density under each mixture component is monotone function of the euclidean distance between the data point and the mixture center
hence in this setup em is soft version of means clustering making probabilistic rather than deterministic assignments of points to cluster centers
as the variance these probabilities become and and the two methods coincide
details are given in exercise
figure illustrates this result for two clusters on the real line
example human tumor microarray data we apply means clustering to the human tumor microarray data described in chapter
this is an example of high dimensional clustering
total within cluster sum of squares for means clustering applied to the human tumor microarray data
human tumor data number of cancer cases of each type in each of the three clusters from means clustering
cluster breast cns colon leukemia mcf cluster melanoma nsclc ovarian prostate renal unknown the data are matrix of real numbers each representing an expression measurement for gene row and sample column
here we cluster the samples each of which is vector of length corresponding to expression values for the genes
each sample has label such as breast for breast cancer melanoma and so on we don't use these labels in the clustering but will examine posthoc which labels fall into which clusters
we applied means clustering with running from to and computed the total within sum of squares for each clustering shown in figure
typically one looks for kink in the sum of squares curve or its logarithm to locate the optimal number of clusters see section
here there is no clear indication for illustration we chose giving the three clusters shown in table
sir ronald
fisher was one of the founders of modern day statistics to whom we owe maximum likelihood sufficiency and many other fundamental concepts
the image on the left is grayscale image at bits per pixel
the center image is the result of block vq using code vectors with compression rate of bits pixel
the right image uses only four code vectors with compression rate of bits pixel we see that the procedure is successful at grouping together samples of the same cancer
in fact the two breast cancers in the second cluster were later found to be misdiagnosed and were melanomas that had metastasized
however means clustering has shortcomings in this application
for one it does not give linear ordering of objects within cluster we have simply listed them in alphabetic order above
secondly as the number of clusters is changed the cluster memberships can change in arbitrary ways
that is with say four clusters the clusters need not be nested within the three clusters above
for these reasons hierarchical clustering described later is probably preferable for this application
vector quantization the means clustering algorithm represents key tool in the apparently unrelated area of image and signal compression particularly in vector quantization or vq gersho and gray
the left image in figure is digitized photograph of famous statistician sir ronald fisher
it consists of pixels where each pixel is grayscale value ranging from to and hence requires bits of storage per pixel
the entire image occupies megabyte of storage
the center image is vq compressed version of the left panel and requires of the storage at some loss in quality
the right image is compressed even more and requires only of the storage at considerable loss in quality
the version of vq implemented here first breaks the image into small blocks in this case blocks of pixels
each of the blocks of four this example was prepared by maya gupta
cluster analysis numbers is regarded as vector in ir
means clustering algorithm also known as lloyd's algorithm in this context is run in this space
the center image uses while the right image
each of the pixel blocks or points is approximated by its closest cluster centroid known as codeword
the clustering process is called the encoding step and the collection of centroids is called the codebook
to represent the approximated image we need to supply for each block the identity of the codebook entry that approximates it
this will require log bits per block
we also need to supply the codebook itself which is real numbers typically negligible
overall the storage for the compressed image amounts to log of the original for for
this is typically expressed as rate in bits per pixel log which are and respectively
the process of constructing the approximate image from the centroids is called the decoding step
why do we expect vq to work at all
the reason is that for typical everyday images like photographs many of the blocks look the same
in this case there are many almost pure white blocks and similarly pure gray blocks of various shades
these require only one block each to represent them and then multiple pointers to that block
what we have described is known as lossy compression since our images are degraded versions of the original
the degradation or distortion is usually measured in terms of mean squared error
in this case for and for
more generally rate distortion curve would be used to assess the tradeoff
one can also perform lossless compression using block clustering and still capitalize on the repeated patterns
if you took the original image and losslessly compressed it the best you would do is bits per pixel
we claimed above that log bits were needed to identify each of the codewords in the codebook
this uses fixed length code and is inefficient if some codewords occur many more times than others in the image
using shannon coding theory we know that in general pk variable length code will do better and the rate then becomes log
the term in the numerator is the entropy of the distribution of the codewords in the image
using variable length coding our rates come down to and respectively
finally there are many generalizations of vq that have been developed for example tree structured vq finds the centroids with top down means style algorithm as alluded to in section
this allows successive refinement of the compression
further details may be found in gersho and gray
medoids as discussed above the means algorithm is appropriate when the dissimilarity measure is taken to be squared euclidean distance xi xi
unsupervised learning algorithm medoids clustering
for given cluster assignment find the observation in the cluster minimizing total distance to other points in that cluster argmin xi xi
then mk xi are the current estimates of the cluster centers
given current set of cluster centers mk minimize the total error by assigning each observation to the closest current cluster center argmin xi mk

iterate steps and until the assignments do not change
this requires all of the variables to be of the quantitative type
in addition using squared euclidean distance places the highest influence on the largest distances
this causes the procedure to lack robustness against outliers that produce very large distances
these restrictions can be removed at the expense of computation
the only part of the means algorithm that assumes squared euclidean distance is the minimization step the cluster representatives mk in are taken to be the means of the currently assigned clusters
the algorithm can be generalized for use with arbitrarily defined dissimilarities xi xi by replacing this step by an explicit optimization with respect to mk in
in the most common form centers for each cluster are restricted to be one of the observations assigned to the cluster as summarized in algorithm
this algorithm assumes attribute data but the approach can also be applied to data described only by proximity matrices section
there is no need to explicitly compute cluster centers rather we just keep track of the indices
solving for each provisional cluster requires an amount of computation proportional to the number of observations assigned to it whereas for solving the computation increases to nk
given set of cluster centers ik obtaining the new assignments argmin dii requires computation proportional to as before
thus medoids is far more computationally intensive than means
alternating between and represents particular heuristic search strategy for trying to solve
cluster analysis table
data from political science survey values are average pairwise dissimilarities of countries from questionnaire given to political science students
bel bra chi cub egy fra ind isr usa uss yug bra chi cub egy fra ind isr usa uss yug zai min diik
ik kaufman and rousseeuw propose an alternative strategy for directly solving that provisionally exchanges each center ik with an observation that is not currently center selecting the exchange that produces the greatest reduction in the value of the criterion
this is repeated until no advantageous exchanges can be found
massart et al derive branch and bound combinatorial method that finds the global minimum of that is practical only for very small data sets
example country dissimilarities this example taken from kaufman and rousseeuw comes from study in which political science students were asked to provide pairwise dissimilarity measures for countries belgium brazil chile cuba egypt france india israel united states union of soviet socialist republics yugoslavia and zaire
the average dissimilarity scores are given in table
we applied medoid clustering to these dissimilarities
note that means clustering could not be applied because we have only distances rather than raw observations
the left panel of figure shows the dissimilarities reordered and blocked according to the medoid clustering
the right panel is two dimensional multidimensional scaling plot with the medoid clusters assignments indicated by colors multidimensional scaling is discussed in section
both plots show three well separated clusters but the mds display indicates that egypt falls about halfway between two clusters
survey of country dissimilarities
left panel dissimilarities reordered and blocked according to medoid clustering
heat map is coded from most similar dark red to least similar bright red
right panel two dimensional multidimensional scaling plot with medoid clusters indicated by different colors
practical issues in order to apply means or medoids one must select the number of clusters and an initialization
the latter can be defined by specifying an initial set of centers mk or ik or an initial encoder
usually specifying the centers is more convenient
suggestions range from simple random selection to deliberate strategy based on forward stepwise assignment
at each step new center ik is chosen to minimize the criterion or given the centers ik chosen at the previous steps
this continues for steps thereby producing initial centers with which to begin the optimization algorithm
choice for the number of clusters depends on the goal
for data segmentation is usually defined as part of the problem
for example company may employ sales people and the goal is to partition customer database into segments one for each sales person such that the customers assigned to each one are as similar as possible
often however cluster analysis is used to provide descriptive statistic for ascertaining the extent to which the observations comprising the data base fall into natural distinct groupings
here the number of such groups is unknown and one requires that it as well as the groupings themselves be estimated from the data
data based methods for estimating typically examine the withincluster dissimilarity wk as function of the number of clusters
separate solutions are obtained for kmax
the corresponding values
cluster analysis wkmax generally decrease with increasing
this will be the case even when the criterion is evaluated on an independent test set since large number of cluster centers will tend to fill the feature space densely and thus will be close to all data points
thus cross validation techniques so useful for model selection in supervised learning cannot be utilized in this context
the intuition underlying the approach is that if there are actually distinct groupings of the observations as defined by the dissimilarity measure then for the clusters returned by the algorithm will each contain subset of the true underlying groups
that is the solution will not assign observations in the same naturally occurring group to different estimated clusters
to the extent that this is the case the solution criterion value will tend to decrease substantially with each successive increase in the number of specified clusters wk wk as the natural groups are successively assigned to separate clusters
for one of the estimated clusters must partition at least one of the natural groups into two subgroups
this will tend to provide smaller decrease in the criterion as is further increased
splitting natural group within which the observations are all quite close to each other reduces the criterion less than partitioning the union of two well separated groups into their proper constituents
to the extent this scenario is realized there will be sharp decrease in successive differences in criterion value wk wk at
that is wk wk wk wk
an estimate for is then obtained by identifying kink in the plot of wk as function of
as with other aspects of clustering procedures this approach is somewhat heuristic
the recently proposed gap statistic tibshirani et al compares the curve log wk to the curve obtained from data uniformly distributed over rectangle containing the data
it estimates the optimal number of clusters to be the place where the gap between the two curves is largest
essentially this is an automatic way of locating the aforementioned kink
it also works reasonably well when the data fall into single cluster and in that case will tend to estimate the optimal number of clusters to be one
this is the scenario where most other competing methods fail
figure shows the result of the gap statistic applied to simulated data of figure
the left panel shows log wk for clusters green curve and the expected value of log wk over simulations from uniform data blue curve
the right panel shows the gap curve which is the expected curve minusp the observed curve
shown also are error bars of halfwidth sk where sk is the standard deviation of log wk over the simulations
the gap curve is maximized at clusters
if is the gap curve at clusters the formal rule for estimating is argmin
left panel observed green and expected blue values of log wk for the simulated data of figure
both curves have been translated to equal zero at one cluster
right panel gap curve equal to the difference between the observed and expected values of log wk
the gap estimate is the smallest producing gap within one standard deviation of the gap at here
this gives which looks reasonable from figure
hierarchical clustering the results of applying means or medoids clustering algorithms depend on the choice for the number of clusters to be searched and starting configuration assignment
in contrast hierarchical clustering methods do not require such specifications
instead they require the user to specify measure of dissimilarity between disjoint groups of observations based on the pairwise dissimilarities among the observations in the two groups
as the name suggests they produce hierarchical representations in which the clusters at each level of the hierarchy are created by merging clusters at the next lower level
at the lowest level each cluster contains single observation
at the highest level there is only one cluster containing all of the data
strategies for hierarchical clustering divide into two basic paradigms agglomerative bottom up and divisive top down
agglomerative strategies start at the bottom and at each level recursively merge selected pair of clusters into single cluster
this produces grouping at the next higher level with one less cluster
the pair chosen for merging consist of the two groups with the smallest intergroup dissimilarity
divisive methods start at the top and at each level recursively split one of the existing clusters at
cluster analysis that level into two new clusters
the split is chosen to produce two new groups with the largest between group dissimilarity
with both paradigms there are levels in the hierarchy
each level of the hierarchy represents particular grouping of the data into disjoint clusters of observations
the entire hierarchy represents an ordered sequence of such groupings
it is up to the user to decide which level if any actually represents natural clustering in the sense that observations within each of its groups are sufficiently more similar to each other than to observations assigned to different groups at that level
the gap statistic described earlier can be used for this purpose
recursive binary splitting agglomeration can be represented by rooted binary tree
the nodes of the trees represent groups
the root node represents the entire data set
the terminal nodes each represent one of the individual observations singleton clusters
each nonterminal node parent has two daughter nodes
for divisive clustering the two daughters represent the two groups resulting from the split of the parent for agglomerative clustering the daughters represent the two groups that were merged to form the parent
all agglomerative and some divisive methods when viewed bottom up possess monotonicity property
that is the dissimilarity between merged clusters is monotone increasing with the level of the merger
thus the binary tree can be plotted so that the height of each node is proportional to the value of the intergroup dissimilarity between its two daughters
the terminal nodes representing individual observations are all plotted at zero height
this type of graphical display is called dendrogram
dendrogram provides highly interpretable complete description of the hierarchical clustering in graphical format
this is one of the main reasons for the popularity of hierarchical clustering methods
for the microarray data figure shows the dendrogram resulting from agglomerative clustering with average linkage agglomerative clustering and this example are discussed in more detail later in this chapter
cutting the dendrogram horizontally at particular height partitions the data into disjoint clusters represented by the vertical lines that intersect it
these are the clusters that would be produced by terminating the procedure when the optimal intergroup dissimilarity exceeds that threshold cut value
groups that merge at high values relative to the merger values of the subgroups contained within them lower in the tree are candidates for natural clusters
note that this may occur at several different levels indicating clustering hierarchy that is clusters nested within clusters
such dendrogram is often viewed as graphical summary of the data itself rather than description of the results of the algorithm
however such interpretations should be treated with caution
first different hierarchical methods see below as well as small changes in the data can lead to quite different dendrograms
also such summary will be valid only to the extent that the pairwise observation dissimilarities possess the hierar
dendrogram from agglomerative hierarchical clustering with mcf repro average linkage to the human tumor microarray data chical structure produced by the algorithm
hierarchical methods impose hierarchical structure whether or not such structure actually exists in the data
the extent to which the hierarchical structure produced by dendrogram actually represents the data itself can be judged by the cophenetic correlation coefficient
this is the correlation between the pairwise observation dissimilarities dii input to the algorithm and their corresponding cophenetic dissimilarities cii derived from the dendrogram
the cophenetic dissimilarity cii between two observations is the intergroup dissimilarity at which observations and are first joined together in the same cluster
the cophenetic dissimilarity is very restrictive dissimilarity measure
first the cii over the observations must contain many ties since only of the total values can be distinct
also these dissimilarities obey the ultrametric inequality cii max cik ci
cluster analysis for any three observations
as geometric example suppose the data were represented as points in euclidean coordinate system
in order for the set of interpoint distances over the data to conform to the triangles formed by all triples of points must be isosceles triangles with the unequal length no longer than the length of the two equal sides jain and dubes
therefore it is unrealistic to expect general dissimilarities over arbitrary data sets to closely resemble their corresponding cophenetic dissimilarities as calculated from dendrogram especially if there are not many tied values
thus the dendrogram should be viewed mainly as description of the clustering structure of the data as imposed by the particular algorithm employed
agglomerative clustering agglomerative clustering algorithms begin with every observation representing singleton cluster
at each of the steps the closest two least dissimilar clusters are merged into single cluster producing one less cluster at the next higher level
therefore measure of dissimilarity between two clusters groups of observations must be defined
let and represent two such groups
the dissimilarity between and is computed from the set of pairwise observation dissimilarities dii where one member of the pair is in and the other is in
single linkage sl agglomerative clustering takes the intergroup dissimilarity to be that of the closest least dissimilar pair dsl min dii
this is also often called the nearest neighbor technique
complete linkage cl agglomerative clustering furthest neighbor technique takes the intergroup dissimilarity to be that of the furthest most dissimilar pair dcl max dii
group average ga clustering uses the average dissimilarity between the groups xx dga dii ng where ng and nh are the respective number of observations in each group
although there have been many other proposals for defining intergroup dissimilarity in the context of agglomerative clustering the above three are the ones most commonly used
figure shows examples of all three
if the data dissimilarities dii exhibit strong clustering tendency with each of the clusters being compact and well separated from others then all three methods produce similar results
clusters are compact if all of the
dendrograms from agglomerative hierarchical clustering of human tumor microarray data observations within them are relatively close together small dissimilarities as compared with observations in different clusters
to the extent this is not the case results will differ
single linkage only requires that single dissimilarity dii and be small for two groups and to be considered close together irrespective of the other observation dissimilarities between the groups
it will therefore have tendency to combine at relatively low thresholds observations linked by series of close intermediate observations
this phenomenon referred to as chaining is often considered defect of the method
the clusters produced by single linkage can violate the compactness property that all observations within each cluster tend to be similar to one another based on the supplied observation dissimilarities dii
if we define the diameter dg of group of observations as the largest dissimilarity among its members dg max dii then single linkage can produce clusters with very large diameters
complete linkage represents the opposite extreme
two groups and are considered close only if all of the observations in their union are relatively similar
it will tend to produce compact clusters with small diameters
however it can produce clusters that violate the closeness property
that is observations assigned to cluster can be much
cluster analysis closer to members of other clusters than they are to some members of their own cluster
group average clustering represents compromise between the two extremes of single and complete linkage
it attempts to produce relatively compact clusters that are relatively far apart
however its results depend on the numerical scale on which the observation dissimilarities dii are measured
applying monotone strictly increasing transformation to the dii hii dii can change the result produced by
in contrast and depend only on the ordering of the dii and are thus invariant to such monotone transformations
this invariance is often used as an argument in favor of single or complete linkage over group average methods
one can argue that group average clustering has statistical consistency property violated by single and complete linkage
assume we have attribute value data xp and that each cluster is random sample from some population joint density pk
the complete data set is random sample from mixture of such densities
the group average dissimilarity dga is an estimate of pg ph dx dx where is the dissimilarity between points and in the space of attribute values
as the sample size approaches infinity dga approaches which is characteristic of the relationship between the two densities pg and ph
for single linkage dsl approaches zero as independent of pg and ph
for complete linkage dcl becomes infinite as again independent of the two densities
thus it is not clear what aspects of the population distribution are being estimated by dsl and dcl
example human cancer microarray data continued the left panel of figure shows the dendrogram resulting from average linkage agglomerative clustering of the samples columns of the microarray data
the middle and right panels show the result using complete and single linkage
average and complete linkage gave similar results while single linkage produced unbalanced groups with long thin clusters
we focus on the average linkage clustering
like means clustering hierarchical clustering is successful at clustering simple cancers together
however it has other nice features
by cutting off the dendrogram at various heights different numbers of clusters emerge and the sets of clusters are nested within one another
secondly it gives some partial ordering information about the samples
in figure we have arranged the genes rows and samples columns of the expression matrix in orderings derived from hierarchical clustering
unsupervised learning note that if we flip the orientation of the branches of dendrogram at any merge the resulting dendrogram is still consistent with the series of hierarchical clustering operations
hence to determine an ordering of the leaves we must add constraint
to produce the row ordering of figure we have used the default rule in plus at each merge the subtree with the tighter cluster is placed to the left toward the bottom in the rotated dendrogram in the figure
individual genes are the tightest clusters possible and merges involving two individual genes place them in order by their observation number
the same rule was used for the columns
many other rules are possible for example ordering by multidimensional scaling of the genes see section
the two way rearrangement of figure produces an informative picture of the genes and samples
this picture is more informative than the randomly ordered rows and columns of figure of chapter
furthermore the dendrograms themselves are useful as biologists can for example interpret the gene clusters in terms of biological processes
divisive clustering divisive clustering algorithms begin with the entire data set as single cluster and recursively divide one of the existing clusters into two daughter clusters at each iteration in top down fashion
this approach has not been studied nearly as extensively as agglomerative methods in the clustering literature
it has been explored somewhat in the engineering literature gersho and gray in the context of compression
in the clustering setting potential advantage of divisive over agglomerative methods can occur when interest is focused on partitioning the data into relatively small number of clusters
the divisive paradigm can be employed by recursively applying any of the combinatorial methods such as means section or medoids section with to perform the splits at each iteration
however such an approach would depend on the starting configuration specified at each step
in addition it would not necessarily produce splitting sequence that possesses the monotonicity property required for dendrogram representation
divisive algorithm that avoids these problems was proposed by macnaughton smith et al
it begins by placing all observations in single cluster
it then chooses that observation whose average dissimilarity from all the other observations is largest
this observation forms the first member of second cluster
at each successive step that observation in whose average distance from those in minus that for the remaining observations in is largest is transferred to
this continues until the corresponding difference in averages becomes negative
that is there are no longer any observations in that are on average closer to those in
the result is split of the original cluster into two daughter clusters
dna microarray data average linkage hierarchical clustering has been applied independently to the rows genes and columns samples determining the ordering of the rows and columns see text
the colors range from bright green negative under expressed to bright red positive over expressed
unsupervised learning the observations transferred to and those remaining in
these two clusters represent the second level of the hierarchy
each successive level is produced by applying this splitting procedure to one of the clusters at the previous level
kaufman and rousseeuw suggest choosing the cluster at each level with the largest diameter for splitting
an alternative would be to choose the one with the largest average dissimilarity among its members xx af dii
ng the recursive splitting continues until all clusters either become singletons or all members of each one have zero dissimilarity from one another
self organizing maps this method can be viewed as constrained version of means clustering in which the prototypes are encouraged to lie in oneor two dimensional manifold in the feature space
the resulting manifold is also referred to as constrained topological map since the original high dimensional observations can be mapped down onto the two dimensional coordinate system
the original som algorithm was online observations are processed one at time and later batch version was proposed
the technique also bears close relationship to principal curves and surfaces which are discussed in the next section
we consider som with two dimensional rectangular grid of prototypes mj irp other choices such as hexagonal grids can also be used
each of the prototypes are parametrized with respect to an integer coordinate pair
here similarly and
the mj are initialized for example to lie in the two dimensional principal component plane of the data next section
we can think of the prototypes as buttons sewn on the principal component plane in regular pattern
the som procedure tries to bend the plane so that the buttons approximate the data points as well as possible
once the model is fit the observations can be mapped down onto the two dimensional grid
the observations xi are processed one at time
we find the closest prototype mj to xi in euclidean distance in irp and then for all neighbors mk of mj move mk toward xi via the update mk mk xi mk
the neighbors of mj are defined to be all mk such that the distance between and is small
the simplest approach uses euclidean distance and small is determined by threshold
this neighborhood always includes the closest prototype mj itself
self organizing maps notice that distance is defined in the space of integer topological coordinates of the prototypes rather than in the feature space irp
the effect of the update is to move the prototypes closer to the data but also to maintain smooth two dimensional spatial relationship between the prototypes
the performance of the som algorithm depends on the learning rate and the distance threshold
typically is decreased from say to over few thousand iterations one per observation
similarly is decreased linearly from starting value to over few thousand iterations
we illustrate method for choosing in the example below
we have described the simplest version of the som
more sophisticated versions modify the update step according to distance mk mk xi mk where the neighborhood function gives more weight to prototypes mk with indices closer to than to those further away
if we take the distance small enough so that each neighborhood contains only one point then the spatial connection between prototypes is lost
in that case one can show that the som algorithm is an online version of means clustering and eventually stabilizes at one of the local minima found by means
since the som is constrained version of means clustering it is important to check whether the constraint is reasonable in any given problem
one can do this by computing the reconstruction error kx mj summed over observations for both methods
this will necessarily be smaller for means but should not be much smaller if the som is reasonable approximation
as an illustrative example we generated data points in three dimensions near the surface of half sphere of radius
the points were in each of three clusters red green and blue located near and
the data are shown in figure by design the red cluster was much tighter than the green or blue ones
full details of the data generation are given in exercise
grid of prototypes was used with initial grid size this meant that about third of the prototypes were initially in each neighborhood
we did total of passes through the dataset of observations and let and decrease linearly over the iterations
in figure the prototypes are indicated by circles and the points that project to each prototype are plotted randomly within the corresponding circle
the left panel shows the initial configuration while the right panel shows the final one
the algorithm has succeeded in separating the clusters however the separation of the red cluster indicates that the manifold has folded back on itself see figure
since the distances in the two dimensional display are not used there is little indication in the som projection that the red cluster is tighter than the others
simulated data in three classes near the surface of half sphere
self organizing map applied to half sphere data example
left panel is the initial configuration right panel the final one
the grid of prototypes are indicated by circles and the points that project to each prototype are plotted randomly within the corresponding circle
wiremesh representation of the fitted som model in ir
the lines represent the horizontal and vertical edges of the topological lattice
the double lines indicate that the surface was folded diagonally back on itself in order to model the red points
the cluster members have been jittered to indicate their color and the purple points are the node centers
figure shows the reconstruction error equal to the total sum of squares of each data point around its prototype
for comparison we carried out means clustering with centroids and indicate its reconstruction error by the horizontal line on the graph
we see that the som significantly decreases the error nearly to the level of the means solution
this provides evidence that the two dimensional constraint used by the som is reasonable for this particular dataset
in the batch version of the som we update each mj via wk xk mj
wk the sum is over points xk that mapped were closest to neighbors mk of mj
the weight function may be rectangular that is equal to for the neighbors of mk or may decrease smoothly with distance as before
if the neighborhood size is chosen small enough so that it consists only of mk with rectangular weights this reduces to the means clustering procedure described earlier
it can also be thought of as discrete version of principal curves and surfaces described in section
half sphere data reconstruction error for the som as function of iteration
error for means clustering is indicated by the horizontal line
example document organization and retrieval document retrieval has gained importance with the rapid development of the internet and the web and soms have proved to be useful for organizing and indexing large corpora
this example is taken from the websom homepage http websom hut fi kohonen et al
figure represents som fit to newsgroup comp ai neural nets articles
the labels are generated automatically by the websom software and provide guide as to the typical content of node
in applications such as this the documents have to be reprocessed in order to create feature vector
term document matrix is created where each row represents single document
the entries in each row are the relative frequency of each of predefined set of terms
these terms could be large set of dictionary entries words or an even larger set of bigrams word pairs or subsets of these
these matrices are typically very sparse and so often some preprocessing is done to reduce the number of features columns
sometimes the svd next section is used to reduce the matrix kohonen et al use randomized variant thereof
these reduced vectors are then the input to the som
heatmap representation of the som model fit to corpus of newsgroup comp ai neural nets contributions courtesy websom homepage
the lighter areas indicate higher density areas
populated nodes are automatically labeled according to typical content
the first linear principal component of set of data
the line minimizes the total squared distance from each point to its orthogonal projection onto the line
in this application the authors have developed zoom feature which allows one to interact with the map in order to get more detail
the final level of zooming retrieves the actual news articles which can then be read
principal components curves and surfaces principal components are discussed in sections where they shed light on the shrinkage mechanism of ridge regression
principal components are sequence of projections of the data mutually uncorrelated and ordered in variance
in the next section we present principal components as linear manifolds approximating set of points xi irp
we then present some nonlinear generalizations in section
other recent proposals for nonlinear approximating manifolds are discussed in section
principal components the principal components of set of data in irp provide sequence of best linear approximations to that data of all ranks
denote the observations by xn and consider the rank linear model for representing them
principal components curves and surfaces bb vq bb where is location vector in irp vq is matrix with orthogonal unit vectors as columns and bb is vector of parameters
this is the parametric representation of an affine hyperplane of rank
figures and illustrate for and respectively
fitting such model to the data by least squares amounts to minimizing the reconstruction error min kxi vq bb
bb vq we can partially optimize for and the bb exercise to obtain bb vqt xi
this leaves us to find the orthogonal matrix vq min xi vq vqt xi
vq for convenience we assume that otherwise we simply replace the observations by their centered versions xi
the matrix hq vq vqt is projection matrix and maps each point xi onto its rankq reconstruction hq xi the orthogonal projection of xi onto the subspace spanned by the columns of vq
the solution can be expressed as follows
stack the centered observations into the rows of an matrix
we construct the singular value decomposition of udvt
this is standard decomposition in numerical analysis and many algorithms exist for its computation golub and van loan for example
here is an orthogonal matrix ut ip whose columns uj are called the left singular vectors is orthogonal matrix vt ip with columns vj called the right singular vectors and is diagonal matrix with diagonal elements dp known as the singular values
for each rank the solution vq to consists of the first columns of
the columns of ud are called the principal components of see section
the optimal bb in are given by the first principal components the rows of the matrix uq dq
the one dimensional principal component line in ir is illustrated in figure
for each data point xi there is closest point on the line given by ui
here is the direction of the line and bb ui measures distance along the line from the origin
similarly figure shows the
the best rank two linear approximation to the half sphere data
the right panel shows the projected points with coordinates given by the first two principal components of the data two dimensional principal component surface fit to the half sphere data left panel
the right panel shows the projection of the data onto the first two principal components
this projection was the basis for the initial configuration for the som method shown earlier
the procedure is quite successful at separating the clusters
since the half sphere is nonlinear nonlinear projection will do better job and this is the topic of the next section
principal components have many other nice properties for example the linear combination xv has the highest variance among all linear combinations of the features xv has the highest variance among all linear combinations satisfying orthogonal to and so on
example handwritten digits principal components are useful tool for dimension reduction and compression
we illustrate this feature on the handwritten digits data described in chapter
figure shows sample of handwritten each digitized grayscale image from total of such
we see considerable variation in writing styles character thickness and orientation
we consider these images as points xi in ir and compute their principal components via the svd
figure shows the first two principal components of these data
for each of these first two principal components ui and ui we computed the and quantile points and used them to define the rectangular grid superimposed on the plot
the circled points indicate
sample of handwritten shows variety of writing styles those images close to the vertices of the grid where the distance measure focuses mainly on these projected coordinates but gives some weight to the components in the orthogonal subspace
the right plot shows the images corresponding to these circled points
this allows us to visualize the nature of the first two principal components
we see that the horizontal movement mainly accounts for the lengthening of the lower tail of the three while vertical movement accounts for character thickness
in terms of the parametrized model this two component model has the form bb bb bb bb bb
here we have displayed the first two principal component directions and as images
although there are possible principal components approximately account for of the variation in the threes account for
figure compares the singular values to those obtained for equivalent uncorrelated data obtained by randomly scrambling each column of
the pixels in digitized image are inherently correlated and since these are all the same digit the correlations are even stronger
left panel the first two principal components of the handwritten threes
the circled points are the closest projected images to the vertices of grid defined by the marginal quantiles of the principal components
right panel the images corresponding to the circled points
these show the nature of the first two principal components
the singular values for the digitized threes compared to those for randomized version of the data each column of was scrambled
principal components curves and surfaces relatively small subset of the principal components serve as excellent lower dimensional features for representing the high dimensional data
left panel two different digitized handwritten ss each represented by corresponding points in ir
the green has been deliberately rotated and translated for visual effect
right panel procrustes transformation applies translation and rotation to best match up the two set of points
figure represents two sets of points the orange and green in the same plot
in this instance these points represent two digitized versions of handwritten extracted from the signature of subject suresh
figure shows the entire signatures from which these were extracted third and fourth panels
the signatures are recorded dynamically using touch screen devices familiar sights in modern supermarkets
there are points representing each which we denote by the matrices and
there is correspondence between the points the ith rows of and are meant to represent the same positions along the two s's
in the language of morphometrics these points represent landmarks on the two objects
how one finds such corresponding landmarks is in general difficult and subject specific
in this particular case we used dynamic time warping of the speed signal along each signature hastie et al but will not go into details here
in the right panel we have applied translation and rotation to the green points so as best to match the orange so called procrustes transformation mardia et al for example
consider the problem min t procrustes was an african bandit in greek mythology who stretched or squashed his visitors to fit his iron bed eventually killing them
unsupervised learning with and both matrices of corresponding points an orthonormal matrix and vector of location coordinates
here trace xt is the squared frobenius matrix norm
let and be the column mean vectors of the matrices and and be the versions of these matrices with the means removed
consider the svd udvt
then the solution to is given by exercise uvt and the minimal distances is referred to as the procrustes distance
from the form of the solution we can center each matrix at its column centroid and then ignore location completely
hereafter we assume this is the case
the procrustes distance with scaling solves slightly more general problem min where is positive scalar
the solution for is as before with trace
related to procrustes distance is the procrustes average of collection of shapes which solves the problem min that is find the shape closest in average squared procrustes distance to all the shapes
this is solved by simple alternating algorithm
initialize for example
solve the procrustes rotation problems with fixed yielding xr
let
steps and are repeated until the criterion converges
figure shows simple example with three shapes
note that we can only expect solution up to rotation alternatively we can impose constraint such as that be upper triangular to force uniqueness
one can easily incorporate scaling in the definition see exercise
most generally we can define the affine invariant average of set of shapes via to simplify matters we consider only orthogonal matrices which include reflections as well as rotations the group although reflections are unlikely here these methods can be restricted further to allow only rotations so group
the procrustes average of three versions of the leading in suresh's signatures
the left panel shows the preshape average with each of the shapes in preshape space superimposed
the right three panels map the preshape separately to match each of the original s's
min where the are any nonsingular matrices
here we require standardization such as mt to avoid trivial solution
the solution is attractive and can be computed without iteration exercise
let xt be the rank projection matrix defined by
mpis the matrix formed from the largest eigenvectors of
principal curves and surfaces principal curves generalize the principal component line providing smooth one dimensional curved approximation to set of data points in irp
principal surface is more general providing curved manifold approximation of dimension or more
we will first define principal curves for random variables irp and then move to the finite data case
let bb be parameterized smooth curve in irp
hence bb is vector function with coordinates each smooth function of the single parameter bb
the parameter bb can be chosen for example to be arc length along the curve from some fixed origin
for each data value let bb define the closest point on the curve to
then bb is called principal curve for the distribution of the random vector if bb bb bb
this says bb is the average of all data points that project to it that is the points for which it is responsible
this is also known as self consistency property
although in practice continuous multivariate distributes have infinitely many principal curves duchamp and stuetzle we are
the principal curve of set of data
each point on the curve is the average of all data points that project there interested mainly in the smooth ones
principal curve is illustrated in figure
principal points are an interesting related concept
consider set of prototypes and for each point in the support of distribution identify the closest prototype that is the prototype that is responsible for it
this induces partition of the feature space into so called voronoi regions
the set of points that minimize the expected distance from to its prototype are called the principal points of the distribution
each principal point is self consistent in that it equals the mean of in its voronoi region
for example with the principal point of circular normal distribution is the mean vector with they are pair of points symmetrically placed on ray through the mean vector
principal points are the distributional analogs of centroids found by means clustering
principal curves can be viewed as principal points but constrained to lie on smooth curve in similar way that som constrains means cluster centers to fall on smooth manifold
to find principal curve bb of distribution we consider its coordinate functions bb bb bb fp bb and let xp
consider the following alternating steps bb xj bb bb bb argmin bb bb
the first equation fixes bb and enforces the self consistency requirement
the second equation fixes the curve and finds the closest point on
principal surface fit to half sphere data
left panel fitted two dimensional surface
right panel projections of data points onto the surface resulting in coordinates bb bb the curve to each data point
with finite data the principal curve algorithm starts with the linear principal component and iterates the two steps in until convergence
scatterplot smoother is used to estimate the conditional expectations in step by smoothing each xj as function of the arc length bb and the projection in is done for each of the observed data points
proving convergence in general is difficult but one can show that if linear least squares fit is used for the scatterplot smoothing then the procedure converges to the first linear principal component and is equivalent to the power method for finding the largest eigenvector of matrix
principal surfaces have exactly the same form as principal curves but are of higher dimension
the mostly commonly used is the two dimensional principal surface with coordinate functions bb bb bb bb fp bb bb
the estimates in step above are obtained from two dimensional surface smoothers
principal surfaces of dimension greater than two are rarely used since the visualization aspect is less attractive as is smoothing in high dimensions
figure shows the result of principal surface fit to the half sphere data
plotted are the data points as function of the estimated nonlinear coordinates bb xi bb xi
the class separation is evident
principal surfaces are very similar to self organizing maps
if we use kernel surface smoother to estimate each coordinate function fj bb bb this has the same form as the batch version of soms
the som weights wk are just the weights in the kernel
there is difference however
unsupervised learning the principal surface estimates separate prototype bb xi bb xi for each data point xi while the som shares smaller number of prototypes for all data points
as result the som and principal surface will agree only as the number of som prototypes grows very large
there also is conceptual difference between the two
principal surfaces provide smooth parameterization of the entire manifold in terms of its coordinate functions while soms are discrete and produce only the estimated prototypes for approximating the data
the smooth parameterization in principal surfaces preserves distance locally in figure it reveals that the red cluster is tighter than the green or blue clusters
in simple examples the estimates coordinate functions themselves can be informative see exercise
spectral clustering traditional clustering methods like means use spherical or elliptical metric to group data points
hence they will not work well when the clusters are non convex such as the concentric circles in the top left panel of figure
spectral clustering is generalization of standard clustering methods and is designed for these situations
it has close connections with the local multidimensional scaling techniques section that generalize mds
the starting point is matrix of pairwise similarities sii between all observation pairs
we represent the observations in an undirected similarity graph hv ei
the vertices vi represent the observations and pairs of vertices are connected by an edge if their similarity is positive or exceeds some threshold
the edges are weighted by the sii
clustering is now rephrased as graph partition problem where we identify connected components with clusters
we wish to partition the graph such that edges between different groups have low weight and within group have high weight
the idea in spectral clustering is to construct similarity graphs that represent the local neighborhood relationships between observations
to make things more concrete consider set of points xi irp and let dii be the euclidean distance between xi and xi
we will use as similarity matrix the radial kernel gram matrix that is sii exp ii where is scale parameter
there are many ways to define similarity matrix and its associated similarity graph that reflect local behavior
the most popular is the mutual nearest neighbor graph
define nk to be the symmetric set of nearby pairs of points specifically pair is in nk if point is among the nearest neighbors of or vice versa
then we connect all symmetric nearest neighbors and give them edge weight wii sii otherwise the edge weight is zero
equivalently we set to zero all the pairwise similarities not in nk and draw the graph for this modified similarity matrix
principal components curves and surfaces alternatively fully connected graph includes all pairwise edges with weights wii sii and the local behavior is controlled by the scale parameter
the matrix of edge weights wii from similarityp graph is called the adjacency matrix
the degree of vertex is gi wii the sum of the weights of the edges connected to it
let be diagonal matrix with diagonal elements gi
finally the graph laplacian is defined by this is called the unnormalized graph laplacian number of normalized versions have been proposed these standardize the laplacian with respect to the node degrees gi for example
spectral clustering finds the eigenvectors zn corresponding to the smallest eigenvalues of ignoring the trivial constant eigenvector
using standard method like means we then cluster the rows of to yield clustering of the original data points
an example is presented in figure
the top left panel shows simulated data points in three circular clusters indicated by the colors
kmeans clustering would clearly have difficulty identifying the outer clusters
we applied spectral clustering using nearest neighbor similarity graph and display the eigenvector corresponding to the second and third smallest eigenvalue of the graph laplacian in the lower left
the smallest eigenvalues are shown in the top right panel
the two eigenvectors shown have identified the three clusters and scatterplot of the rows of the eigenvector matrix in the bottom right clearly separates the clusters
procedure such as means clustering applied to these transformed points would easily identify the three groups
why does spectral clustering work
for any vector we have lf gi fi fi fi wii xx wii fi fi
formula suggests that small value of lf will be achieved if pairs of points with large adjacencies have coordinates fi and fi close together
since for any graph the constant vector is trivial eigenvector with eigenvalue zero
not so obvious is the fact that if the graph is connected it is the only zero eigenvector exercise
generalizing this argument it is easy to show that for graph with connected components graph is connected if any two nodes can be reached via path of connected nodes
toy example illustrating spectral clustering
data in top left are points falling in three concentric clusters of points each
the points are uniformly distributed in angle with radius and in the three groups and gaussian noise with standard deviation added to each point
using nearest neighbor similarity graph the eigenvector corresponding to the second and third smallest eigenvalues of are shown in the bottom left the smallest eigenvector is constant
the data points are colored in the same way as in the top left
the smallest eigenvalues are shown in the top right panel
the coordinates of the nd and rd eigenvectors the rows of are plotted in the bottom right panel
spectral clustering does standard means clustering of these points and will easily recover the three original clusters
principal components curves and surfaces the nodes can be reordered so that is block diagonal with block for each connected component
then has eigenvectors of eigenvalue zero and the eigenspace of eigenvalue zero is spanned by the indicator vectors of the connected components
in practice one has strong and weak connections so zero eigenvalues are approximated by small eigenvalues
spectral clustering is an interesting approach for finding non convex clusters
when normalized graph laplacian is used there is another way to view this method
defining we consider random walk on the graph with transition probability matrix
then spectral clustering yields groups of nodes such that the random walk seldom transitions from one group to another
there are number of issues that one must deal with in applying spectral clustering in practice
we must choose the type of similarity graph eg fully connected or nearest neighbors and associated parameters such as the number of nearest of neighbors or the scale parameter of the kernel
we must also choose the number of eigenvectors to extract from and finally as with all clustering methods the number of clusters
in the toy example of figure we obtained good results for the value corresponding to fully connected graph
with the results deteriorated
looking at the top right panel of figure we see no strong separation between the smallest three eigenvalues and the rest
hence it is not clear how many eigenvectors to select
kernel principal components spectral clustering is related to kernel principal components non linear version of linear principal components
standard linear principal components pca are obtained from the eigenvectors of the covariance matrix and give directions in which the data have maximal variance
kernel pca scho lkopf et al expand the scope of pca mimicking what we would obtain if we were to expand the features by non linear transformations and then apply pca in this transformed feature space
we show in section that the principal components variables of data matrix can be computed from the inner product gram matrix xxt
in detail we compute the eigen decomposition of the doublecentered version of the gram matrix ud ut with and then ud
exercise shows how to compute the projections of new observations in this space
kernel pca simply mimics this procedure interpreting the kernel matrix xi xi as an inner product matrix of the implicit features xi xi and finding its eigenvectors
the elements of the mth component zm mth column of can be written up to centering as pn zim jm xi xj where jm ujm dm exercise
unsupervised learning we can gain more insight into kernel pca by viewing the zm as sample evaluations of principal component functions gm hk with hk the reproducing kernel hilbert space generated by see section
the first principal component function solves max vart subject to hk hk here vart refers to the sample variance over training data
the norm constraint hk controls the size and roughness of the function as dictated by the kernel
as in the regression case it can be shown that the solution to is finite dimensional with representation pn cj xj
exercise shows that the solution is defined by above
the second principal component function is defined in similar way with the additional constraint that hg ihk and so on scho lkopf et al demonstrate the use of kernel principal components as features for handwritten digit classification and show that they can improve the performance of classifier when these are used instead of linear principal components
note that if we use the radial kernel exp kx then the kernel matrix has the same form as the similarity matrix in spectral clustering
the matrix of edge weights is localized version of setting to zero all similarities for pairs of points that are not nearest neighbors
kernel pca finds the eigenvectors corresponding to the largest eigenvale this is equivalent to finding the eigenvectors corresponding to the ues of smallest eigenvalues of
this is almost the same as the laplacian the differences being the centering of ke and the fact that has the degrees of the nodes along the diagonal
figure examines the performance of kernel principal components in the toy example of figure
in the upper left panel we used the radial kernel with the same value that was used in spectral clustering
this does not separate the groups but with upper right panel the first component separates the groups well
in the lower left panel we applied kernel pca using the nearest neighbor radial kernel from spectral clustering
in the lower right panel we use the kernel matrix itself as the this section benefited from helpful discussions with jonathan taylor
kernel principal components applied to the toy example of figure using different kernels
top left radial kernel with
top right radial kernel with
bottom left nearest neighbor radial kernel from spectral clustering
bottom right spectral clustering with laplacian constructed from the radial kernel
unsupervised learning similarity matrix for constructing the laplacian in spectral clustering
in neither case do the projections separate the two groups
adjusting did not help either
in this toy example we see that kernel pca is quite sensitive to the scale and nature of the kernel
we also see that the nearest neighbor truncation of the kernel is important for the success of spectral clustering
sparse principal components we often interpret principal components by examining the direction vectors vj also known as loadings to see which variables play role
we did this with the image loadings in
often this interpretation is made easier if the loadings are sparse
in this section we briefly discuss some methods for deriving principal components with sparse loadings
they are all based on lasso penalties
we start with an data matrix with centered columns
the proposed methods focus on either the maximum variance property of principal components or the minimum reconstruction error
the scotlass procedure of joliffe et al takes the first approach by solving pp max xt subject to vj
the absolute value constraint encourages some of the loadings to be zero and hence to be sparse
further sparse principal components are found in the same way by forcing the kth component to be orthogonal to the first components
unfortunately this problem is not convex and the computations are difficult
zou et al start instead with the regression reconstruction property of pca similar to the approach in section
let xi be the ith row of
for single component their sparse principal component technique solves min xi xi bb bb subject to
lets examine this formulation in more detail
for any bb and bb the solution for is proportional to the largest principal component direction
standard and sparse principal components from study of the corpus callosum variation
the shape variations corresponding to significant principal components red curves are overlaid on the mean cc shape black curves
for multiple components the sparse principal components procedures minimizes xi vt xi bb vk bb vk subject to ik
here is matrix with columns vk and is also
criterion is not jointly convex in and but it is convex in each parameter with the other parameter fixed
minimization over with fixed is equivalent to elastic net problems section and can be done efficiently
on the other hand minimization over with fixed is version of the procrustes problem and is solved by simple svd calculation exercise
these steps are alternated until convergence
figure shows an example of sparse principal components analysis using taken from sjo strand et al
here the shape of the mid sagittal cross section of the corpus callosum cc is related to various clinical parameters in study involving elderly persons
in this exam note that the usual principal component criterion for example is not jointly convex in the parameters either
nevertheless the solution is well defined and an efficient algorithm is available
we thank rasmus larsen and karl sjo strand for suggesting this application and supplying us with the postscript figures reproduced here
an example of mid saggital brain slice with the corpus collosum annotated with landmarks ple pca is applied to shape data and is popular tool in morphometrics
for such applications number of landmarks are identified along the circumference of the shape an example is given in figure
these are aligned by procrustes analysis to allow for rotations and in this case scaling as well see section
the features used for pca are the sequence of coordinate pairs for each landmark unpacked into single vector
in this analysis both standard and sparse principal components were computed and components that were significantly associated with various clinical parameters were identified
in the figure the shape variations corresponding to significant principal components red curves are overlaid on the mean cc shape black curves
low walking speed relates to ccs that are thinner displaying atrophy in regions connecting the motor control and cognitive centers of the brain
low verbal fluency relates to ccs that are thinner in regions connecting auditory visual cognitive centers
the sparse principal components procedure gives more parsimonious and potentially more informative picture of the important differences
non negative matrix factorization non negative matrix factorization non negative matrix factorization lee and seung is recent alternative approach to principal components analysis in which the data and components are assumed to be non negative
it is useful for modeling non negative data such as images
the data matrix is approximated by wh where is and is max
we assume that xij wik hkj
the matrices and are found by maximizing xij log wh ij wh ij
this is the log likelihood from model in which xij has poisson distribution with mean wh ij quite reasonable for positive data
the following alternating algorithm lee and seung converges to local maximum of pp hkj xij wh ij wik wik pp hkj pn wik xij wh ij hkj hkj pn wik this algorithm can be derived as minorization procedure for maximizing exercise and is also related to the iterative proportionalscaling algorithm for log linear models exercise
figure shows an example taken from lee and seung comparing non negative matrix factorization nmf vector quantization vq equivalent to means clustering and principal components analysis pca
the three learning methods were applied to database of facial images each consisting of pixels resulting in matrix
as shown in the array of montages each image each method has learned set of basis images
positive values are illustrated with black pixels and negative values with red pixels
particular instance of face shown at top right is approximated by linear superposition of basis images
the coefficients of the linear superposition are shown next to each montage in array and the resulting superpositions are shown to the right of the equality sign
the authors point we thank sebastian seung for providing this image
these arrangements allow for compact display and have no structural significance
unsupervised learning out that unlike vq and pca nmf learns to represent faces with set of basis images resembling parts of faces
donoho and stodden point out potentially serious problem with non negative matrix factorization
even in situations where wh holds exactly the decomposition may not be unique
figure illustrates the problem
the data points lie in dimensions and there is open space between the data and the coordinate axes
we can choose the basis vectors and anywhere in this open space and represent each data point exactly with nonnegative linear combination of these vectors
this nonuniqueness means that the solution found by the above algorithm depends on the starting values and it would seem to hamper the interpretability of the factorization
despite this interpretational drawback the non negative matrix factorization and its applications has attracted lot of interest
archetypal analysis this method due to cutler and breiman approximates data points by prototypes that are themselves linear combinations of data points
in this sense it has similar flavor to means clustering
however rather than approximating each data point by single nearby prototype archetypal analysis approximates each data point by convex combination of collection of prototypes
the use of convex combination forces the prototypes to lie on the convex hull of the data cloud
in this sense the prototypes are pure or archetypal
as in the data matrix is modeled as wh pr where is and is
we assume that wik and wik
hence the data points rows of in dimensional space are represented by convex combinations of the archetypes rows of
we also assume that bx pn where is with bki and bki
thus the archetypes themselves are convex combinations of the data points
using both and we minimize wh wbx over the weights and
this function is minimized in an alternating fashion with each separate minimization involving convex optimization
the overall problem is not convex however and so the algorithm converges to local minimum of the criterion
non negative matrix factorization nmf vector quantization vq equivalent to means clustering and principal components analysis pca applied to database of facial images
details are given in the text
unlike vq and pca nmf learns to represent faces with set of basis images resembling parts of faces
non uniqueness of the non negative matrix factorization
there are data points in two dimensions
any choice of the basis vectors and in the open space between the coordinate axes and data gives an exact reconstruction of the data
figure shows an example with simulated data in two dimensions
the top panel displays the results of archetypal analysis while the bottom panel shows the results from means clustering
in order to best reconstruct the data from convex combinations of the prototypes it pays to locate the prototypes on the convex hull of the data
this is seen in the top panels of figure and is the case in general as proven by cutler and breiman
means clustering shown in the bottom panels chooses prototypes in the middle of the data cloud
we can think of means clustering as special case of the archetypal model in which each row of has single one and the rest of the entries are zero
notice also that the archetypal model has the same general form as the non negative matrix factorization model
however the two models are applied in different settings and have somewhat different goals
non negative matrix factorization aims to approximate the columns of the data matrix and the main output of interest are the columns of representing the primary non negative components in the data
archetypal analysis focuses instead on the approximation of the rows of using the rows of which represent the archetypal data points
non negative matrix factorization also assumes that
with we can get an exact reconstruction simply choosing to be the data with columns scaled so that they sum to
in contrast archetypal analysis requires but allows
in figure for example while or
the additional constraint implies that the archetypal approximation will not be perfect even if
figure shows the results of archetypal analysis applied to the database of displayed in figure
the three rows in figure are the resulting archetypes from three runs specifying two three and four
archetypal analysis top panels and means clustering bottom panels applied to data points drawn from bivariate gaussian distribution
the colored points show the positions of the prototypes in each case archetypes respectively
as expected the algorithm has produced extreme both in size and shape
independent component analysis and exploratory projection pursuit multivariate data are often viewed as multiple indirect measurements arising from an underlying source which typically cannot be directly measured
archetypal analysis applied to the database of digitized
the rows in the figure show the resulting archetypes from three runs specifying two three and four archetypes respectively fluences and other driving forces that may be hard to identify or measure
factor analysis is classical technique developed in the statistical literature that aims to identify these latent sources
factor analysis models are typically wed to gaussian distributions which has to some extent hindered their usefulness
more recently independent component analysis has emerged as strong competitor to factor analysis and as we will see relies on the non gaussian nature of the underlying sources for its success
latent variables and factor analysis udv the singular value decomposition latent variable has representation
writing and dvt we have sat and hence each of the columns of is linear combination of the columns of
now since is orthogonal and assuming as before that the columns of and hence each have mean zero this implies that the columns of have zero mean are uncorrelated and have unit variance
in terms of random variables we can interpret the svd or the corresponding principal component analysis pca as an estimate of latent variable model
independent component analysis and exploratory projection pursuit sp sp
xp ap sp app sp or simply as
the correlated xj are each represented as linear expansion in the uncorrelated unit variance variables
this is not too satisfactory though because given any orthogonal matrix we can write as art rs and cov cov rt
hence there are many such decompositions and it is therefore impossible to identify any particular latent variables as unique underlying sources
the svd decomposition does have the property that any rank truncated decomposition approximates in an optimal way
the classical factor analysis model developed primarily by researchers in psychometrics alleviates these problems to some extent see for example mardia et al
with factor analysis model has the form sq sq
xp ap apq sq or as
here is vector of underlying latent variables or factors is matrix of factor loadings and the are uncorrelated zero mean disturbances
the idea is that the latent variables are common sources of variation amongst the xj and account for their correlation structure while the uncorrelated are unique to each xj and pick up the remaining unaccounted variation
typically the sj and the are modeled as gaussian random variables and the model is fit by maximum likelihood
the parameters all reside in the covariance matrix aat where diag var var
the sj being gaussian and uncorrelated makes them statistically independent random variables
thus battery of educational test scores would be thought to be driven by the independent underlying factors such as intelligence drive and so on
the columns of are referred to as the factor loadings and are used to name and interpret the factors
unsupervised learning unfortunately the identifiability issue remains since and art are equivalent in for any orthogonal
this leaves certain subjectivity in the use of factor analysis since the user can search for rotated versions of the factors that are more easily interpretable
this aspect has left many analysts skeptical of factor analysis and may account for its lack of popularity in contemporary statistics
although we will not go into details here the svd plays key role in the estimation of
for example if the var are all assumed to be equal the leading components of the svd identify the subspace determined by
because of the separate disturbances for each xj factor analysis can be seen to be modeling the correlation structure of the xj rather than the covariance structure
this can be easily seen by standardizing the covariance structure in exercise
this is an important distinction between factor analysis and pca although not central to the discussion here
exercise discusses simple example where the solutions from factor analysis and pca differ dramatically because of this distinction
independent component analysis the independent component analysis ica model has exactly the same form as except the si are assumed to be statistically independent rather than uncorrelated
intuitively lack of correlation determines the second degree cross moments covariances of multivariate distribution while in general statistical independence determines all of the crossmoments
these extra moment conditions allow us to identify the elements of uniquely
since the multivariate gaussian distribution is determined by its second moments alone it is the exception and any gaussian independent components can be determined only up to rotation as before
hence identifiability problems in and can be avoided if we assume that the si are independent and non gaussian
here we will discuss the full component model as in where the are independent with unit variance ica versions of the factor analysis model exist as well
our treatment is based on the survey article by hyva rinen and oja
we wish to recover the mixing matrix in as
without loss of generality we can assume that has already been whitened to have cov this is typically achieved via the svd described above
this in turn implies that is orthogonal since also has covariance
so solving the ica problem amounts to finding an orthogonal such that the components of the vector random variable at are independent and non gaussian
figure shows the power of ica in separating two mixed signals
this is an example of the classical cocktail party problem where different microphones xj pick up mixtures of different independent sources music speech from different speakers etc
ica is able to perform blind
illustration of ica vs
pca on artificial time series data
the upper left panel shows the two source signals measured at uniformly spaced time points
the upper right panel shows the observed mixed signals
the lower two panels show the principal components and independent component solutions source separation by exploiting the independence and non gaussianity of the original sources
many of the popular approaches to ica are based on entropy
the differential entropy of random variable with density is given by log dy
well known result in information theory says that among all random variables with equal variance gaussian variables have the maximum entropy
finally the mutual information between the components of the random vector is natural measure of dependence yj
qp distance between the the quantity is called the kullback leibler density of and its independence version gj yj where gj yj is the marginal density of yj
now if has covariance and at with orthogonal then it is easy to show that yj log det xp yj
finding an to minimize at looks for the orthogonal transformation that leads to the most independence between its components
mixtures of independent uniform random variables
the upper left panel shows realizations from the two independent uniform sources the upper right panel their mixed versions
the lower two panels show the pca and ica solutions respectively light of this is equivalent to minimizing the sum of the entropies of the separate components of which in turn amounts to maximizing their departures from gaussianity
for convenience rather than using the entropy yj hyva rinen and oja use the negentropy measure yj defined by yj zj yj where zj is gaussian random variable with the same variance as yj
negentropy is non negative and measures the departure of yj from gaussianity
they propose simple approximations to negentropy which can be computed and optimized on data
the ica solutions shown in figures use the approximation yj eg yj eg zj where log cosh au for
when applied to sample of xi the expectations are replaced by data averages
this is one of the options in the fastica software provided by these authors
more classical and less robust measures are based on fourth moments and hence look for departures from the gaussian via kurtosis
see hyva rinen and oja for more details
in section we describe their approximate newton algorithm for finding the optimal directions
in summary then ica applied to multivariate data looks for sequence of orthogonal projections such that the projected data look as far from
comparison of the first five ica components computed using fastica above diagonal with the first five pca components below diagonal
each component is standardized to have unit variance
gaussian as possible
with pre whitened data this amounts to looking for components that are as independent as possible
ica starts from essentially factor analysis solution and looks for rotations that lead to independent components
from this point of view ica is just another factor rotation method along with the traditional varimax and quartimax methods used in psychometrics
example handwritten digits we revisit the handwritten threes analyzed by pca in section
figure compares the first five standardized principal components with the first five ica components all shown in the same standardized units
note that each plot is two dimensional projection from dimensional
the highlighted digits from figure
by comparing with the mean digits we see the nature of the ica component space
while the pca components all appear to have joint gaussian distributions the ica components have long tailed distributions
this is not too surprising since pca focuses on variance while ica specifically looks for non gaussian distributions
all the components have been standardized so we do not see the decreasing variances of the principal components
for each ica component we have highlighted two of the extreme digits as well as pair of central digits and displayed them in figure
this illustrates the nature of each of the components
for example ica component five picks up the long sweeping tailed threes
example eeg time courses ica has become an important tool in the study of brain dynamics the example we present here uses ica to untangle the components of signals in multi channel electroencephalographic eeg data onton and makeig
subjects wear cap embedded with lattice of eeg electrodes which record brain activity at different locations on the scalp
figure top panel shows seconds of output from subset of nine of these electrodes from subject performing standard two back learning task over minute period
the subject is presented with letter or at roughly ms intervals and responds by pressing one of two buttons to indicate whether the letter presented is the same or different from that presented two steps back
depending on the answer the subject earns or loses points and occasionally earns bonus or loses penalty points
the time course data show spatial correlation in the eeg signals the signals of nearby sensors look very similar
the key assumption here is that signals recorded at each scalp electrode are mixture of independent potentials arising from different cortical ac reprinted from progress in brain research vol
julie onton and scott makeig information based modeling of event related brain dynamics page copyright with permission from elsevier
we thank julie onton and scott makeig for supplying an electronic version of the image
independent component analysis and exploratory projection pursuit tivities as well as non cortical artifact domains see the reference for detailed overview of ica in this domain
the lower part of figure shows selection of ica components
the colored images represent the estimated unmixing coefficient vectors as heatmap images superimposed on the scalp indicating the location of activity
the corresponding time courses show the activity of the learned ica components
for example the subject blinked after each performance feedback signal colored vertical lines which accounts for the location and artifact signal in ic and ic
ic is an artifact associated with the cardiac pulse
ic and ic account for frontal theta band activities and appear after stretch of correct performance
see onton and makeig for more detailed discussion of this example and the use of ica in eeg modeling
exploratory projection pursuit friedman and tukey proposed exploratory projection pursuit graphical exploration technique for visualizing high dimensional data
their view was that most low oneor two dimensional projections of highdimensional data look gaussian
interesting structure such as clusters or long tails would be revealed by non gaussian projections
they proposed number of projection indices for optimization each focusing on different departure from gaussianity
since their initial proposal variety of improvements have been suggested huber friedman and variety of indices including entropy are implemented in the interactive graphics package xgobi swayne et al now called ggobi
these projection indices are exactly of the same form as yj above where yj atj normalized linear combination of the components of
in fact some of the approximations and substitutions for cross entropy coincide with indices proposed for projection pursuit
typically with projection pursuit the directions aj are not constrained to be orthogonal
friedman transforms the data to look gaussian in the chosen projection and then searches for subsequent directions
despite their different origins ica and exploratory projection pursuit are quite similar at least in the representation described here
direct approach to ica independent components have by definition joint product density fs fj sj so here we present an approach that estimates this density directly using generalized additive models section
full details can be found in
fifteen seconds of eeg data of seconds at nine of scalp channels top panel as well as nine ica components lower panel
while nearby electrodes record nearly identical mixtures of brain and non brain activity ica components are temporally distinct
the colored scalps represent the ica unmixing coefficients as heatmap showing brain or scalp location of the source
independent component analysis and exploratory projection pursuit hastie and tibshirani and the method is implemented in the package prodenica available from cran
in the spirit of representing departures from gaussianity we represent each fj as fj sj sj egj sj tilted gaussian density
here is the standard gaussian density and gj satisfies the normalization conditions required of density
assuming as before that is pre whitened the log likelihood for the observed data as is gj log atj xi gj atj xi which we wish to maximize subject to the constraints that is orthogonal and that the gj result in densities in
it can further be shown that the solution densities eg each have mean zero and variance one exercise
as we increase bb these solutions approach the standard gaussian
algorithm product density ica algorithm prodenica
initialize random gaussian matrix followed by orthogonalization
alternate until convergence of given optimize gj separately for each given gj perform one step of fixed point algorithm towards finding the optimal
we fit the functions gj and directions aj by optimizing in an alternating fashion as described in algorithm
unsupervised learning step amounts to semi parametric density estimation which can be solved using novel application of generalized additive models
for convenience we extract one of the separate problems log si si dt bb dt
although the second integral in leads to smoothing spline the first integral is problematic and requires an approximation
we construct fine grid of values in increments covering the observed values si and count the number of si in the resulting bins si
typically we pick to be which is more than adequate
we can then approximate by yi log eg bb ds
this last expression can be seen to be proportional to penalized poisson log likelihood with response and penalty parameter bb and mean eg
this is generalized additive spline model hastie and tibshirani efron and tibshirani with an offset term log and can be fit using newton algorithm in operations
although quartic spline is called for we find in practice that cubic spline is adequate
we have tuning parameters bb to set in practice we make them all the same and specify the amount of smoothing via the effective degrees of freedom df bb
our software uses df as default value
step in algorithm requires optimizing with respect to holding the fixed
only the first terms in the sum involve and since is orthogonal the collection of terms involving do not depend on exercise
hence we need to maximize xx atj xi cj aj is log likelihood ratio between the fitted density and gaussian and can be seen as an estimate of negentropy with each contrast function as in
the fixed point update in step is modified newton step exercise
independent component analysis and exploratory projection pursuit
for each update aa aj atj atj aj where represents expectation the sample xi
since is fitted quartic or cubic spline the first and second derivatives are readily available
orthogonalize using the symmetric square root transformation aat
if udvt is the svd of it is easy to show that this leads to the update uvt
our prodenica algorithm works as well as fastica on the artificial time series data of figure the mixture of uniforms data of figure and the digit data in figure
the left panel shows distributions used for comparisons
these include the uniform exponential mixtures of exponentials symmetric and asymmetric gaussian mixtures
the right panel shows on the log scale the average amari metric for each method and each distribution based on simulations in ir for each distribution
figure shows the results of simulation comparing prodenica to fastica and another semi parametric competitor kernelica bach and jordan
the left panel shows the distributions used as basis of comparison
for each distribution we generated pair of independent components and random mixing matrix in ir with condition number between and
we used our implementations of fastica using the negentropy criterion and prodenica
for kernelica we used
unsupervised learning the authors matlab code since the search criteria are nonconvex we used five random starts for each method
each of the algorithms delivers an orthogonal mixing matrix the data were pre whitened which is available for comparison with the generating orthogonalized mixing matrix
we used the amari metric bach and jordan as measure of the closeness of the two frames pp
pp rij rij maxj rij maxi rij where rij ao ij
the right panel in figure compares the averages on the log scale of the amari metric between the truth and the estimated mixing matrices
prodenica is competitive with fastica and kernelica in all situations and dominates most of the mixture simulations
multidimensional scaling both self organizing maps and principal curves and surfaces map data points in irp to lower dimensional manifold
multidimensional scaling mds has similar goal but approaches the problem in somewhat different way
we start with observations xn irp and let dij be the distance between observations and
often we choose euclidean distance dij xi xj but other distances may be used
further in some applications we may not even have available the data points xi but only have some dissimilarity measure dij see section
for example in wine tasting experiment dij might be measure of how different subject judged wines and and the subject provides such measure for all pairs of wines
mds requires only the dissimilarities dij in contrast to the som and principal curves and surfaces which need the data points xi
multidimensional scaling seeks values zn irk to minimize the so called stress function sm zn dii zi zi
this is known as least squares or kruskal shephard scaling
the idea is to find lower dimensional representation of the data that preserves the pairwise distances as well as possible
notice that the approximation is francis bach kindly supplied this code and helped us set up the simulations
some authors define stress as the square root of since it does not affect the optimization we leave it squared to make comparisons with other criteria simpler
multidimensional scaling in terms of the distances rather than squared distances which results in slightly messier algebra
gradient descent algorithm is used to minimize sm
variation on least squares scaling is the so called sammon mapping which minimizes dii zi zi ssm zn
dii here more emphasis is put on preserving smaller pairwise distances
in classical scaling we instead start with similarities sii often we use the centered inner product sii hxi xi
the problem then is to minimize sc zn sii hzi zi over zn irk
this is attractive because there is an explicit solution in terms of eigenvectors see exercise
if we have distances rather than inner products we can convert them to centered inner products if the distances are euclidean see on page in chapter
if the similarities are in fact centered inner products classical scaling is exactly equivalent to principal components an inherently linear dimensionreduction technique
classical scaling is not equivalent to least squares scaling the loss functions are different and the mapping can be nonlinear
least squares and classical scaling are referred to as metric scaling methods in the sense that the actual dissimilarities or similarities are approximated
shephard kruskal nonmetric scaling effectively uses only ranks
nonmetric scaling seeks to minimize the stress function zi zi dii snm zn zi zi over the zi and an arbitrary increasing function
with fixed we minimize over zi by gradient descent
with the zi fixed the method of isotonic regression is used to find the best monotonic approximation dii to zi zi
these steps are iterated until the solutions stabilize
like the self organizing map and principal surfaces multidimensional scaling represents high dimensional data in low dimensional coordinate system
principal surfaces and soms go step further and approximate the original data by low dimensional manifold parametrized in the low dimensional coordinate system
in principal surface and som points an distance matrix is euclidean if the entries represent pairwise euclidean distances between points in some dimensional space
first two coordinates for half sphere data from classical multidimensional scaling close together in the original feature space should map close together on the manifold but points far apart in feature space might also map close together
this is less likely in multidimensional scaling since it explicitly tries to preserve all pairwise distances
figure shows the first two mds coordinates from classical scaling for the half sphere example
there is clear separation of the clusters and the tighter nature of the red cluster is apparent
nonlinear dimension reduction and local multidimensional scaling several methods have been recently proposed for nonlinear dimension reduction similar in spirit to principal surfaces
the idea is that the data lie close to an intrinsically low dimensional nonlinear manifold embedded in high dimensional space
these methods can be thought of as flattening the manifold and hence reducing the data to set of low dimensional coordinates that represent their relative positions in the manifold
they are useful for problems where signal to noise ratio is very high physical systems and are probably not as useful for observational data with lower signal to noise ratios
the basic goal is illustrated in the left panel of figure
the data lie near parabola with substantial curvature
classical mds does not pre
the orange points show data lying on parabola while the blue points shows multidimensional scaling representations in one dimension
classical multidimensional scaling left panel does not preserve the ordering of the points along the curve because it judges points on opposite ends of the curve to be close together
in contrast local multidimensional scaling right panel does good job of preserving the ordering of the points along the curve serve the ordering of the points along the curve because it judges points on opposite ends of the curve to be close together
the right panel shows the results of local multi dimensional scaling one of the three methods for non linear multi dimensional scaling that we discuss below
these methods use only the coordinates of the points in dimensions and have no other information about the manifold
local mds has done good job of preserving the ordering of the points along the curve
we now briefly describe three new approaches to nonlinear dimension reduction and manifold mapping
isometric feature mapping isomap tenenbaum et al constructs graph to approximate the geodesic distance between points along the manifold
specifically for each data point we find its neighbors points within some small euclidean distance of that point
we construct graph with an edge between any two neighboring points
the geodesic distance between any two points is then approximated by the shortest path between points on the graph
finally classical scaling is applied to the graph distances to produce low dimensional mapping
local linear embedding roweis and saul takes very different approach trying to preserve the local affine structure of the high dimensional data
each data point is approximated by linear combination of neighboring points
then lower dimensional representation is constructed that
unsupervised learning best preserves these local approximations
the details are interesting so we give them here
for each data point xi in dimensions we find its nearest neighbors in euclidean distance
we approximate each point by an affine mixture of the points in its neighborhood min xi wik xk wik pn over weights wik satisfying wik wik wik is the contribution of point to the reconstruction of point
note that for hope of unique solution we must have
finally we find points yi in space of dimension to minimize yi wik yk with wik fixed
in step we minimize tr wy wy tr yt where is is for some small
the solutions are the trailing eigenvectors of
since is trivial eigenvector with eigenvalue we discard it and keep the next
this has the side effect that and hence the embedding coordinates are mean centered
local mds chen and buja takes the simplest and arguably the most direct approach
we define to be the symmetric set of nearby pairs of points specifically pair is in if point is among the nearest neighbors of or vice versa
then we construct the stress function sl zn dii zi zi zi zi
here is some large constant and is weight
the idea is that points that are not neighbors are considered to be very far apart such pairs are given small weight so that they don't dominate the overall stress function
to simplify the expression we take and let
expanding this gives
images of faces mapped into the embedding space described by the first two coordinates of lle
next to the circled points representative faces are shown in different parts of the space
the images at the bottom of the plot correspond to points along the top right path linked by solid line and illustrate one particular mode of variability in pose and expression
unsupervised learning sl zn dii zi zi zi zi where wd
the first term in tries to preserve local structure in the data while the second term encourages the representations zi zi for pairs that are non neighbors to be farther apart
local mds minimizes the stress function over zi for fixed values of the number of neighbors and the tuning parameter
the right panel of figure shows the result of local mds using neighbors and
we used coordinate descent with multiple starting values to find good minimum of the nonconvex stress function
the ordering of the points along the curve has been largely preserved figure shows more interesting application of one of these methods lle
the data consist of photographs digitized as grayscale images
the result of the first two coordinates of lle are shown and reveal some variability in pose and expression
similar pictures were produced by local mds
in experiments reported in chen and buja local mds shows superior performance as compared to isomap and lle
they also demonstrate the usefulness of local mds for graph layout
there are also close connections between the methods discussed here spectral clustering section and kernel pca section
the google pagerank algorithm in this section we give brief description of the original pagerank algorithm used by the google search engine an interesting recent application of unsupervised learning methods
we suppose that we have web pages and wish to rank them in terms of importance
for example the pages might all contain string match to statistical learning and we might wish to rank the pages in terms of their likely relevance to websurfer
the pagerank algorithm considers webpage to be important if many other webpages point to it
however the linking webpages that point to given page are not treated equally the algorithm also takes into account both the importance pagerank of the linking pages and the number of outgoing links that they have
linking pages with higher pagerank are given more weight while pages with more outgoing links are given less weight
these ideas lead to recursive definition for pagerank detailed next
sam roweis and lawrence saul kindly provided this figure
the google pagerank algorithm let lij if page points to page and zero otherwise
let cj pn lij equal the number of pages pointed to by page number of outlinks
then the google pageranks pi are defined by the recursive relationship xn lij pi pj cj where is positive constant apparently set to
the idea is that the importance of page is the sum of the importances of pages that point to that page
the sums are weighted by cj that is each page distributes total vote of to other pages
the constant ensures that each page gets pagerank of at least
in matrix notation ld where is vector of ones and dc diag is diagonal matrix with diagonal elements cj
introducing the normalization et the average pagerank is we can write as eet dld ap where the matrix is the expression in square braces
exploiting connection with markov chains see below it can be shown that the matrix has real eigenvalue equal to one and one is its largest eigenvalue
this means that we can find by the power method starting with some we iterate pk pk apk pk
pk the fixed points are the desired pageranks
in the original paper of page et al the authors considered pagerank as model of user behavior where random web surfer clicks on links at random without regard to content
the surfer does random walk on the web choosing among available outgoing links at random
the factor is the probability that he does not click on link but jumps instead to random webpage
some descriptions of pagerank have as the first term in definition which would better coincide with the random surfer interpretation
then the page rank solution divided by is the stationary distribution of an irreducible aperiodic markov chain over the webpages
definition also corresponds to an irreducible aperiodic markov chain with different transition probabilities than those from he version
viewing pagerank as markov chain makes clear why the matrix has maximal real eigenvalue of
since has positive entries with
pagerank algorithm example of small network each column summing to one markov chain theory tells us that it has unique eigenvector with eigenvalue one corresponding to the stationary distribution of the chain bremaud
small network is shown for illustration in figure
the link matrix is eb ec ec ed and the number of outlinks is
the pagerank solution is
notice that page has no incoming links and hence gets the minimum pagerank of
bibliographic notes there are many books on clustering including hartigan gordon and kaufman and rousseeuw
means clustering goes back at least to lloyd forgy jancey and macqueen
applications in engineering especially in image compression via vector quantization can be found in gersho and gray
the medoid procedure is described in kaufman and rousseeuw
association rules are outlined in agrawal et al
the self organizing map was proposed by kohonen and kohonen kohonen et al give more recent account
principal components analysis and multidimensional scaling are described in standard books on multivariate analysis for example mardia et al
buja et al have implemented powerful environment called ggvis for multidimensional scaling and the user manual
exercises contains lucid overview of the subject
figures left panel and left panel were produced in xgobi multidimensional data visualization package by the same authors
ggobi is more recent implementation cook and swayne
goodall gives technical overview of procrustes methods in statistics and ramsay and silverman discuss the shape registration problem
principal curves and surfaces were proposed in hastie and hastie and stuetzle
the idea of principal points was formulated in flury tarpey and flury give an exposition of the general concept of self consistency
an excellent tutorial on spectral clustering can be found in von luxburg this was the main source for section
luxborg credits donath and hoffman and fiedler with the earliest work on the subject
history of spectral clustering my be found in spielman and teng
independent component analysis was proposed by comon with subsequent developments by bell and sejnowski our treatment in section is based on hyva rinen and oja
projection pursuit was proposed by friedman and tukey and is discussed in detail in huber
dynamic projection pursuit algorithm is implemented in ggobi
exercises ex
weights for clustering
show that weighted euclidean distance pp wl xil xi de xi xi pp wl satisfies xi xi de zi zi zil zi where wl zil xil pp
wl thus weighted euclidean distance based on is equivalent to unweighted euclidean distance based on
consider mixture model density in dimensional feature space gk where gk k and with
here k and are unknown parameters
unsupervised learning suppose we have data xn and we wish to fit the mixture model
write down the log likelihood of the data
derive an em algorithm for computing the maximum likelihood estimates see section
show that if has known value in the mixture model and we take then in sense this em algorithm coincides with means clustering
in section we discuss the use of cart or prim for constructing generalized association rules
show that problem occurs with either of these methods when we generate the random data from the productmarginal distribution by randomly permuting the values for each of the variables
propose ways to overcome this problem
cluster the demographic data of table using classification tree
specifically generate reference sample of the same size of the training set by randomly permuting the values within each feature
build classification tree to the training sample class and the reference sample class and describe the terminal nodes having highest estimated class probability
compare the results to the prim results near table and also to the results of means clustering applied to the same data
generate data with three features with data points in each of three classes as follows sin cos sin sin cos sin cos sin sin cos sin cos sin sin cos here indicates uniform variate on the range and wjk are independent normal variates with standard deviation
hence the data
exercises lie near the surface of sphere in three clusters centered at and
write program to fit som to these data using the learning rates given in the text
carry out means clustering of the same data and compare the results to those in the text
write programs to implement means clustering and selforganizing map som with the prototype lying on two dimensional grid
apply them to the columns of the human tumor microarray data using centroids for both
demonstrate that as the size of the som neighborhood is taken to be smaller and smaller the som solution becomes more similar to the means solution
derive and in section
show that is not unique and characterize the family of equivalent solutions
derive the solution to the procrustes problem
derive also the solution to the procrustes problem with scaling
write an algorithm to solve min
apply it to the three s's and compare the results to those shown in figure
derive the solution to the affine invariant average problem
apply it to the three s's and compare the results to those computed in exercise
classical multidimensional scaling
let be the centered inner product matrix with elements hxi xj
let bb bb bb be the largest eigenvalues of with associated eigenvectors evk ek
let dk be diagonal matrix with diagonal entries bb bb bb
show that the solutions zi to the classical scaling problem are the rows of ek dk
consider the sparse pca criterion
show that with fixed solving for amounts to separate elasticnet regression problems with responses the elements of xi
show that with fixed solving for amounts to reduced rank version of the procrustes problem which reduces to max trace subject to ik where and are both with
if udqt is the svd of show that the optimal uqt
unsupervised learning ex
generate data points with three features lying close to helix
in detail define cos sin where takes on equally spaced values between and and are independent and have standard gaussian distributions fit principal curve to the data and plot the estimated coordinate functions
compare them to the underlying functions cos sin and fit self organizing map to the same data and see if you can discover the helical shape of the original point cloud
preand post multiply equation by diagonal matrix containing the inverse variances of the xj
hence obtain an equivalent decomposition for the correlation matrix in the sense that simple scaling is applied to the matrix
generate observations of three variates according to where are independent standard normal variates
compute the leading principal component and factor analysis directions
hence show that the leading principal component aligns itself in the maximal variance direction while the leading factor essentially ignores the uncorrelated component and picks up the correlated component geoffrey hinton personal communication
consider the kernel principal component procedure outlined in section
argue that the number of principal components is equal to the rank of which is the number of non zero elements in
show that the mth component zm mth column of can be written up to pn centering as zim jm xi xj where jm ujm dm
show that pn of new observation to the mth component is given by the mapping jm xj
pn ex
show that with cj xj the solution to is given by uj where is the first column of in and the first diagonal element of
show that the second and subsequent principal component functions are defined in similar manner hint see section
consider the regularized log likelihood for the density estimation problem arising in ica
exercises log si si eg dt bb dt
the solution is quartic smoothing spline and can be written as where is quadratic function in the null space of the penalty
let
by examining the stationarity conditions for show that the solution eg is density and has mean zero and variance one
if we used second derivative penalty dt instead what simple modification could we make to the problem to maintain the three moment conditions
if is orthogonal show that the first term in on page log atj xi with aj the jth column of does not depend on
fixed point algorithm for ica hyva rinen et al
consider maximizing at with respect to with and cov
use lagrange multiplier to enforce the norm constraint and write down the first two derivatives of the modified criterion
use the approximation xx at xx at to show that the newton update can be written as the fixed point update
consider an undirected graph with non negative edge weights wii and graph laplacian
suppose there are connected components am in the graph
show that there are eigenvectors of corresponding to eigenvalue zero and the indicator vectors of these components ia ia iam span the zero eigenspace
show that definition implies that the sum of the pageranks pi is the number of web pages write program to compute the pagerank solutions by the power method using formulation
apply it to the network of figure
algorithm for non negative matrix factorization wu and lange
function to said to minorize function if
example of small network for all in the domain
this is useful for maximizing since it is easy to show that is nondecreasing under the update xs argmaxx xs there are analogous definitions for majorization for minimizing function
the resulting algorithms are known as mm algorithms for minorizemaximize or majorize minimize lange
it also can be shown that the em algorithm is an example of an mm algorithm see section and exercise for details consider maximization of the function in written here without the matrix notation
xij log wik hkj wik hkj using the concavity ofplog show that for any set of values yk and ck with ck
log yk ck log yk ck hence

asikj bsij log wik hkj log wik hkj bsij asikj where asikj wik hkj and bsij wik hkj and indicates the current iteration
exercises hence show that ignoring constants the function asikj ws hs uij log wik log kj bsij wik hkj minorizes set the partial derivatives of ws hs to zero and hence derive the updating steps
consider the non negative matrix factorization in the rank one case show that the updates reduce to pp xij wi wi pp wi hj pn xij hj hj pni wi hj where wi wi hj
this is an example of the iterative proportional scaling procedure applied to the independence model for two way contingency table fienberg for example show that the final iterates have the explicit form pp pn xij xik wi pn pp hk pn pp ij xij for any constant
these are equivalent to the usual row and column estimates for two way independence model
fit non negative matrix factorization model to the collection of two's in the digits database
use basis elements and compare with component plus mean pca model
in both cases display the and matrices as in figure
unsupervised learning
this is page printer opaque this random forests introduction bagging or bootstrap aggregation section is technique for reducing the variance of an estimated prediction function
bagging seems to work especially well for high variance low bias procedures such as trees
for regression we simply fit the same regression tree many times to bootstrapsampled versions of the training data and average the result
for classification committee of trees each cast vote for the predicted class
boosting in chapter was initially proposed as committee method as well although unlike bagging the committee of weak learners evolves over time and the members cast weighted vote
boosting appears to dominate bagging on most problems and became the preferred choice
random forests breiman is substantial modification of bagging that builds large collection of de correlated trees and then averages them
on many problems the performance of random forests is very similar to boosting and they are simpler to train and tune
as consequence random forests are popular and are implemented in variety of packages
definition of random forests the essential idea in bagging section is to average many noisy but approximately unbiased models and hence reduce the variance
trees are ideal candidates for bagging since they can capture complex interaction
random forests algorithm random forest for regression or classification
for to draw bootstrap sample of size from the training data grow random forest tree tb to the bootstrapped data by recursively repeating the following steps for each terminal node of the tree until the minimum node size nmin is reached
select variables at random from the variables ii
pick the best variable split point among the iii
split the node into two daughter nodes
output the ensemble of trees tb
to make prediction at new point pb regression rfb tb
classification let be the class prediction of the bth random forest tree
then rfb majority vote structures in the data and if grown sufficiently deep have relatively low bias
since trees are notoriously noisy they benefit greatly from the averaging
moreover since each tree generated in bagging is identically distributed the expectation of an average of such trees is the same as the expectation of any one of them
this means the bias of bagged trees is the same as that of the individual trees and the only hope of improvement is through variance reduction
this is in contrast to boosting where the trees are grown in an adaptive way to remove bias and hence are not
an average of random variables each with variance has variance
if the variables are simply identically distributed but not necessarily independent with positive pairwise correlation the variance of the average is exercise
as increases the second term disappears but the first remains and hence the size of the correlation of pairs of bagged trees limits the benefits of averaging
the idea in random forests algorithm is to improve the variance reduction of bagging by reducing the correlation between the trees without increasing the variance too much
this is achieved in the tree growing process through random selection of the input variables
specifically when growing tree on bootstrapped dataset before each split select of the input variables at random as candidates for splitting
definition of random forests typically values for are or even as low as
after such trees are grown the random forest regression predictor is rfb
as in section page characterizes the bth random forest tree in terms of split variables cutpoints at each node and terminal node values
intuitively reducing will reduce the correlation between any pair of trees in the ensemble and hence by reduce the variance of the average
bagging random forest and gradient boosting applied to the spam data
for boosting node trees were used and the number of trees were chosen by fold cross validation trees
each step in the figure corresponds to change in single misclassification in test set of
not all estimators can be improved by shaking up the data like this
it seems that highly nonlinear estimators such as trees benefit the most
for bootstrapped trees is typically small or lower is typical see figure while is not much larger than the variance for the original tree
on the other hand bagging does not change linear estimates such as the sample mean hence its variance either the pairwise correlation between bootstrapped means is about exercise
random forests random forests are popular
leo breiman's collaborator adele cutler maintains random forest website where the software is freely available with more than downloads reported by
there is randomforest package in maintained by andy liaw available from the cran website
the authors make grand claims about the success of random forests most accurate most interpretable and the like
in our experience random forests do remarkably well with very little tuning required
random forest classifier achieves misclassification error on the spam test data which compares well with all other methods and is not significantly worse than gradient boosting at
bagging achieves which is significantly worse than either using the mcnemar test outlined in exercise so it appears on this example the additional randomization helps
the results of simulations from the nested spheres model in ir
the bayes decision boundary is the surface of sphere additive
rf refers to random forest with and gbm gradient boosted model with interaction order six similarly for rf and gbm
the training sets were of size and the test sets
figure shows the test error progression on trees for the three methods
in this case there is some evidence that gradient boosting has started to overfit although fold cross validation chose all trees
sadly leo breiman died in july http www math usu edu adele forests
random forests compared to gradient boosting on the california housing data
the curves represent mean absolute error on the test data as function of the number of trees in the models
two random forests are shown with and
the two gradient boosted models use shrinkage parameter bd in and have interaction depths of and
the boosted models outperform random forests
figure shows the results of simulation comparing random forests to gradient boosting on the nested spheres problem equation in chapter
boosting easily outperforms random forests here
notice that smaller is better here although part of the reason could be that the true decision boundary is additive
figure compares random forests to boosting with shrinkage in regression problem using the california housing data section
boosting is slowed down by the shrinkage as well as the fact that the trees are much smaller
at terms the weaker boosting model gbm depth has smaller error than the stronger details the random forests were fit using the package randomforest with trees
the gradient boosting models were fit using package gbm with shrinkage parameter set to and trees
for larger the random forests performed no better
details of random forests we have glossed over the distinction between random forests for classification versus regression
when used for classification random forest obtains class vote from each tree and then classifies using majority vote see section on bagging for similar discussion
when used for regression the predictions from each tree at target point are simply averaged as in
in practice the best values for these parameters will depend on the problem and they should be treated as tuning parameters
in figure the performs much better than the default value
out of bag samples an important feature of random forests is its use of out of bag oob samples
details of random forests for each observation zi xi yi construct its random forest predictor by averaging only those trees corresponding to bootstrap samples in which zi did not appear
an oob error estimate is almost identical to that obtained by fold crossvalidation see exercise
hence unlike many other nonlinear estimators random forests can be fit in one sequence with cross validation being performed along the way
once the oob error stabilizes the training can be terminated
figure shows the oob misclassification error for the spam data compared to the test error
although trees are averaged here it appears from the plot that about would be sufficient
variable importance variable importance plots can be constructed for random forests in exactly the same way as they were for gradient boosted models section
at each split in each tree the improvement in the split criterion is the importance measure attributed to the splitting variable and is accumulated over all the trees in the forest separately for each variable
the left plot of figure shows the variable importances computed in this way for the spam data compare with the corresponding figure on page for gradient boosting
boosting ignores some variables completely while the random forest does not
the candidate split variable selection increases the chance that any single variable gets included in random forest while no such selection occurs with boosting
random forests also use the oob samples to construct different variableimportance measure apparently to measure the prediction strength of each variable
when the bth tree is grown the oob samples are passed down the tree and the prediction accuracy is recorded
then the values for the jth variable are randomly permuted in the oob samples and the accuracy is again computed
the decrease in accuracy as result of this permuting is averaged over all trees and is used as measure of the importance of variable in the random forest
these are expressed as percent of the maximum in the right plot in figure
although the rankings of the two methods are similar the importances in the right plot are more uniform over the variables
the randomization effectively voids the effect of variable much like setting coefficient to zero in linear model exercise
this does not measure the effect on prediction were this variable not available because if the model was refitted without the variable other variables could be used as surrogates
random forests gini randomization table table parts parts cs addresses addresses direct report cs direct make conference project original report conference telnet credit lab data project people technology telnet data lab font original credit address make people labs pm all address order order technology labs mail meeting font email mail over over receive receive pm re email internet all will will money internet meeting business business hpl hpl edu you re george you our our your money captot captot george hp edu capmax capmax your free capave hp free capave remove remove
variable importance plots for classification random forest grown on the spam data
the left plot bases the importance on the gini splitting index as in gradient boosting
the rankings compare well with the rankings produced by gradient boosting figure on page
the right plot uses oob randomization to compute variable importances and tends to spread the importances more uniformly
left proximity plot for random forest classifier grown to the mixture data
right decision boundary and training data for random forest on mixture data
six points have been identified in each plot
proximity plots one of the advertised outputs of random forest is proximity plot
figure shows proximity plot for the mixture data defined in section in chapter
in growing random forest an proximity matrix is accumulated for the training data
for every tree any pair of oob observations sharing terminal node has their proximity increased by one
this proximity matrix is then represented in two dimensions using multidimensional scaling section
the idea is that even though the data may be high dimensional involving mixed variables etc the proximity plot gives an indication of which observations are effectively close together in the eyes of the random forest classifier
proximity plots for random forests often look very similar irrespective of the data which casts doubt on their utility
they tend to have star shape one arm per class which is more pronounced the better the classification performance
since the mixture data are two dimensional we can map points from the proximity plot to the original coordinates and get better understanding of what they represent
it seems that points in pure regions class wise map to the extremities of the star while points nearer the decision boundaries map nearer the center
this is not surprising when we consider the construction of the proximity matrices
neighboring points in pure regions will often end up sharing bucket since when terminal node is pure it is no longer
random forests split by random forest tree growing algorithm
on the other hand pairs of points that are close but belong to different classes will sometimes share terminal node but not always
random forests and overfitting when the number of variables is large but the fraction of relevant variables small random forests are likely to perform poorly with small
at each split the chance can be small that the relevant variables will be selected
figure shows the results of simulation that supports this claim
details are given in the figure caption and exercise
at the top of each pair we see the hyper geometric probability that relevant variable will be selected at any split by random forest tree in this simulation the relevant variables are all equal in stature
as this probability gets small the gap between boosting and random forests increases
when the number of relevant variables increases the performance of random forests is surprisingly robust to an increase in the number of noise variables
for example with of relevant variable being relevant and noise variables the probability selected at any split is assuming
according to figure this does not hurt the performance of random forests compared with boosting
this robustness is largely due to the relative insensitivity of misclassification cost to the bias and variance of the probability estimates in each tree
we consider random forests for regression in the next section
another claim is that random forests cannot overfit the data
it is certainly true that increasing does not cause the random forest sequence to overfit like bagging the random forest estimate approximates the expectation rf lim rf with an average over realizations of
the distribution of here is conditional on the training data
however this limit can overfit the data the average of fully grown trees can result in too rich model and incur unnecessary variance
segal demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests
our experience is that using full grown trees seldom costs much and results in one less tuning parameter
figure shows the modest effect of depth control in simple regression example
classifiers are less sensitive to variance and this effect of overfitting is seldom seen with random forest classification
comparison of random forests and gradient boosting on problems with increasing numbers of noise variables
in each case the true decision boundary depends on two variables and an increasing number of noise variables are included
random forests uses its default value
at the top of each pair is the probability that one of the relevant variables is chosen at any split
the results are based on simulations for each pair with training sample of and test sample of
analysis of random forests in this section we analyze the mechanisms at play with the additional randomization employed by random forests
for this discussion we focus on regression and squared error loss since this gets at the main points and bias and variance are more complex with loss see section
furthermore even in the case of classification problem we can consider the random forest average as an estimate of the class posterior probabilities for which bias and variance are appropriate descriptors
variance and the de correlation effect the limiting form of the random forest regression estimator is rf where we have made explicit the dependence on the training data
here we consider estimation at single target point
from we see that
the effect of tree size on the error in random forest regression
in this example the true surface was additive in two of the variables plus additive unit variance gaussian noise
tree depth is controlled here by the minimum node size the smaller the minimum node size the deeper the trees
varf rf
it is easy to confuse with the average correlation between fitted trees in given random forest ensemble that is think of the fitted trees as vectors and compute the average pairwise correlation between these vectors conditioned on the data
this is not the case this conditional correlation is not directly relevant in the averaging process and the dependence on in warns us of the distinction
rather is the theoretical correlation between pair of random forest trees evaluated at induced by repeatedly making training sample draws from the population and then drawing pair of random forest trees
in statistical jargon this is the correlation induced by the sampling distribution of and
more precisely the variability averaged over in the calculations in and is both
in fact the conditional covariance of pair of tree fits at is zero because the bootstrap and feature sampling is see exercise
correlations between pairs of trees drawn by random forest regression algorithm as function of
the boxplots represent the correlations at randomly chosen prediction points
the following demonstrations are based on simulation model xj with all the xj and iid gaussian
we use training sets of size and single set of test locations of size
since regression trees are nonlinear in the patterns we see below will differ somewhat depending on the structure of the model
figure shows how the correlation between pairs of trees decreases as decreases pairs of tree predictions at for different training sets are likely to be less similar if they do not use the same splitting variables
in the left panel of figure we consider the variances of single tree predictors vart averaged over prediction points drawn randomly from our simulation model
this is the total variance and can be
random forests decomposed into two parts using standard conditional variance arguments see exercise var varz ez var total variance varz rf within variance the second term is the within variance result of the randomization which increases as decreases
the first term is in fact the sampling variance of the random forest ensemble shown in the right panel which decreases as decreases
the variance of the individual trees does not change appreciably over much of the range of hence in light of the variance of the ensemble is dramatically lower than this tree variance
simulation results
the left panel shows the average variance of single random forest tree as function of
within refers to the average within sample contribution to the variance resulting from the bootstrap sampling and split variable sampling
total includes the sampling variability of
the horizontal line is the average variance of single fully grown tree without bootstrap sampling
the right panel shows the average mean squared error squared bias and variance of the ensemble as function of
note that the variance axis is on the right same scale different level
the horizontal line is the average squared bias of fully grown tree
bias as in bagging the bias of random forest is the same as the bias of any of the individual sampled trees
analysis of random forests bias ez rf ez
this is also typically greater in absolute terms than the bias of an unpruned tree grown to since the randomization and reduced sample space impose restrictions
hence the improvements in prediction obtained by bagging or random forests are solely result of variance reduction
any discussion of bias depends on the unknown true function
figure right panel shows the squared bias for our additive model simulation estimated from the realizations
although for different models the shape and rate of the bias curves may differ the general trend is that as decreases the bias increases
shown in the figure is the mean squared error and we see classical bias variance trade off in the choice of
for all the squared bias of the random forest is greater than that for single tree horizontal line
these patterns suggest similarity with ridge regression section
ridge regression is useful in linear models when one has large number of variables with similarly sized coefficients ridge shrinks their coefficients toward zero and those of strongly correlated variables toward each other
although the size of the training sample might not permit all the variables to be in the model this regularization via ridge stabilizes the model and allows all the variables to have their say albeit diminished
random forests with small perform similar averaging
each of the relevant variables get their turn to be the primary split and the ensemble averaging reduces the contribution of any individual variable
since this simulation example is based on linear model in all the variables ridge regression achieves lower mean squared error about with df bb opt
adaptive nearest neighbors the random forest classifier has much in common with the nearest neighbor classifier section in fact weighted version thereof
since each tree is grown to maximal size for particular is the response value for one of the training samples
the tree growing algorithm finds an optimal path to that observation choosing the most informative predictors from those at its disposal
the averaging process assigns weights to these training responses which ultimately vote for the prediction
hence via the random forest voting mechanism those observations close to the target point get assigned weights an equivalent kernel which combine to form the classification decision
figure demonstrates the similarity between the decision boundary of nearest neighbors and random forests on the mixture data
we gloss over the fact that pure nodes are not split further and hence there can be more than one observation in terminal node
random forests versus nn on the mixture data
the axis oriented nature of the individual trees in random forest lead to decision regions with an axis oriented flavor
bibliographic notes random forests as described here were introduced by breiman although many of the ideas had cropped up earlier in the literature in different forms
notably ho introduced the term random forest and used consensus of trees grown in random subspaces of the features
the idea of using stochastic perturbation and averaging to avoid overfitting was introduced by kleinberg and later in kleinberg
amit and geman used randomized trees grown on image features for image classification problems
breiman introduced bagging precursor to his version of random forests
dietterich also proposed an improvement on bagging using additional randomization
his approach was to rank the top candidate splits at each node and then select from the list at random
he showed through simulations and real examples that this additional randomization improved over the performance of bagging
friedman and hall showed that sub sampling without replacement is an effective alternative to bagging
they showed that growing and averaging trees on samples of size is approximately equivalent in terms bias variance considerations to bagging while using smaller fractions of reduces the variance even further through decorrelation
there are several free software implementations of random forests
in this chapter we used the randomforest package in maintained by andy liaw available from the cran website
this allows both split variable selection as well as sub sampling
adele cutler maintains random forest website http www math usu edu adele forests where as of august the software written by leo breiman and adele cutler is freely
exercises available
their code and the name random forests is exclusively licensed to salford systems for commercial release
the weka machine learning archive http www cs waikato ac nz ml weka at waikato university new zealand offers free java implementation of random forests
exercises ex
derive the variance formula
this appears to fail if is negative diagnose the problem in this case
show that as the number of bootstrap samples gets large the oob error estimate for random forest approaches its fold cv error estimate and that in the limit the identity is exact
consider the simulation model used in figure mease and wyner
binary observations are generated with probabilities ee pr xj fb where and is some predefined even number
describe this probability surface and give the bayes error rate
suppose xi are iid
let and be two bootstrap realizations of the sample mean
show that the sampling corn relation corr
along the way derive var and the variance of the bagged mean bag
here is linear statistic bagging produces no reduction in variance for linear statistics
show that the sampling correlation between pair of randomforest trees at point is given by varz
varz ez var the term in the numerator is varz rf and the second term in the denominator is the expected conditional variance due to the randomization in random forests
fit series of random forest classifiers to the spam data to explore the sensitivity to the parameter
plot both the oob error as well as the test error against suitably chosen range of values for
random forests ex
suppose we fit linear regression model to observations with response yi and predictors xi xip
assume that all variables are standardized to have mean zero and standard deviation one
let rss be the mean squared residual on the training data and the estimated coefficient
denote by rssj the mean squared residual on the training data using the same but with the values for the jth variable randomly permuted before the predictions are calculated
show that ep rssj rss where ep denotes expectation with respect to the permutation distribution
argue that this is approximately true when the evaluations are done using an independent test set
this is page printer opaque this ensemble learning introduction the idea of ensemble learning is to build prediction model by combining the strengths of collection of simpler base models
we have already seen number of examples that fall into this category
bagging in section and random forests in chapter are ensemble methods for classification where committee of trees each cast vote for the predicted class
boosting in chapter was initially proposed as committee method as well although unlike random forests the committee of weak learners evolves over time and the members cast weighted vote
stacking section is novel approach to combining the strengths of number of fitted models
in fact one could characterize any dictionary method such as regression splines as an ensemble method with the basis functions serving the role of weak learners
bayesian methods for nonparametric regression can also be viewed as ensemble methods large number of candidate models are averaged with respect to the posterior distribution of their parameter settings
neal and zhang
ensemble learning can be broken down into two tasks developing population of base learners from the training data and then combining them to form the composite predictor
in this chapter we discuss boosting technology that goes step further it builds an ensemble model by conducting regularized and supervised search in high dimensional space of weak learners
ensemble learning an early example of learning ensemble is method designed for multiclass classification using error correcting output codes dietterich and bakiri ecoc
consider the class digit classification problem and the coding matrix given in table
part of bit error correcting coding matrix for the class digit classification problem
each column defines two class classification problem
digit
note that the th column of the coding matrix defines two class variable that merges all the original classes into two groups
the method works as follows
learn separate classifier for each of the two class problems defined by the columns of the coding matrix
at test point let be the predicted probability of one for the th response
define ck the discriminant function for the kth class where ck is the entry for row and column in table
each row of is binary code for representing that class
the rows have more bits than is necessary and the idea is that the redundant errorcorrecting bits allow for some inaccuracies and can improve performance
in fact the full code matrix above has minimum hamming distance of between any pair of rows
note that even the indicator response coding section is redundant since classes require only log bits for their unique representation
dietterich and bakiri showed impressive improvements in performance for variety of multiclass problems when classification trees were used as the base classifier
james and hastie analyzed the ecoc approach and showed that random code assignment worked as well as the optimally constructed error correcting codes
they also argued that the main benefit of the coding was in variance reduction as in bagging and random forests because the different coded problems resulted in different trees and the decoding step above has similar effect as averaging
the hamming distance between two vectors is the number of mismatches between corresponding entries
boosting and regularization paths boosting and regularization paths in section of the first edition of this book we suggested an analogy between the sequence of models produced by gradient boosting algorithm and regularized model fitting in high dimensional feature spaces
this was primarily motivated by observing the close connection between boosted version of linear regression and the lasso section
these connections have been pursued by us and others and here we present our current thinking in this area
we start with the original motivation which fits more naturally in this chapter on ensemble learning
penalized regression intuition for the success of the shrinkage strategy of gradient boosting page in chapter can be obtained by drawing analogies with penalized linear regression with large basis expansion
consider the dictionary of all possible terminal node regression trees tk that could be realized on the training data as basis functions in irp
the linear model is tk where card
suppose the coefficients are to be estimated by least squares
since the number of such trees is likely to be much larger than even the largest training data sets some form of regularization is required
let bb solve
fc xn xk fd min yi tk xi bb fe is function of the coefficients that generally penalizes larger values
examples are ridge regression lasso both covered in section
as discussed there the solution to the lasso problem with moderate to large bb tends to be sparse many of the bb
that is only small fraction of all possible trees enter the model
ensemble learning algorithm forward stagewise linear regression
initialize
set to some small constant and large
for to pn pk arg min yi xi xi
sign
pk
output fm tk
this seems reasonable since it is likely that only small fraction of all possible trees will be relevant in approximating any particular target function
however the relevant subset will be different for different targets
those coefficients that are not set to zero are shrunk by the lasso in that their absolute values are smaller than their corresponding least squares values bb
as bb increases the coefficients all shrink each one ultimately becoming zero
owing to the very large number of basis functions tk directly solving with the lasso penalty is not possible
however feasible forward stagewise strategy exists that closely approximates the effect of the lasso and is very similar to boosting and the forward stagewise algorithm
algorithm gives the details
although phrased in terms of tree basis functions tk the algorithm can be used with any set of basis functions
initially all coefficients are zero in line this corresponds to bb in
at each successive step the tree tk is selected that best fits the current residuals in line
its corresponding coefficient is then incremented or decremented by an infinitesimal amount in while all other coefficients are left unchanged
in principle this process could be iterated until either all the residuals are zero or
the latter case can occur if and at that point the coefficient values represent least squares solution
this corresponds to bb in
after applying algorithm with iterations many of the coefficients will be zero namely those that have yet to be incremented
the others will tend to have absolute values smaller than their corresponding least squares solution values
therefore this iteration solution qualitatively resembles the lasso with inversely related to bb
figure shows an example using the prostate data studied in chapter
here instead of using trees tk as basis functions we use the origi if there is in general no unique least squares value since infinitely many solutions will exist that fit the data perfectly
we can pick the minimum norm solution amongst these which is the unique lasso solution
profiles of estimated coefficients from linear regression for the shows the results from the lasso prostate data studied in chapter
the left panel for different values of the bound parameter
the right panel shows the results of the stagewise linear regression algorithm using consecutive steps of size nal variables xk themselves that is multiple linear regression model
the left panel displays the profiles of estimated coefficients from the lasso for different values of the bound parameter
the right panel shows the results of the stagewise algorithm with and
the left and right panels of figure are the same as figure and the left panel of figure respectively
the similarity between the two graphs is striking
in some situations the resemblance is more than qualitative
for example if all of the basis functions tk are mutually uncorrelated then as such that algorithm yields exactly the same solution as the lasso for bound parameter and likewise for all solutions along the path
of course tree based regressors are not uncorrelated
however the solution sets are also identical if the coefficients bb are all monotone functions of bb
this is often the case when the correlation between the variables is low
when the bb are not monotone in bb then the solution sets are not identical
the solution sets for algorithm tend to change less rapidly with changing values of the regularization parameter than those of the lasso
ensemble learning efron et al make the connections more precise by characterizing the exact solution paths in the limiting case
they show that the coefficient paths are piece wise linear functions both for the lasso and forward stagewise
this facilitates efficient algorithms which allow the entire paths to be computed with the same cost as single least squares fit
this least angle regression algorithm is described in more detail in section
hastie et al show that this infinitesimal forward stagewise algorithm fs fits monotone version of the lasso which optimally reduces at each step the loss function for given increase in the arc length of the coefficient path see sections and
the arc length for the case is and hence proportional to the number of steps
tree boosting algorithm with shrinkage closely resembles algorithm with the learning rate parameter bd corresponding to
for squared error loss the only difference is that the optimal tree to be selected at each iteration tk is approximated by the standard top down greedy tree induction algorithm
for other loss functions such as the exponential loss of adaboost and the binomial deviance rosset et al show similar results to what we see here
thus one can view tree boosting with shrinkage as form of monotone ill posed regression on all possible jterminal node trees with the lasso penalty as regularizer
we return to this topic in section
the choice of no shrinkage bd in equation is analogous to forward stepwise regression and its more aggressive cousin best subset selection which penalizes the number of non zero coefficients
with small fraction of dominant variables best subset approaches often work well
but with moderate fraction of strong variables it is well known that subset selection can be excessively greedy copas often yielding poor results when compared to less aggressive strategies such as the lasso or ridge regression
the dramatic improvements often seen when shrinkage is used with boosting are yet another confirmation of this approach
the bet on sparsity principle as shown in the previous section boosting's forward stagewise strategy with shrinkage approximately minimizes the same loss function with lasso style penalty
the model is built up slowly searching through model space and adding shrunken basis functions derived from important predictors
in contrast the penalty is computationally much easier to deal with as shown in section
with the basis functions and penalty chosen to match particular positive definite kernel one can solve the corresponding optimization problem without explicitly searching over individual basis functions
however the sometimes superior performance of boosting over procedures such as the support vector machine may be largely due to the implicit use of the versus penalty
the shrinkage resulting from the
boosting and regularization paths penalty is better suited to sparse situations where there are few basis functions with nonzero coefficients among all possible choices
we can strengthen this argument through simple example taken from friedman et al
suppose we have data points and our model is linear combination of million trees
if the true population coefficients of these trees arose from gaussian distribution then we know that in bayesian sense the best predictor is ridge regression exercise
that is we should use an rather than an penalty when fitting the coefficients
on the other hand if there are only small number coefficients that are nonzero the lasso penalty will work better
we think of this as sparse scenario while the first case gaussian coefficients is dense
note however that in the dense scenario although the penalty is best neither method does very well since there is too little data from which to estimate such large number of nonzero coefficients
this is the curse of dimensionality taking its toll
in sparse setting we can potentially do well with the penalty since the number of nonzero coefficients is small
the penalty fails again
in other words use of the penalty follows what we call the bet on sparsity principle for high dimensional problems use procedure that does well in sparse problems since no procedure does well in dense problems
larger training sets allow us to estimate coefficients with smaller standard errors
likewise in situations with small nsr we can identify more nonzero coefficients with given sample size than in situations where the nsr is larger
increasing the size of the dictionary may lead to sparser representation for our function but the search problem becomes more difficult leading to higher variance
figure illustrates these points in the context of linear models using simulation
we compare ridge regression and lasso both for classification and regression problems
each run has observations with independent gaussian predictors
in the top row all coefficients are nonzero generated from gaussian distribution
in the middle row only are nonzero and generated from gaussian and the last row has non zero gaussian coefficients
for regression standard gaussian noise is
simulations that show the superiority of the lasso penalty over ridge in regression and classification
each run has observations with independent gaussian predictors
in the top row all coefficients are nonzero generated from gaussian distribution
in the middle row only are nonzero and the last row has nonzero
gaussian errors are added to the linear predictor for the regression problems and binary responses generated via the inverse logit transform for the classification problems
scaling of resulted in the noise to signal ratios shown
lasso is used in the left sub columns ridge in the right
we report the optimal percentage of error explained on test data relative to the error of constant model displayed as boxplots over realizations for each combination
in the only situation where ridge beats lasso top row neither do well
boosting and regularization paths added to the linear predictor to produce continuous response
for classification the linear predictor is transformed via the inverselogit to probability and binary response is generated
five different noise to signal ratios are presented obtained by scaling prior to generating the response
in both cases this is defined to be nsr var var
both the ridge regression and lasso coefficient paths were fit using series of values of bb corresponding to range of df from to see chapter for details
the models were evaluated on large test set infinite for gaussian for binary and in each case the value for bb was chosen to minimize the test set error
we report percentage variance explained for the regression problems and percentage misclassification error explained for the classification problems relative to baseline error of
there are simulation runs for each scenario
note that for the classification problems we are using squared error loss to fit the binary response
note also that we do not using the training data to select bb but rather are reporting the best possible behavior for each method in the different scenarios
the penalty performs poorly everywhere
the lasso performs reasonably well in the only two situations where it can sparse coefficients
as expected the performance gets worse as the nsr increases less so for classification and as the model becomes denser
the differences are less marked for classification than for regression
these empirical results are supported by large body of theoretical results donoho and johnstone donoho and elad donoho candes and tao that support the superiority of estimation in sparse settings
regularization paths over fitting and margins it has often been observed that boosting does not overfit or more astutely is slow to overfit
part of the explanation for this phenomenon was made earlier for random forests misclassification error is less sensitive to variance than is mean squared error and classification is the major focus in the boosting community
in this section we show that the regularization paths of boosted models are well behaved and that for certain loss functions they have an appealing limiting form
figure shows the coefficient paths for lasso and infinitesimal forward stagewise fs in simulated regression setting
the data consists of dictionary of gaussian variables strongly correlated within blocks of but uncorrelated between blocks
the generating model has nonzero coefficients for variables one drawn from each block and the coefficient values are drawn from standard gaussian
finally gaussian noise is added with noise to signal ratio of exercise
the fs algorithm is limiting form of algorithm where the step size is shrunk to zero section
the grouping of the variables is intended to mimic the correlations of nearby trees and with the forward stagewise
comparison of lasso and infinitesimal forward stagewise paths on simulated regression data
the number of samples is and the number of variables is
the forward stagewise paths fluctuate less than those of lasso in the final stages of the algorithms algorithm this setup is intended as an idealized version of gradient boosting with shrinkage
for both these algorithms the coefficient paths can be computed exactly since they are piecewise linear see the lars algorithm in section
here the coefficient profiles are similar only in the early stages of the paths
for the later stages the forward stagewise paths tend to be monotone and smoother while those for the lasso fluctuate widely
this is due to the strong correlations among subsets of the variables lasso suffers somewhat from the multi collinearity problem exercise
the performance of the two models is rather similar figure and they achieve about the same minimum
in the later stages forward stagewise takes longer to overfit likely consequence of the smoother paths
hastie et al show that fs solves monotone version of the lasso problem for squared error loss
let be the augmented dictionary obtained by including apnegative copy of every basis element in
we consider models tk tk with non negative coefficients
in this expanded space the lasso coefficient paths are positive while those of fs are monotone nondecreasing
the monotone lasso path is characterized by differential equation ml
mean squared error for lasso and infinitesimal forward stagewise on the simulated data
despite the difference in the coefficient paths the two models perform similarly over the critical part of the regularization path
in the right tail lasso appears to overfit more rapidly with initial condition where is the arc length of the path exercise
the monotone lasso move direction velocity vector ml decreases the loss at the optimal quadratic rate per unit increase in the arc length of the path
since ml the solution paths are monotone
the lasso can similarly be characterized as the solution to differential equation as in except that the move directions decrease the loss optimally per unit increase in the norm of the path
as consequence they are not necessarily positive and hence the lasso paths need not be monotone
in this augmented dictionary restricting the coefficients to be positive is natural since it avoids an obvious ambiguity
it also ties in more naturally with tree boosting we always find trees positively correlated with the current residual
there have been suggestions that boosting performs well for two class classification because it exhibits maximal margin properties much like the support vector machines of chapters and
schapire et al define the normalized margin of fitted model tk as yi xi min pk
here the minimum is taken over the training sample and yi
unlike the margin of support vector machines the margin measures the distance to the closest training point in units maximum coordinate distance
the left panel shows the margin for the adaboost classifier on the mixture data as function of the number of node trees
the model was fit using the package gbm with shrinkage factor of
after trees has settled down
note that when the margin crosses zero the training error becomes zero
the right panel shows the test error which is minimized at trees
in this case adaboost overfits dramatically if run to convergence
schapire et al prove that with separable data adaboost increases with each iteration converging to margin symmetric solution
ra tsch and warmuth prove the asymptotic convergence of adaboost with shrinkage to margin maximizing solution
rosset et al consider regularized models of the form for general loss functions
they show that as bb for particular loss functions the solution converges to margin maximizing configuration
in particular they show this to be the case for the exponential loss of adaboost as well as binomial deviance
collecting together the results of this section we reach the following summary for boosted classifiers the sequence of boosted classifiers form an regularized monotone path to margin maximizing solution
of course the margin maximizing end of the path can be very poor overfit solution as it is in the example in figure
early stopping amounts to picking point along the path and should be done with the aid of validation dataset
learning ensembles the insights learned from the previous sections can be harnessed to produce more effective and efficient ensemble model
again we consider functions
learning ensembles of the form tk tk where is dictionary of basis functions typically trees
for gradient boosting and random forests is very large and it is quite typical for the final model to involve many thousands of trees
in the previous section we argue that gradient boosting with shrinkage fits an regularized monotone path in this space of trees
in its simplest form this model could be seen as way of post processing boosting or random forests taking for tl the collection of trees produced by the gradient boosting or random forest algorithms
by fitting the lasso path to these trees we would typically use much reduced set which would save in computations and storage for future predictions
in the next section we describe modifications of this prescription that reduce the correlations in the ensemble tl and improve the performance of the lasso post processor
as an initial illustration we apply this procedure to random forest ensemble grown on the spam data
figure shows that lasso post processing offers modest improvement over the random forest blue curve and reduces the forest to about trees rather than the original
the post processed performance matches that of gradient boosting
the orange curves represent modified version of random forests designed to reduce the correlations between trees even more
here random sub sample without replacement of of the training sample is used to grow each tree and the trees are restricted to be shallow about six terminal nodes
the post processing offers more dramatic improvements here and the training costs are reduced by factor of about
however the performance of the post processed model falls somewhat short of the blue curves
learning good ensemble not all ensembles tl will perform well with post processing
in terms of basis functions we want collection that covers the space well in places
application of the lasso post processing to the spam data
the horizontal blue line is the test error of random forest fit to the spam data using trees grown to maximum depth with see algorithm
the jagged blue curve is the test error after post processing the first trees using the lasso as function of the number of trees with nonzero coefficients
the orange curve line use modified form of random forest where random draw of of the data are used to grow each tree and the trees are forced to be shallow typically six terminal nodes
here the post processing offers much greater improvement over the random forest that generated the ensemble where they are needed and are sufficiently different from each other for the post processor to be effective
friedman and popescu gain insights from numerical quadrature and importance sampling
they view the unknown function as an integral where indexes the basis functions
for example if the basis functions are trees then indexes the splitting variables the split points and the values in the terminal nodes
numerical quadrature amounts to pm points and corresponding weights finding set of evaluation so that fm approximates well over the domain of
importance sampling amounts to sampling at random but giving more weight to relevant regions of the space
friedman and popescu suggest measure of lack of relevance that uses the loss function
learning ensembles min yi xi evaluated on the training data
if single basis function were to be selected tree it would be the global minimizer arg min
introducing randomness in the selection of would necessarily produce less optimal values with
they propose natural measure of the characteristic width of the sampling scheme es
friedman and popescu use sub sampling as mechanism for introducing randomness leading to their ensemble generation algorithm
algorithm isle ensemble generation
pn arg minc yi
for to do arg min sm yi fm xi xi fm fm bd
tisle
sm refers to subsample of of the training observ simulations suggest picking vations typically without replacement
their and for large picking
reducing increases the randomness and hence the width
the parameter bd introduces memory into the randomization process the larger bd the more the procedure avoids similar to those found before
number of familiar randomization schemes are special cases of algorithm bagging has but samples with replacement and has bd
friedman and hall argue that sampling without replacement with is equivalent to sampling with replacement with and the former is much more efficient
ensemble learning random forest sampling is similar with more randomness introduced by the selection of the splitting variable
reducing in algorithm has similar effect to reducing in random forests but does not suffer from the potential biases discussed in section
gradient boosting with shrinkage uses but typically does not produce sufficient width
stochastic gradient boosting friedman follows the recipe exactly
the authors recommend values bd and and call their combined procedure ensemble generation and post processing importance sampled learning ensemble isle
figure shows the performance of an isle on the spam data
importance sampling learning ensemble isle fit to the spam data
here we used bd and trees with five terminal nodes
the lasso post processed ensemble does not improve the prediction error in this case but it reduces the number of trees by factor of five not improve the predictive performance but is able to produce more parsimonious model
note that in practice the post processing includes the selection of the regularization parameter bb in which would be
learning ensembles chosen by cross validation
here we simply demonstrate the effects of postprocessing by showing the entire path on the test data
figure shows various isles on regression example
demonstration of ensemble methods on regression simulation example
the notation gbm refers to gradient boosted model with parameters bd
we report mean squared error from the true known function
note that the sub sampled gbm model green outperforms the full gbm model orange
the lasso post processed version achieves similar error
the random forest is outperformed by its post processed version but both fall short of the other models function is xj xj where the last elements are noise variables
the response where we chose resulting in signal to noise ratio of approximately
we used training sample of size and estimated the mean squared error by averaging over test set of samples
the sub sampled gbm curve light blue is an instance of stochastic gradient boosting friedman discussed in section and it outperforms gradient boosting on this example
ensemble learning rule ensembles here we describe modification of the tree ensemble method that focuses on individual rules friedman and popescu
we encountered rules in section in the discussion of the prim method
the idea is to enlarge an ensemble of trees by constructing set of rules from each of the trees in the collection
typical tree in an ensemble from which rules can be derived
figure depicts small tree with numbered nodes
the following rules can be derived from this tree linear expansion in rules and is equivalent to the tree itself exercise hence is an over complete basis for the tree
for each tree tm in an ensemble we can construct its mini ensemble of rules trule and then combine them all to form larger ensemble trule trule
this is then treated like any other ensemble and post processed via the lasso or similar regularized procedure
mean squared error for rule ensembles using realizations of the simulation example
friedman and popescu demonstrate the power of this procedure on number of illustrative examples including the simulation example
figure shows boxplots of the mean squared error from the true model for twenty realizations from this model
the models were all fit using the rulefit software available on the esl homepage which runs in an automatic mode
on the same training set as used in figure the rule based model achieved mean squared error of
although slightly worse than the best achieved in that figure the results are not comparable because crossvalidation was used here to select the final model
bibliographic notes as noted in the introduction many of the new methods in machine learning have been dubbed ensemble methods
these include neural networks boosting bagging and random forests dietterich gives survey of tree based ensemble methods
neural networks chapter are perhaps more deserving of the name since they simultaneously learn the parameters esl homepage www stat stanford edu elemstatlearn
ensemble learning of the hidden units basis functions along with how to combine them
bishop discusses neural networks in some detail along with the bayesian perspective mackay neal
support vector machines chapter can also be regarded as an ensemble method they perform regularized model fitting in high dimensional feature spaces
boosting and lasso exploit sparsity through regularization to overcome the highdimensionality while svms rely on the kernel trick characteristic of regularization
quinlan is commercial tree and rule generation package with some goals in common with rulefit
there is vast and varied literature often referred to as combining classifiers which abounds in ad hoc schemes for mixing methods of different types to achieve better performance
for principled approach see kittler et al
exercises ex
describe exactly how to generate the block correlated data used in the simulation in section
let irp be piecewise differentiable and continuous coefficient profile with
the arc length of from time to is defined by dt
show that with equality iff is monotone
show that fitting linear regression model using rules and in equation gives the same fit as the regression tree corresponding to this tree
show the same is true for classification if logistic regression model is fit
program and run the simulation study described in figure
this is page printer opaque this undirected graphical models introduction graph consists of set of vertices nodes along with set of edges joining some pairs of the vertices
in graphical models each vertex represents random variable and the graph gives visual way of understanding the joint distribution of the entire set of random variables
they can be useful for either unsupervised or supervised learning
in an undirected graph the edges have no directional arrows
we restrict our discussion to undirected graphical models also known as markov random fields or markov networks
in these graphs the absence of an edge between two vertices has special meaning the corresponding random variables are conditionally independent given the other variables
figure shows an example of graphical model for flow cytometry dataset with proteins measured on cells from sachs et al
each vertex in the graph corresponds to the real valued expression level of protein
the network structure was estimated assuming multivariate gaussian distribution using the graphical lasso procedure discussed later in this chapter
sparse graphs have relatively small number of edges and are convenient for interpretation
they are useful in variety of domains including genomics and proteomics where they provide rough models of cell pathways
much work has been done in defining and understanding the structure of graphical models see the bibliographic notes for references
example of sparse undirected graph estimated from flow cytometry dataset with proteins measured on cells
the network structure was estimated using the graphical lasso procedure discussed in this chapter
as we will see the edges in graph are parametrized by values or potentials that encode the strength of the conditional dependence between the random variables at the corresponding vertices
the main challenges in working with graphical models are model selection choosing the structure of the graph estimation of the edge parameters from data and computation of marginal vertex probabilities and expectations from their joint distribution
the last two tasks are sometimes called learning and inference in the computer science literature
we do not attempt comprehensive treatment of this interesting area
instead we introduce some basic concepts and then discuss few simple methods for estimation of the parameters and structure of undirected graphical models methods that relate to the techniques already discussed in this book
the estimation approaches that we present for continuous and discrete valued vertices are different so we treat them separately
sections and may be of particular interest as they describe new regression based procedures for estimating graphical models
there is large and active literature on directed graphical models or bayesian networks these are graphical models in which the edges have directional arrows but no directed cycles
directed graphical models represent probability distributions that can be factored into products of conditional distributions and have the potential for causal interpretations
we refer the reader to wasserman for brief overview of both undirected and directed graphs the next section follows closely his chapter
examples of undirected graphical models or markov networks
each node or vertex represents random variable and the lack of an edge between two nodes indicates conditional independence
for example in graph and are conditionally independent given
in graph is independent of each of and
longer list of useful references is given in the bibliographic notes on page
markov graphs and their properties in this section we discuss the basic properties of graphs as models for the joint distribution of set of random variables
we defer discussion of parametrization and estimation of the edge parameters from data and estimation of the topology of graph to later sections
figure shows four examples of undirected graphs
graph consists of pair where is set of vertices and the set of edges defined by pairs of vertices
two vertices and are called adjacent if there is edge joining them this is denoted by
path xn is set of vertices that are joined that is xi xi for
complete graph is graph with every pair of vertices joined by an edge
subgraph is subset of vertices together with their edges
for example in figure form path but not complete graph
suppose that we have graph whose vertex set represents set of random variables having joint distribution
in markov graph the absence of an edge implies that the corresponding random variables are conditionally independent given the variables at the other vertices
this is expressed with the following notation
undirected graphical models no edge joining and rest where rest refers to all of the other vertices in the graph
for example in figure
these are known as the pairwise markov independencies of
if and are subgraphs then is said to separate and if every path between and intersects node in
for example separates and in figures and and separates and in
in figure is not connected to so we say that the two sets are separated by the empty set
in figure separates and
separators have the nice property that they break the graph into conditionally independent pieces
specifically in markov graph with subgraphs and if separates and then
these are known as the global markov properties of
it turns out that the pairwise and global markov properties of graph are equivalent for graphs with positive distributions
that is the set of graphs with associated probability distributions that satisfy the pairwise markov independencies and global markov assumptions are the same
this result is useful for inferring global independence relations from simple pairwise properties
for example in figure since it is markov graph and there is no link joining and
but also separates from and and hence by the global markov assumption we conclude that and
similarly we have
the global markov property allows us to decompose graphs into smaller more manageable pieces and thus leads to essential simplifications in computation and interpretation
for this purpose we separate the graph into cliques
clique is complete subgrapha set of vertices that are all adjacent to one another it is called maximal if it is clique and no other vertices can be added to it and still yield clique
the maximal cliques for the graphs of figure are and
although the following applies to both continuous and discrete distributions much of the development has been for the latter
probability density function over markov graph can be can represented as
markov graphs and their properties xc where is the set of maximal cliques and the positive functions are called clique potentials
these are not in general density functions but rather are affinities that capture the dependence in xc by scoring certain instances xc higher than others
the quantity xc is the normalizing constant also known as the partition function
alternatively the representation implies graph with independence properties defined by the cliques in the product
this result holds for markov networks with positive distributions and is known as the hammersleyclifford theorem hammersley and clifford clifford
many of the methods for estimation and computation on graphs first decompose the graph into its maximal cliques
relevant quantities are computed in the individual cliques and then accumulated across the entire graph
prominent example is the join tree or junction tree algorithm for computing marginal and low order probabilities from the joint distribution on graph
details can be found in pearl lauritzen and spiegelhalter pearl shenoy and shafer jensen et al or koller and friedman
complete graph does not uniquely specify the higher order dependence structure in the joint distribution of the variables
graphical model does not always uniquely specify the higher order dependence structure of joint probability distribution
consider the complete three node graph in figure
it could represent the dependence structure of either of the following distributions
the first specifies only second order dependence and can be represented with fewer parameters
graphical models for discrete data are special if the cliques are separated then the potentials can be densities but this is in general not the case
undirected graphical models case of loglinear models for multiway contingency tables bishop et al in that language is referred to as the no second order interaction model
for the remainder of this chapter we focus on pairwise markov graphs koller and friedman
here there is potential function for each edge pair of variables as in above and at most second order interactions are represented
these are more parsimonious in terms of parameters easier to work with and give the minimal complexity implied by the graph structure
the models for both continuous and discrete data are functions of only the pairwise marginal distributions of the variables represented in the edge set
undirected graphical models for continuous variables here we consider markov networks where all the variables are continuous
the gaussian distribution is almost always used for such graphical models because of its convenient analytical properties
we assume that the observations have multivariate gaussian distribution with mean and covariance matrix
since the gaussian distribution represents at most second order relationships it automatically encodes pairwise markov graph
the graph in figure is an example of gaussian graphical model
the gaussian distribution has the property that all conditional distributions are also gaussian
the inverse covariance matrix contains information about the partial covariances between the variables that is the covariances between pairs and conditioned on all other variables
in particular if the ijth component of is zero then variables and are conditionally independent given the other variables exercise
it is instructive to examine the conditional distribution of one variable versus the rest where the role of is explicit
suppose we partition where xp consists of the first variables and xp is the last
then we have the conditional distribution of give mardia et al
y z zz zy zy zz zy where we have partitioned as zz zy
zy the conditional mean in has exactly the same form as the population multiple linear regression of on with regression coefficient zz zy see on page
if we partition in the same way since standard formulas for partitioned inverses give
undirected graphical models for continuous variables zy zz zy where zy zz zy
hence zz zy zy
here we see explicitly that zero elements in and hence zy mean that the corresponding elements of are conditionally independent of given the rest
thus captures all the second order information both structural and quantitative needed to describe the conditional distribution of each node given the rest and is the so called natural parameter for the gaussian graphical model
another different kind of graphical model is the covariance graph or relevance network in which vertices are connected by bidirectional edges if the covariance rather than the partial covariance between the corresponding variables is nonzero
these are popular in genomics see especially butte et al
the negative log likelihood from these models is not convex making the computations more challenging chaudhuri et al
estimation of the parameters when the graph structure is known given some realizations of we would like to estimate the parameters of an undirected graph that approximates their joint distribution
suppose first that the graph is complete fully connected
we assume that we have multivariate normal realizations xi with population mean and covariance
let xi xi be the empirical covariance matrix with the sample mean vector
ignoring constants the log likelihood of the data can be written as the distribution arising from gaussian graphical model is wishart distribution
this is member of the exponential family with canonical or natural parameter
indeed the partially maximized log likelihood is up to constants the wishart log likelihood
undirected graphical models log det trace
in we have partially maximized with respect to the mean parameter
the quantity is convex function of
it is easy to show that the maximum likelihood estimate of is simply
now to make the graph more useful especially in high dimensional settings let's assume that some of the edges are missing for example the edge between pip and erk is one of several missing in figure
as we have seen for the gaussian distribution this implies that the corresponding entries of are zero
hence we now would like to maximize under the constraints that some pre defined subset of the parameters are zero
this is an equality constrained convex optimization problem and number of methods have been proposed for solving it in particular the iterative proportional fitting procedure speed and kiiveri
this and other methods are summarized for example in whittaker and lauritzen
these methods exploit the simplifications that arise from decomposing the graph into its maximal cliques as described in the previous section
here we outline simple alternate approach that exploits the sparsity in different way
the fruits of this approach will become apparent later when we discuss the problem of estimation of the graph structure
the idea is based on linear regression as inspired by and
in particular suppose that we want to estimate the edge parameters ij for the vertices that are joined to given vertex restricting those that are not joined to be zero
then it would seem that the linear regression of the node values on the other relevant vertices might provide reasonable estimate
but this ignores the dependence structure among the predictors in this regression
it turns out that if instead we use our current model based estimate of the cross product matrix of the predictors when we perform our regressions this gives the correct solutions and solves the constrained maximum likelihood problem exactly
we now give details
to constrain the log likelihood we add lagrange constants for all missing edges log det trace jk jk
the gradient equation for maximizing can be written as using the fact that the derivative of log det equals boyd and vandenberghe for example page
is matrix of lagrange parameters with nonzero values for all pairs with edges absent
we will show how we can use regression to solve for and its inverse one row and column at time
for simplicity let's focus on the last row and column
then the upper right block of equation can be written as
undirected graphical models for continuous variables
here we have partitioned the matrices into two parts as in part being the first rows and columns and part the pth row and column
with and its inverse partitioned in similar fashion we have
this implies where as in
now substituting into gives
these can be interpreted as the estimating equations for the constrained regression of xp on the other predictors except that the observed mean cross products matrix is replaced by the current estimated covariance matrix from the model
now we can solve by simple subset regression
suppose there are nonzero elements in edges constrained to be zero
these rows carry no information and can be removed
furthermore we can reduce to by removing its zero elements yielding the reduced system of equations with solution
this is padded with zeros to give
although it appears from that we only recover the elements up to scale factor it is easy to show that using partitioned inverse formulas
also since the diagonal of in is zero
this leads to the simple iterative procedure given in algorithm for estimating both and its inverse subject to the constraints of the missing edges
note that this algorithm makes conceptual sense
the graph estimation problem is not separate regression problems but rather coupled problems
the use of the common in step in place of the observed cross products matrix couples the problems together in the appropriate fashion
surprisingly we were not able to find this procedure in the literature
however it is related to the covariance selection procedures of
undirected graphical models algorithm modified regression algorithm for estimation of an undirected gaussian graphical model with known structure
initialize
repeat for until convergence partition the matrix into part all but the jth row and column and part the jth row and column
solve for the unconstrained edge parameters using the reduced system of equations as in
obtain by padding with zeros in the appropriate positions update
in the final cycle for each solve for with
simple graph for illustration along with the empirical covariance matrix
dempster and is similar in flavor to the iterative conditional fitting procedure for covariance graphs proposed by chaudhuri et al
here is little example borrowed from whittaker
suppose that our model is as depicted in figure along with its empirical covariance matrix
we apply algorithm to this problem for example in the modified regression for variable in step variable is left out
the procedure quickly converged to the solutions eb eb ec ec ed ed
note the zeroes in corresponding to the missing edges and
note also that the corresponding elements in are the only elements different from
the estimation of is an example of what is sometimes called the positive definite completion of
undirected graphical models for continuous variables estimation of the graph structure in most cases we do not know which edges to omit from our graph and so would like to try to discover this from the data itself
in recent years number of authors have proposed the use of lasso regularization for this purpose
meinshausen and bu hlmann take simple approach to the problem rather than trying to fully estimate or they only estimate which components of ij are nonzero
to do this they fit lasso regression using each variable as the response and the others as predictors
the component ij is then estimated to be nonzero if either the estimated coefficient of variable on is nonzero or the estimated coefficient of variable on is nonzero alternatively they use an and rule
they show that asymptotically this procedure consistently estimates the set of nonzero elements of
we can take more systematic approach with the lasso penalty following the development of the previous section
consider maximizing the penalized log likelihood log det trace bb where is the norm the sum of the absolute values of the elements of and we have ignored constants
the negative of this penalized likelihood is convex function of
it turns out that one can adapt the lasso to give the exact maximizer of the penalized log likelihood
in particular we simply replace the modified regression step in algorithm by modified lasso step
here are the details
the analog of the gradient equation is now bb sign
here we use sub gradient notation with sign jk sign jk if jk else sign jk if jk
continuing the development in the previous section we reach the analog of bb sign recall that and have opposite signs
we will now see that this system is exactly equivalent to the estimating equations for lasso regression
consider the usual regression setup with outcome variables and predictor matrix
there the lasso minimizes bb see on page here we have added factor for convenience
the gradient of this expression is
undirected graphical models algorithm graphical lasso
initialize bb
the diagonal of remains unchanged in what follows
repeat for until convergence partition the matrix into part all but the jth row and column and part the jth row and column solve the estimating equations bb sign using the cyclical coordinate descent algorithm for the modified lasso update
in the final cycle for each solve for with
zt zt bb sign so up to factor zt is the analog of and we replace zt by the estimated cross product matrix from our current model
the resulting procedure is called the graphical lasso proposed by friedman et al building on the work of banerjee et al
it is summarized in algorithm
friedman et al use the pathwise coordinate descent method section to solve the modified lasso problem at each stage
here are the details of pathwise coordinate descent for the graphical lasso algorithm
letting the update has the form vkj bb vjj for where is the soft threshold operator sign
the procedure cycles through the predictors until convergence
it is easy to show that the diagonal elements wjj of the solution matrix are simply sjj bb and these are fixed in step of algorithm
the graphical lasso algorithm is extremely fast and can solve moderately sparse problem with nodes in less than minute
it is easy to modify the algorithm to have edge specific penalty parameters bb jk since an alternative formulation of the problem can be posed where we don't penalize the diagonal of
then the diagonal elements wjj of the solution matrix are sjj and the rest of the algorithm is unchanged
undirected graphical models for continuous variables bb jk will force jk to be zero this algorithm subsumes algorithm
by casting the sparse inverse covariance problem as series of regressions one can also quickly compute and examine the solution paths as function of the penalty parameter bb
more details can be found in friedman et al
four different graphical lasso solutions for the flow cytometry data
figure shows the result of applying the graphical lasso to the flowcytometry dataset
here the lasso penalty parameter bb was set at
in practice it is informative to examine the different sets of graphs that are obtained as bb is varied
figure shows four different solutions
the graph becomes more sparse as the penalty parameter is increased
finally note that the values at some of the nodes in graphical model can be unobserved that is missing or hidden
if only some values are missing at node the em algorithm can be used to impute the missing values
undirected graphical models exercise
however sometimes the entire node is hidden or latent
in the gaussian model if node has all missing values due to linearity one can simply average over the missing nodes to yield another gaussian model over the observed nodes
hence the inclusion of hidden nodes does not enrich the resulting model for the observed nodes in fact it imposes additional structure on its covariance matrix
however in the discrete model described next the inherent nonlinearities make hidden units powerful way of expanding the model
undirected graphical models for discrete variables undirected markov networks with all discrete variables are popular and in particular pairwise markov networks with binary variables being the most common
they are sometimes called ising models in the statistical mechanics literature and boltzmann machines in the machine learning literature where the vertices are referred to as nodes or units and are binary valued
in addition the values at each node can be observed visible or unobserved hidden
the nodes are often organized in layers similar to neural network
boltzmann machines are useful both for unsupervised and supervised learning especially for structured input data such as images but have been hampered by computational difficulties
figure shows restricted boltzmann machine discussed later in which some variables are hidden and only some pairs of nodes are connected
we first consider the simpler case in which all nodes are visible with edge pairs enumerated in
denoting the binary valued variable at node by xj the ising model for their joint probabilities is given by exp jk xj xk for with
as with the gaussian model of the previous section only pairwise interactions are modeled
the ising model was developed in statistical mechanics and is now used more generally to model the joint effects of pairwise interactions
is the log of the partition function and is defined by xh log exp jk xj xk
the partition function ensures that the probabilities add to one over the sample space
the terms jk xj xk represent particular parametrization
undirected graphical models for discrete variables of the log potential functions and for technical reasons requires constant node to be included exercise with edges to all the other nodes
in the statistics literature this model is equivalent to first order interaction poisson log linear model for multiway tables of counts bishop et al mccullagh and nelder agresti
the ising model implies logistic form for each node conditional on the others exercise pr xj exp jk xk where denotes all of the nodes except
hence the parameter jk measures the dependence of xj on xk conditional on the other nodes
estimation of the parameters when the graph structure is known given some data from this model how can we estimate the parameters
suppose we have observations xi xi xi xip
the log likelihood is log pr xi xi ee jk xij xik fb the gradient of the log likelihood is xij xik jk jk and xj xk jk xj xk setting the gradient to zero gives xj xk xj xk where we have defined
undirected graphical models xj xk xij xik the expectation taken with respect to the empirical distribution of the data
looking at we see that the maximum likelihood estimates simply match the estimated inner products between the nodes to their observed inner products
this is standard form for the score gradient equation for exponential family models in which sufficient statistics are set equal to their expectations under the model
to find the maximum likelihood estimates we can use gradient search or newton methods
however the computation of xj xk involves enumeration of over of the possible values of and is not generally feasible for large larger than about
for smaller number of standard statistical approaches are available poisson log linear modeling where we treat the problem as large regression problem exercise
the response vector is the vector of counts in each of the cells of the multiway tabulation of the data
the predictor matrix has rows and up to columns that characterize each of the cells although this number depends on the sparsity of the graph
the computational cost is essentially that of regression problem of this size which is and is manageable for
the newton updates are typically computed by iteratively reweighted least squares and the number of steps is usually in the single digits
see agresti and mccullagh and nelder for details
standard software such as the package glm can be used to fit this model
gradient descent requires at most computations to compute the gradient but may require many more gradient steps than the second order newton methods
nevertheless it can handle slightly larger problems with
these computations can be reduced by exploiting the special clique structure in sparse graphs using the junction tree algorithm
details are not given here
iterative proportional fitting ipf performs cyclical coordinate descent on the gradient equations
at each step parameter is updated so that its gradient equation is exactly zero
this is done in cyclical fashion until all the gradients are zero
one complete cycle costs the same as gradient evaluation but may be more efficient
jirous ek and pr euc il implement an efficient version of ipf using junction trees
each of the cell counts is treated as an independent poisson variable
we get the multinomial model corresponding to by conditioning on the total count which is also poisson under this framework
undirected graphical models for discrete variables when is large other approaches have been used to approximate the gradient
we have not discussed decomposable models for which the maximum likelihood estimates can be found in closed form without any iteration whatsoever
these models arise for example in trees special graphs with tree structured topology
when computational tractability is concern trees represent useful class of models and they sidestep the computational concerns raised in this section
for details see for example chapter of whittaker
hidden nodes we can increase the complexity of discrete markov network by including latent or hidden nodes
suppose that subset of the variables xh are unobserved or hidden and the remainder xv are observed or visible
then the log likelihood of the observed data is log pr xv xiv log exp jk xij xik
xh xh the sum over xh means that we are summing over all possible values for the hidden units
the gradient works out to be xj xk xv xj xk jk the first term is an empirical average of xj xk if both are visible if one or both are hidden they are first imputed given the visible data and then averaged over the hidden variables
the second term is the unconditional expectation of xj xk
the inner expectation in the first term can be evaluated using basic rules of conditional expectation and properties of bernoulli random variables
in detail for observation
undirected graphical models xij xik if xj xk xv xiv xij pr xk xv xiv if pr xj xk xv xiv if
now two separate runs of gibbs sampling are required the first to estimate xj xk by sampling from the model as above and the second to estimate xj xk xv xiv
in this latter run the visible units are fixed clamped at their observed values and only the hidden variables are sampled
gibbs sampling must be done for each observation in the training set at each stage of the gradient search
as result this procedure can be very slow even for moderate sized models
in section we consider further model restrictions to make these computations manageable
estimation of the graph structure the use of lasso penalty with binary pairwise markov networks has been suggested by lee et al and wainwright et al
the first authors investigate conjugate gradient procedure for exact maximization of penalized log likelihood
the bottleneck is the computation of xj xk in the gradient exact computation via the junction tree algorithm is manageable for sparse graphs but becomes unwieldy for dense graphs
the second authors propose an approximate solution analogous to the meinshausen and bu hlmann approach for the gaussian graphical model
they fit an penalized logistic regression model to each node as function of the other nodes and then symmetrize the edge parameter estimates in some fashion
for example if jk is the estimate of the edge parameter from the logistic model for outcome node the min symmetrization sets jk to either jk or kj whichever is smallest in absolute value
the max criterion is defined similarly
they show that under certain conditions either approximation estimates the nonzero edges correctly as the sample size goes to infinity
hoefling and tibshirani extend the graphical lasso to discrete markov networks obtaining procedure which is somewhat faster than conjugate gradients but still must deal with computation of xj xk
they also compare the exact and approximate solutions in an extensive simulation study and find the min or max approximations are only slightly less accurate than the exact procedure both for estimating the nonzero edges and for estimating the actual values of the edge parameters and are much faster
furthermore they can handle denser graphs because they never need to compute the quantities xj xk
finally we point out key difference between the gaussian and binary models
in the gaussian case both and its inverse will often be of interest and the graphical lasso procedure delivers estimates for both of these quantities
however the approximation of meinshausen and bu hlmann for gaussian graphical models analogous to the wainwright et al
restricted boltzmann machine rbm in which there are no connections between nodes in the same layer
the visible units are subdivided to allow the rbm to model the joint density of feature and their labels approximation for the binary case only yields an estimate of
in contrast in the markov model for binary data is the object of interest and its inverse is not of interest
the approximate method of wainwright et al estimates efficiently and hence is an attractive solution for the binary problem
restricted boltzmann machines in this section we consider particular architecture for graphical models inspired by neural networks where the units are organized in layers
restricted boltzmann machine rbm consists of one layer of visible units and one layer of hidden units with no connections within each layer
it is much simpler to compute the conditional expectations as in and if the connections between hidden units are removed
figure shows an example the visible layer is divided into input variables and output variables and there is hidden layer
we denote such network by
for example could be the binary pixels of an image of handwritten digit and could have units one for each of the observed class labels
the restricted form of this model simplifies the gibbs sampling for estimating the expectations in since the variables in each layer are independent of one another given the variables in the other layers
hence they can be sampled together using the conditional probabilities given by expression
the resulting model is less general than boltzmann machine but is still useful for example it can learn to extract interesting features from images
we thank geoffrey hinton for assistance in the preparation of the material on rbms
undirected graphical models by alternately sampling the variables in each layer of the rbm shown in figure it is possible to generate samples from the joint density model
if the part of the visible layer is clamped at particular feature vector during the alternating sampling it is possible to sample from the distribution over labels given
alternatively classification of test items can also be achieved by comparing the unnormalized joint densities of each label category with the observed features
we do not need to compute the partition function as it is the same for all of these combinations
as noted the restricted boltzmann machine has the same generic form as single hidden layer neural network section
the edges in the latter model are directed the hidden units are usually real valued and the fitting criterion is different
the neural network minimizes the error crossentropy between the targets and their model predictions conditional on the input features
in contrast the restricted boltzmann machine maximizes the log likelihood for the joint distribution of all visible units that is the features and targets
it can extract information from the input features that is useful for predicting the labels but unlike supervised learning methods it may also use some of its hidden units to model structure in the feature vectors that is not immediately relevant for predicting the labels
these features may turn out to be useful however when combined with features derived from other hidden layers
unfortunately gibbs sampling in restricted boltzmann machine can be very slow as it can take long time to reach stationarity
as the network weights get larger the chain mixes more slowly and we need to run more steps to get the unconditional estimates
hinton noticed empirically that learning still works well if we estimate the second expectation in by starting the markov chain at the data and only running for few steps instead of to convergence
he calls this contrastive divergence we sample given then given and finally given again
the idea is that when the parameters are far from the solution it may be wasteful to iterate the gibbs sampler to stationarity as just single iteration will reveal good direction for moving the estimates
we now give an example to illustrate the use of an rbm
using contrastive divergence it is possible to train an rbm to recognize hand written digits from the mnist dataset lecun et al
with hidden units visible units for representing binary pixel intensities and one way multinomial visible unit for representing labels the rbm achieves an error rate of on the test set
this is little higher than the achieved by support vector machine and comparable to the error rate achieved by neural network trained with backpropagation
the error rate of the rbm however can be reduced to by replacing the pixel intensities by features that are produced from the images without using any label information
first an rbm with visible units and hidden units is trained using contrastive divergence to model the set of images
then the hidden states of the first rbm are used as data for training
example of restricted boltzmann machine for handwritten digit classification
the network is depicted in the schematic on the left
displayed on the right are some difficult test images that the model classifies correctly second rbm that has visible units and hidden units
finally the hidden states of the second rbm are used as the features for training an rbm with hidden units as joint density model
the details and justification for learning features in this greedy layer by layer way are described in hinton et al
figure gives representation of the composite model that is learned in this way and also shows some examples of the types of distortion that it can cope with
bibliographic notes much work has been done in defining and understanding the structure of graphical models
comprehensive treatments of graphical models can be found in whittaker lauritzen cox and wermuth edwards pearl anderson jordan and koller and friedman
wasserman gives brief introduction and chapter of bishop gives more detailed overview
boltzmann machines were proposed in ackley et al
ripley has detailed chapter on topics in graphical models that relate to machine learning
we found this particularly useful for its discussion of boltzmann machines
exercises ex
for the markov graph of figure list all of the implied conditional independence relations and find the maximal cliques
consider random variables
in each of the following cases draw graph that has the given independence relations and and and
let be the covariance matrix of set of variables
consider the partial covariance matrix aa ab bb ba between the two subsets of variables xa consisting of the first two and xb the rest
this is the covariance matrix between these two variables after linear adjustment for all the rest
in the gaussian distribution this is the covariance matrix of the conditional distribution of xa xb
the partial correlation coefficient jk rest between the pair xa conditional on the rest xb is simply computed from this partial covariance
define
show that aa
show that if any off diagonal element of is zero then the partial correlation coefficient between the corresponding variables is zero
show that if we treat as if it were covariance matrix and compute the corresponding correlation matrix diag diag then rjk jk rest ex
denote by xp the conditional density of given xp
if xp xp show that xp
exercises ex
consider the setup in section with no missing edges
show that are the estimating equations for the multiple regression coefficients of the last variable on the rest
recovery of from algorithm
use expression to derive the standard partitioned inverse expressions
since show that and
thus is simply rescaling of by
write program to implement the modified regression procedure for fitting the gaussian graphical model with pre specified edges missing
test it on the flow cytometry data from the book website using the graph of figure
write program to fit the lasso using the coordinate descent procedure
compare its results to those from the lars program or some other convex optimizer to check that it is working correctly using the program from write code to implement the graphical lasso algorithm
apply it to the flow cytometry data from the book website
vary the regularization parameter and examine the resulting networks
suppose that we have gaussian graphical model in which some or all of the data at some vertices are missing consider the em algorithm for dataset of multivariate observations xi irp with mean and covariance matrix
for each sample let oi and mi index the predictors that are observed and missing respectively
show that in the step the observations are imputed from the current estimates of and mi xi mi xi oi mi mi oi oi oi xi oi oi while in the step and are re estimated from the empirical mean and modified covariance of the imputed data ij
undirected graphical models jj ij ij ci jj where ci jj jj if mi and zero otherwise
explain the reason for the correction term ci jj little and rubin implement the em algorithm for the gaussian graphical model using the modified regression procedure from exercise for the step for the flow cytometry data on the book website set the data for the last protein jnk in the first observations to missing fit the model of figure and compare the predicted values to the actual values for jnk
compare the results to those obtained from regression of jnk on the other vertices with edges to jnk in figure using only the non missing data
using simple binary graphical model with just two variables show why it is essential to include constant node in the model
show that the ising model for the joint probabilities in discrete graphical model implies that the conditional distributions have the logistic form
consider poisson regression problem with binary variables xij and response variable yi which measures the number of observations with predictor xi
the design is balanced in that all possible combinations are measured
we assume log linear model for the poisson mean in each cell log xij xik jk using the same notation as in section including the constant variable xi
we assume the response is distributed as pr
write down the conditional log likelihood for the observed responses yi and compute the gradient show that the gradient equation for computes the partition function show that the gradient equations for the remainder of the parameters are equivalent to the gradient
this is page printer opaque this high dimensional problems when is much bigger than in this chapter we discuss prediction problems in which the number of features is much larger than the number of observations often written
such problems have become of increasing importance especially in genomics and other areas of computational biology
we will see that high variance and overfitting are major concern in this setting
as result simple highly regularized approaches often become the methods of choice
the first part of the chapter focuses on prediction in both the classification and regression settings while the second part discusses the more basic problem of feature selection and assessment
to get us started figure summarizes small simulation study that demonstrates the less fitting is better principle that applies when
for each of samples we generated standard gaussian features with pairwise correlation
the outcome was generated according to linear model xp xj where was generated from standard gaussian distribution
for each dataset the set of coefficients were also generated from standard gaussian distribution
we investigated three cases and
the standard deviation was chosen in each case so that the signal to noise ratio var equaled
as result the number of significant uni
test error results for simulation experiments
shown are boxplots of the relative test errors over simulations for three different values of the number of features
the relative error is the test error divided by the bayes error
from left to right results are shown for ridge regression with three different values of the regularization parameter bb and
the average effective degrees of freedom in the fit is indicated below each plot variate regression coefficients was and respectively averaged over the simulation runs
the case is designed to mimic the kind of data that we might see in high dimensional genomic or proteomic dataset for example
we fit ridge regression to the data with three different values for the regularization parameter bb and
when bb this is nearly the same as least squares regression with little regularization just to ensure that the problem is non singular when
figure shows boxplots of the relative test error achieved by the different estimators in each scenario
the corresponding average degrees of freedom used in each ridge regression fit is indicated computed using formula on page
the degrees of freedom is more interpretable parameter than bb
we see that ridge regression with bb df wins when bb df wins when and bb df wins when here is an explanation for these results
when we fit all the way and we can identify as many of the significant coefficients as possible with bj se we call regression coefficient significant if where is the estimated is its estimated standard error univariate coefficient and se for fixed value of the regularization parameter bb the degrees of freedom depends on the observed predictor values in each simulation
hence we compute the average degrees of freedom over simulations
nearest shrunken centroids low bias
when we can identify some non zero coefficients using moderate shrinkage
finally when even though there are many nonzero coefficients we don't have hope for finding them and we need to shrink all the way down
as evidence of this let tj bj se where its estimated standard error
then is the ridge regression estimate and se using the optimal ridge parameter in each of the three cases the median value of tj was and and the average number of tj values exceeding was equal to and
ridge regression with bb successfully exploits the correlation in the features when but cannot do so when
in the latter case there is not enough information in the relatively small number of samples to efficiently estimate the high dimensional covariance matrix
in that case more regularization leads to superior prediction performance
thus it is not surprising that the analysis of high dimensional data requires either modification of procedures designed for the scenario or entirely new procedures
in this chapter we discuss examples of both kinds of approaches for high dimensional classification and regression these methods tend to regularize quite heavily using scientific contextual knowledge to suggest the appropriate form for this regularization
the chapter ends with discussion of feature selection and multiple testing
diagonal linear discriminant analysis and nearest shrunken centroids gene expression arrays are an important new technology in biology and are discussed in chapters and
the data in our next example form matrix of genes columns and samples rows from set of microarray experiments
each expression value is log ratio log
is the amount of gene specific rna in the target sample that hybridizes to particular gene specific spot on the microarray and is the corresponding amount of rna from reference sample
the samples arose from small round blue cell tumors srbct found in children and are classified into four major types bl burkitt lymphoma ews ewing's sarcoma nb neuroblastoma and rms rhabdomyosarcoma
there is an additional test data set of observations
we will not go into the scientific background here
since we cannot fit full linear discriminant analysis lda to the data some sort of regularization is needed
the method we describe here is similar to the methods of section but with important modifications that achieve feature selection
the simplest form of regularization assumes that the features are independent within each class that is the within class covariance matrix is diagonal
despite the fact that features will rarely be independent within class when we don't have
high dimensional problems enough data to estimate their dependencies
the assumption of independence greatly reduces the number of parameters in the model and often results in an effective and interpretable classifier
thus we consider the diagonal covariance lda rule for classifying the classes
the discriminant score see on page for class is kj log
here is vector of expression values for test observation spj is the pooled within class standard deviation of the jth gene and kj ck xij nk is the mean of the nk values for gene in class with ck being the index set for class
we call kp the centroid of class
the first part of is simply the negative standardized squared distance of to the kth centroid
thepsecond part is correction based on the class prior probability where
the classification rule is then if maxk
we see that the diagonal lda classifier is equivalent to nearest centroid classifier after appropriate standardization
it is also special case of the naive bayes classifier as described in section
it assumes that the features in each class have independent gaussian distributions with the same variance
the diagonal lda classifier is often effective in high dimensional settings
it is also called the independence rule in bickel and levina who demonstrate theoretically that it will often outperform standard linear discriminant analysis in high dimensional problems
here the diagonal lda classifier yielded five misclassification errors for the test samples
one drawback of the diagonal lda classifier is that it uses all of the features genes and hence is not convenient for interpretation
with further regularization we can do better both in terms of test error and interpretability
we would like to regularize in way that automatically drops out features that are not contributing to the class predictions
we can do this by shrinking the classwise mean toward the overall mean for each feature separately
the result is regularized version of the nearest centroid classifier or equivalently regularized version of the diagonal covariance form of lda
we call the procedure nearest shrunken centroids nsc
the shrinkage procedure is defined as follows
let kj dkj mk sj where is the overall mean for gene nk and is small positive constant typically chosen to be the median of the sj values
soft thresholding function sign is shown in orange along with the line in red
this constant guards against large dkj values that arise from expression values near zero
with constant within class variance the variance of the contrast kj in the numerator is and hence the form of the standardization in the denominator
we shrink the dkj toward zero using soft thresholding kj sign dkj dkj see figure
here is parameter to be determined we used fold cross validation in the example see the top panel of figure
each dkj is reduced by an amount in absolute value and is set to zero if its absolute value is less than zero
the soft thresholding function is shown in figure the same thresholding is applied to wavelet coefficients in section
an alternative is to use hard thresholding kj dkj dkj we prefer soft thresholding as it is smoother operation and typically works better
the shrunken versions of kj are then obtained by reversing the transformation in kj mk sj kj
we then use the shrunken centroids kj in place of the original kj in the discriminant score
the estimator can also be viewed as lasso style estimator for the class means exercise
notice that only the genes that have nonzero kj for at least one of the classes play role in the classification rule and hence the vast majority of genes can often be discarded
in this example all but genes were discarded leaving small interpretable set of genes that characterize each class
figure represents the genes in heatmap
figure top panel demonstrates the effectiveness of the shrinkage
with no shrinkage we make errors on the test data and several errors
high dimensional problems on the training and cv data
the shrunken centroids achieve zero test errors for fairly broad band of values for
the bottom panel of figure shows the four centroids for the srbct data gray relative to the overall centroid
the blue bars are shrunken versions of these centroids obtained by soft thresholding the gray bars using
the discriminant scores can be used to construct class probability estimates pk
these can be used to rate the classifications or to decide not to classify particular sample at all
note that other forms of feature selection can be used in this setting including hard thresholding
fan and fan show theoretically the importance of carrying out some kind of feature selection with diagonal linear discriminant analysis in high dimensional problems
linear classifiers with quadratic regularization ramaswamy et al present more difficult microarray classification problem involving training set of patients with different types of cancer and test set of patients
gene expression measurements were available for genes
table shows the prediction results from eight different classification methods
the data from each patient was first standardized to have mean and variance this seems to improve prediction accuracy overall this example suggesting that the shape of each gene expression profile is important rather than the absolute expression levels
heat map of the chosen genes
within each of the horizontal partitions we have ordered the genes by hierarchical clustering and similarly for the samples within each vertical partition
yellow represents overand blue under expression
top error curves for the srbct data
shown are the training fold cross validation and test misclassification errors as the threshold parameter is varied
the value is chosen by cv resulting in subset of selected genes
bottom four centroids profiles dkj for the srbct data gray relative to the overall centroid
each centroid has components and we see considerable noise
the blue bars are shrunken versions kj of these centroids obtained by soft thresholding the gray bars using
high dimensional problems table
prediction results for microarray data with cancer classes
method is described in section
methods and are discussed in section while and are discussed in section
method is described in section
the elastic net penalized multinomial does the best on the test data but the standard error of each test error estimate is about so such comparisons are inconclusive
methods cv errors se test errors number of out of out of genes used
nearest shrunken centroids
penalized discriminant analysis
support vector classifier
lasso regression one vs all nearest neighbors
penalized multinomial
penalized multinomial
elastic net penalized multinomial regularization parameter has been chosen to minimize the cross validation error and the test error at that value of the parameter is shown
when more than one value of the regularization parameter yields the minimal cross validation error the average test error at these values is reported
rda regularized discriminant analysis regularized multinomial logistic regression and the support vector machine are more complex methods that try to exploit multivariate information in the data
we describe each in turn as well as variety of regularization methods including both and and some in between
regularized discriminant analysis regularized discriminant analysis rda is described in section
linear discriminant analysis involves the inversion of within covariance matrix
when this matrix can be huge has rank at most and hence is singular
rda overcomes the singularity issues by regularizing the within covariance estimate
here we use version of rda that shrinks towards its diagonal diag with
note that corresponds to diagonal lda which is the no shrinkage version of nearest shrunken centroids
the form of shrinkage in is
linear classifiers with quadratic regularization much like ridge regression section which shrinks the total covariance matrix of the features towards diagonal scalar matrix
in fact viewing linear discriminant analysis as linear regression with optimal scoring of the categorical response see in section the equivalence becomes more precise
the computational burden of inverting this large matrix is overcome using the methods discussed in section
the value of was chosen by cross validation in line of table all values of gave the same cv and test error
further development of rda including shrinkage of the centroids in addition to the covariance matrix can be found in guo et al
logistic regression with quadratic regularization logistic regression section can be modified in similar way to deal with the case
with classes we use symmetric version of the multiclass logistic model on page exp xt pr pk
exp this has coefficient vectors of log odds parameters
we regularize the fitting by maximizing the penalized log likelihood bb max log pr gi xi
this regularization automatically pk resolves the redundancy in the parametrization and forces kj exercise
note that the constant terms are not regularized and so one should be set to zero
the resulting optimization problem is convex and can be solved by newton algorithm or other numerical techniques
details are given in zhu and hastie
friedman et al provide software for computing the regularization path for the twoand multiclass logistic regression models
table line reports the results for the multiclass logistic regression model referred to there as multinomial
it can be shown rosset et al that for separable data as bb the regularized twoclass logistic regression estimate renormalized converges to the maximal margin classifier section
this gives an attractive alternative to the support vector machine discussed next especially in the multiclass case
the support vector classifier the support vector classifier is described for the two class case in section
when it is especially attractive because in general the
high dimensional problems classes are perfectly separable by hyperplane unless there are identical feature vectors in different classes
without any regularization the support vector classifier finds the separating hyperplane with the largest margin that is the hyperplane yielding the biggest gap between the classes in the training data
somewhat surprisingly when the unregularized support vector classifier often works about as well as the best regularized version
overfitting often does not seem to be problem partly because of the insensitivity of misclassification loss
there are many different methods for generalizing the two class support classes
in the one versus one ovo approach vector classifier to we compute all pairwise classifiers
for each test point the predicted class is the one that wins the most pairwise contests
in the one versus all ova approach each class is compared to all of the others in two class comparisons
to classify test point we compute the confidences signed distance from the hyperplane for each of the classifiers
the winner is the class with the highest confidence
finally vapnik and weston and watkins suggested somewhat complex multiclass criteria which generalize the two class criterion
tibshirani and hastie propose the margin tree classifier in which support vector classifiers are used in binary tree much as in cart chapter
the classes are organized in hierarchical manner which can be useful for classifying patients into different cancer types for example
line of table shows the results for the support vector classifier using the ova method ramaswamy et al reported and we confirmed that this approach worked best for this problem
the errors are very similar to those in line as we might expect from the comments at the end of the previous section
the error rates are insensitive to the choice of the regularization parameter in on page for values of
since the support vector hyperplane can perfectly separate the training data by setting
feature selection feature selection is an important scientific requirement for classifier when is large
neither discriminant analysis logistic regression nor the supportvector classifier perform feature selection automatically because all use quadratic regularization
all features have nonzero weights in both models
ad hoc methods for feature selection have been proposed for example removing genes with small coefficients and refitting the classifier
this is done in backward stepwise manner starting with the smallest weights and moving on to larger weights
this is known as recursive feature elimination guyon et al
it was not successful in this example ramaswamy et al report for example that the accuracy of the support vector classifier starts to degrade as the number of genes is reduced from the full
linear classifiers with quadratic regularization set of
this is rather remarkable as the number of training samples is only
we do not have an explanation for this behavior
all three methods discussed in this section rda lr and svm can be modified to fit nonlinear decision boundaries using kernels
usually the motivation for such an approach is to increase the model complexity
with the models are already sufficiently complex and overfitting is always danger
yet despite the high dimensionality radial kernels section sometimes deliver superior results in these high dimensional problems
the radial kernel tends to dampen inner products between points far away from each other which in turn leads to robustness to outliers
this occurs often in high dimensions and may explain the positive results
we tried radial kernel with the svm in table but in this case the performance was inferior
computational shortcuts when the computational techniques discussed in this section apply to any method that fits linear model with quadratic regularization on the coefficients
that includes all the methods discussed in this section and many more
when the computations can be carried out in an dimensional space rather than via the singular value decomposition introduced in section
here is the geometric intuition just like two points in threedimensional space always lie on line points in dimensional space lie in an dimensional affine subspace
given the data matrix let udvt rv be the singular value decomposition svd of that is is with orthonormal columns is orthogonal and diagonal matrix with elements dn
the matrix is with rows rit
as simple example let's first consider the estimates from ridge regression xt bb xt
replacing by rvt and after some further manipulations this can be shown to equal rt bb rt exercise
thus where is the ridge regression estimate using the observations ri yi
in other words we can simply reduce the data matrix from to and work with the rows of
this trick reduces the computational cost from to pn when
high dimensional problems these results can be generalized to all models that are linear in the parameters and have quadratic penalties
consider any supervised learning problem where we use linear function to model parameter in the conditional distribution of
we fit the parameters pn by minimizing some loss function yi xi over the data with quadratic penalty on
logistic regression is useful example to have in mind
then we have the following simple theorem let ri rit with ri defined in and consider the pair of optimization problems arg min yi xti bb ir arg min yi rit bb
irn then the and
the theorem says that we can simply replace the vectors xi by the vectors ri and perform our penalized fit as before but with far fewer predictors
the vector solution is then transformed back to the pvector solution via simple matrix multiplication
this result is part of the statistics folklore and deserves to be known more widely see hastie and tibshirani for further details
geometrically we are rotating the features to coordinate system in which all but the first coordinates are zero
such rotations are allowed since the quadratic penalty is invariant under rotations and linear models are equivariant
this result can be applied to many of the learning methods discussed in this chapter such as regularized multiclass logistic regression linear discriminant analysis exercise and support vector machines
it also applies to neural networks with quadratic regularization section
note however that it does not apply to methods such as the lasso which uses nonquadratic penalties on the coefficients
typically we use cross validation to select the parameter bb
it can be seen exercise that we only need to construct once on the original data and use it as the data for each of the cv folds
the support vector kernel trick of section exploits the same reduction used in this section in slightly different context
suppose we have at our disposal the gram inner product matrix xxt
from we have ud ut and so captures the same information as
exercise shows how we can exploit the ideas in this section to fit ridged logistic regression with using its svd
linear classifiers with regularization linear classifiers with regularization the methods of the previous chapter use an penalty to regularize their parameters just as in ridge regression
all of the estimated coefficients are nonzero and hence no feature selection is performed
in this section we discuss methods that use penalties instead and hence provide automatic feature selection
recall the lasso of section xp xp min yi xij bb which we have written in the lagrange form
as discussed there the use of the penalty causes subset of the solution coefficients to be exactly zero for sufficiently large value of the tuning parameter bb
in section we discussed the lars algorithm an efficient procedure for computing the lasso solution for all bb
when as in this chapter as bb approaches zero the lasso fits the training data exactly
in fact by convex duality one can show that when the number of non zero coefficients is at most for all values of bb rosset and zhu for example
thus the lasso provides severe form of feature selection
lasso regression can be applied to two class classification problem by coding the outcome and applying cutoff usually to the predictions
for more than two classes there are many possible approaches including the ova and ovo methods discussed in section
we tried the ovaapproach on the cancer data in section
the results are shown in line of table
its performance is among the best
more natural approach for classification problems is to use the lasso penalty to regularize logistic regression
several implementations have been proposed in the literature including path algorithms similar to lars park and hastie
because the paths are piecewise smooth but nonlinear exact methods are slower than the lars algorithm and are less feasible when is large
friedman et al provide very fast algorithms for fitting penalized logistic and multinomial regression models
they use the symmetric multinomial logistic regression model as in in section and maximize the penalized log likelihood ee xn max log pr gi xi bb kj fb irp compare with
their algorithm computes the exact solution at pre chosen sequence of values for bb by cyclical coordinate descent section and exploits the fact that solutions are sparse when
high dimensional problems as well as the fact that solutions for neighboring values of bb tend to be very similar
this method was used in line of table with the overall tuning parameter bb chosen by cross validation
the performance was similar to that of the best methods except here the automatic feature selection chose genes altogether
similar approach is used in genkin et al although they present their model from bayesian point of view they in fact compute the posterior mode which solves the penalized maximum likelihood problem
regularized logistic regression paths for the leukemia data
the left panel is the lasso path the right panel the elastic net path with
at the ends of the path extreme left there are nonzero coefficients for the lasso and for the elastic net
the averaging effect of the elastic net results in more non zero coefficients than the lasso but with smaller magnitudes
in genomic applications there are often strong correlations among the variables genes tend to operate in molecular pathways
the lasso penalty is somewhat indifferent to the choice among set of strong but correlated variables exercise
the ridge penalty on the other hand tends to shrink the coefficients of correlated variables toward each other exercise on page
the elastic net penalty zou and hastie is compromise and has the form
the second term encourages highly correlated features to be averaged while the first term encourages sparse solution in the coefficients of these aver
linear classifiers with regularization aged features
the elastic net penalty can be used with any linear model in particular for regression or classification
hence the multinomial problem above with elastic net penalty becomes ee xn max log pr gi xi bb fb kj kj
irp the parameter determines the mix of the penalties and is often prechosen on qualitative grounds
the elastic net can yield more that nonzero coefficients when potential advantage over the lasso
line in table uses this model with and bb chosen by cross validation
we used sequence of values of between and and values of bb uniform on the log scale covering the entire range
values of gave the minimum cv error with values of bb for all tied solutions
although it has the lowest test error among all methods the margin is small and not significant
interestingly when cv is performed separately for each value of minimum test error of is achieved at but this is not the value chosen in the two dimensional cv
training test and fold cross validation curves for lasso logistic regression on the leukemia data
the left panel shows misclassification errors the right panel shows deviance
figure shows the lasso and elastic net coefficient paths on the twoclass leukemia data golub et al
there are gene expression measurements on samples of them in class all acute lymphocytic leukemia and in class aml acute myelogenous leukemia
there is also test set with samples
since the data are linearly separable the solution is undefined at bb exercise and degrades for very small values of bb
hence the paths have been truncated as the fitted probabilities approach and
there are non zero coefficients in the left plot and in the right
figure left panel shows the misclas
high dimensional problems sification errors for the lasso logistic regression on the training and test data as well as for fold cross validation on the training data
the right panel uses binomial deviance to measure errors and is much smoother
the small sample sizes lead to considerable sampling variance in these curves even though individual curves are relatively smooth see for example figure on page
both of these plots suggest that the limiting solution bb is adequate leading to misclassifications in the test set
the corresponding figures for the elastic net are qualitatively similar and are not shown
for the limiting coefficients diverge for all regularized logistic regression models so in practical software implementations minimum value for bb is either explicitly or implicitly set
however renormalized versions of the coefficients converge and these limiting solutions can be thought of as interesting alternatives to the linear optimal separating hyperplane svm
with the limiting solution coincides with the svm see end of section but all the genes are selected
with the limiting solution coincides with an separating hyperplane rosset et al and includes at most genes
as decreases from the elastic net solutions include more genes in the separating hyperplane
application of lasso to protein mass spectroscopy protein mass spectrometry has become popular technology for analyzing the proteins in blood and can be used to diagnose disease or understand the processes underlying it
for each blood serum sample we observe the intensity xij for many time of flight values tj
this intensity is related to the number of particles observed to take approximately tj time to pass from the emitter to the detector during cycle of operation of the machine
the time of flight has known relationship to the mass over charge ratio of the constituent proteins in the blood
hence the identification of peak in the spectrum at certain tj tells us that there is protein with corresponding mass and charge
the identity of this protein can then be determined by other means
figure shows an example taken from adam et al
it shows the average spectra for healthy patients and those with prostate cancer
there are sites in total ranging in value from to
the full dataset consists of healthy patients and with cancer and the goal is to find sites that discriminate between the two groups
this is an example of functional data the predictors can be viewed as function of
there has been much interest in this problem in the past few years see
petricoin et al
the data were first standardized baseline subtraction and normalization and we restricted attention to values between and spectra outside of this range were not of interest
we then applied near
protein mass spectrometry data average profiles from normal and prostate cancer patients est shrunken centroids and lasso regression to the data with the results for both methods shown in table
by fitting harder to the data the lasso achieves considerably lower test error rate
however it may not provide scientifically useful solution
ideally protein mass spectrometry resolves biological sample into its constituent proteins and these should appear as peaks in the spectra
the lasso doesn't treat peaks in any special way so not surprisingly only some of the non zero lasso weights were situated near peaks in the spectra
furthermore the same protein may yield peak at slightly different values in different spectra
in order to identify common peaks some kind of warping is needed from sample to sample
to address this we applied standard peak extraction algorithm to each spectrum yielding total of peaks in the training spectra
our idea was to pool the collection of peaks from all patients and hence construct set of common peaks
for this purpose we applied hierarchical clustering to the positions of these peaks along the log axis
we cut the resulting dendrogram horizontally at height log and computed averages of the peak positions in each resulting cluster
this process yielded common clusters and their corresponding peak centers
given these common peaks we determined which of these were present in each individual spectrum and if present the height of the peak
peak height of zero was assigned if that peak was not found
this produced matrix of peak heights as features which was used in lasso regression
we scored the test spectra for the same peaks
use of the value means that peaks with positions less than apart are considered the same peak fairly common assumption
high dimensional problems table
results for the prostate data example
the standard deviation for the test errors is about
method test errors number of sites
nearest shrunken centroids
lasso on peaks the prediction results for this application of the lasso to the peaks are shown in the last line of table it does fairly well but not as well as the lasso on the raw spectra
however the fitted model may be more useful to the biologist as it yields peak positions for further study
on the other hand the results suggest that there may be useful discriminatory information between the peaks of the spectra and the positions of the lasso sites from line of the table also deserve further examination
the fused lasso for functional data in the previous example the features had natural order determined by the mass to charge ratio
more generally we may have functional features xi that are ordered according to some index variable
we have already discussed several approaches for exploiting such structure
we can represent xi by their coefficients in basis of functions in such as splines wavelets or fourier bases and then apply regression using these coefficients as predictors
equivalently one can instead represent the coefficients of the original features in these bases
these approaches are described in section
in the classification setting we discuss the analogous approach of penalized discriminant analysis in section
this uses penalty that explicitly controls the resulting smoothness of the coefficient vector
the above methods tend to smooth the coefficients uniformly
here we present more adaptive strategy that modifies the lasso penalty to take into account the ordering of the features
the fused lasso tibshirani et al solves min yi xij bb bb
irp this criterion is strictly convex in so unique solution exists
the first penalty encourages the solution to be sparse while the second encourages it to be smooth in the index
the difference penalty in assumes an uniformly spaced index
if instead the underlying index variable has nonuniform values tj natural generalization of would be based on divided differences
fused lasso applied to cgh data
each point represents the copy number of gene in tumor sample relative to that of control on the log base scale bb
tj tj this amounts to having penalty modifier for each of the terms in the series
particularly useful special case arises when the predictor matrix in the identity matrix
this is special case of the fused lasso used to approximate sequence yi
the fused lasso signal approximator solves min yi bb bb
irn figure shows an example taken from tibshirani and wang
the data in the panel come from comparative genomic hybridization cgh array measuring the approximate log base two ratio of the number of copies of each gene in tumor sample as compared to normal sample
the horizontal axis represents the chromosomal location of each gene
the idea is that in cancer cells genes are often amplified duplicated or deleted and it is of interest to detect these events
furthermore these events tend to occur in contiguous regions
the smoothed signal estimate from the fused lasso signal approximator is shown in dark red with appropriately chosen values for bb and bb
the significantly nonzero regions can be used to detect locations of gains and losses of genes in the tumor
there is also two dimensional version of the fused lasso in which the parameters are laid out in grid of pixels and penalty is applied to the
high dimensional problems first differences to the left right above and below the target pixel
this can be useful for denoising or classifying images
friedman et al develop fast generalized coordinate descent algorithms for the oneand two dimensional fused lasso
classification when features are unavailable in some applications the objects under study are more abstract in nature and it is not obvious how to define feature vector
as long as we can fill in an proximity matrix of similarities between pairs of objects in our database it turns out we can put to use many of the classifiers in our arsenal by interpreting the proximities as inner products
protein structures fall into this category and we explore an example in section below
in other applications such as document classification feature vectors are available but can be extremely high dimensional
here we may not wish to compute with such high dimensional data but rather store the innerproducts between pairs of documents
often these inner products can be approximated by sampling techniques
pairwise distances serve similar purpose because they can be turned into centered inner products
proximity matrices are discussed in more detail in chapter
example string kernels and protein classification an important problem in computational biology is to classify proteins into functional and structural classes based on their sequence similarities
protein molecules are strings of amino acids differing in both length and composition
in the example we consider the lengths vary between amino acid molecules each of which can be one of different types labeled using letters
here are two examples of length and respectively iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv erlfknlslikkyidgqkkkcgeerrrvnqfldylqeflgvmntewi phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaerlqenlqayrtfhvlla rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk lwglkvlqelsqwtvrsihdlrfisshqtgip there have been many proposals for measuring the similarity between pair of protein molecules
here we focus on measure based on the count of matching substrings leslie et al such as the lqe above
to construct our features we count the number of times that given sequence of length occurs in our string and we compute this number
classification when features are unavailable for all possible sequences of length
formally for string we define feature map am where am is the set of subsequences of length and is the number of times that occurs in our string
using this we define the inner product km which measures the similarity between the two strings
this can be used to drive for example support vector classifier for classifying strings into different protein classes
now the number of possible sequences is am which can be very large for moderate and the vast majority of the subsequences do not match the strings in our training set
it turns out that we can compute the inner product matrix or string kernel km efficiently using tree structures without actually computing the individual vectors
this methodology and the data to follow come from leslie et al
the data consist of proteins in two classesnegative and positive
the two examples above which we will call and are from this set
we have marked the occurrences of subsequence lqe which appears in both proteins
there are possible subsequences so will be vector of length
for this example lqe and lqe
using software from leslie et al we computed the string kernel for which was then used in support vector classifier to find the maximal margin solution in this dimensional feature space
we used fold cross validation to compute the svm predictions on all of the training data
the orange curve in figure shows the cross validated roc curve for the support vector classifier computed by varying the cutpoint on the real valued predictions from the cross validated support vector classifier
the area under the curve is
leslie et al show that the string kernel method is competitive with but perhaps not as accurate as more specialized methods for protein string matching
many other classifiers can be computed using only the information in the kernel matrix some details are given in the next section
the results for the nearest centroid classifier green and distance weighted one nearest neighbors blue are shown in figure
their performance is similar to that of the support vector classifier
we thank christina leslie for her help and for providing the data which is available on our book website
cross validated roc curves for protein example using the string kernel
the numbers next to each method in the legend give the area under the curve an overall measure of accuracy
the svm achieves better sensitivities than the other two which achieve better specificities
classification and other models using inner product kernels and pairwise distances there are number of other classifiers besides the support vector machine that can be implemented using only inner product matrices
this also implies they can be kernelized like the svm
an obvious example is nearest neighbor classification since we can transform pairwise inner products to pairwise distances xi xi hxi xi hxi xi hxi xi
variation of nn classification is used in figure which produces continuous discriminant score needed to construct roc curve
this distance weighted nn makes use of the distance of test points to the closest member of each class see exercise
nearest centroid classification follows easily as well
for training pairs xi gi test point and class centroids we can write hx hx xi hxi xi nk nk gi gi gi
classification when features are unavailable hence we can compute the distance of the test point to each of the centroids and perform nearest centroid classification
this also implies that methods like means clustering can also be implemented using only the inner products of the data points
logistic and multinomial regression with quadratic regularization can also be implemented with inner product kernels see section and exercise
exercise derives linear discriminant analysis using an inner product kernel
principal components can be computed using inner product kernels as well since this is frequently useful we give some details
suppose first that we have centered data matrix and let udvt be its svd
then ud is the matrix of principal component variables see section
but if xxt then it follows that ud ut and hence we can compute from the eigen decomposition of
if is not centered then we can center it using where is the mean operator
thus we compute the eigenvectors of the doublecentered kernel for the principal components from an uncentered inner product matrix
exercise explores this further and section discusses in more detail kernel pca for general kernels such as the radial kernel used in svms
if instead we had available only the pairwise squared euclidean distances between observations ii xi xi it turns out we can do all of the above as well
the trick is to convert the pairwise distances to centered inner products and then proceed as before
we write ii xi xi hxi xi
defining ii we double center it is easy to check that ii hxi xi the centered inner product matrix
distances and inner products also allow us to compute the medoid in each class the observation with smallest average distance to other observations in that class
this can be used for classification closest medoids as well as to drive medoids clustering section
with abstract data objects like proteins medoids have practical advantage over means
the medoid is one of the training examples and can be displayed
we tried closest medoids in the example in the next section see table and its performance is disappointing
it is useful to consider what we cannot do with inner product kernels and distances
high dimensional problems table
cross validated error rates for the abstracts example
the nearest shrunken centroids ended up using no shrinkage but does use word by word standardization section
this standardization gives it distinct advantage over the other methods
method cv error se
nearest shrunken centroids
nearest medoids
in particular we cannot perform individual tests fit the nearest shrunken centroids model or fit any model that uses the lasso penalty
if as is often the case the ratio of relevant to irrelevant variables is small methods that use kernels are not likely to work as well as methods that do feature selection
example abstracts classification this somewhat whimsical example serves to illustrate limitation of kernel approaches
we collected the abstracts from papers each from bradley efron be trevor hastie and rob tibshirani ht frequent coauthors and jerome friedman jf
we extracted all unique words from these abstracts and defined features xij to be the number of times word appears in abstract
this is the so called bag of words representation
quotations parentheses and special characters were first removed from the abstracts and all characters were converted to lower case
we also removed the word we which could unfairly discriminate ht abstracts from the others
there were total words of which were unique
we sought to classify the documents into be ht or jf on the basis of the features xij
although it is artificial this example allows us to assess the possible degradation in performance if information specific to the raw features is not used
we first applied the nearest shrunken centroid classifier to the data using fold cross validation
it essentially chose no shrinkage and so used all the features see the first line of table
the error rate is the number of features can be reduced to about without much loss in accuracy
classification when features are unavailable note that the nearest shrunken classifier requires the raw feature matrix in order to standardize the features individually
abstracts example top scores from nearest shrunken centroids
each score is the standardized difference in frequency for the word in the given class be ht or jf versus all classes
thus positive score to the right of the vertical grey zero lines indicates higher frequency in that class negative score indicates lower relative frequency top discriminating words with positive score indicating that word appears more in that class than in the other classes
some of these terms make sense for example frequentist and bayesian reflect efron's greater emphasis on statistical inference
however many others are surprising and reflect personal writing styles for example friedman's use of presented and ht's use of propose
we then applied the support vector classifier with linear kernel and no regularization using the all pairs ovo method to handle the three classes regularization of the svm did not improve its performance
the result is shown in table
it does somewhat worse than the nearest shrunken centroid classifier
as mentioned the first line of table represents nearest shrunken centroids with no shrinkage
denote by sj the pooled within class standard deviation for feature and the median of the sj values
then line also corresponds to nearest centroid classification after first standardizing each feature by sj recall on page
line shows that the performance of nearest medoids is very poor something which surprised us
it is perhaps due to the small sample sizes
high dimensional problems and high dimensions with medoids having much higher variance than means
the performance of the one nearest neighbor classifier is also poor
the performance of the nearest centroid classifier is also shown in table in line it is better than nearest medoids but worse than that of nearest shrunken centroids even with no shrinkage
the difference seems to be the standardization of each feature that is done in nearest shrunken centroids
this standardization is important here and requires access to the individual feature values
nearest centroids uses spherical metric and relies on the fact that the features are in similar units
the support vector machine estimates linear combination of the features and can better deal with unstandardized features
high dimensional regression supervised principal components in this section we describe simple approach to regression and generalized regression that is especially useful when
we illustrate the method on another microarray data example
the data is taken from rosenwald et al and consists of samples from patients with diffuse large cell lymphoma dlbcl with gene expression measurements for genes
the outcome is survival time either observed or right censored
we randomly divided the lymphoma samples into training set of size and test set of size
although supervised principal components is useful for linear regression its most interesting applications may be in survival studies which is the focus of this example
we have not yet discussed regression with censored survival data in this book it represents generalized form of regression in which the outcome variable survival time is only partly observed for some individuals
suppose for example we carry out medical study that lasts for days and for simplicity all subjects are recruited on day one
we might observe one individual to die days after the start of the study
another individual might still be alive at days when the study ends
this individual is said to be right censored at days
we know only that he or she lived at least days
although we do not know how long past days the individual actually lived the censored observation is still informative
this is illustrated in figure
figure shows the survival curve estimated by the kaplan meier method for the patients in the test set
see for example kalbfleisch and prentice for description of the kaplan meier method
our objective in this example is to find set of features genes that can predict the survival of an independent set of patients
this could be
censored survival data
for illustration there are four patients
the first and third patients die before the study ends
the second patient is alive at the end of the study days while the fourth patient is lost to follow up before the study ends
for example this patient might have moved out of the country
the survival times for patients two and four are said to be censored
lymphoma data
the kaplan meier estimate of the survival function for the patients in the test set along with one standard error curves
the curve estimates the probability of surviving past months
the ticks indicate censored observations
underlying conceptual model for supervised principal components
there are two cell types and patients with the good cell type live longer on the average
supervised principal components estimate the cell type by averaging the expression of genes that reflect it useful as prognostic indicator to aid in choosing treatments or to help understand the biological basis for the disease
the underlying conceptual model for supervised principal components is shown in figure
we imagine that there are two cell types and patients with the good cell type live longer on the average
however there is considerable overlap in the two sets of survival times
we might think of survival time as noisy surrogate for cell type
fully supervised approach would give the most weight to those genes having the strongest relationship with survival
these genes are partially but not perfectly related to cell type
if we could instead discover the underlying cell types of the patients often reflected by sizable signature of genes acting together in pathways then we might do better job of predicting patient survival
although the cell type in figure is discrete it is useful to imagine continuous cell type define by some linear combination of the features
we will estimate the cell type as continuous quantity and then discretize it for display and interpretation
how can we find the linear combination that defines the important underlying cell types
principal components analysis section is an effective method for finding linear combinations of features that exhibit large variation in dataset
but what we seek here are linear combinations with both high variance and significant correlation with the outcome
the lower right panel of figure shows the result of applying standard principal components in this example the leading component does not correlate strongly with survival details are given in the figure caption
hence we want to encourage principal component analysis to find linear combinations of features that have high correlation with the outcome
to do this we restrict attention to features which by themselves have sizable correlation with the outcome
this is summarized in the supervised principal components algorithm and illustrated in figure
the details in steps and will depend on the type of outcome variable
for standard regression problem we use the univariate linear least squares coefficients in step and linear least squares model in
supervised principal components on the lymphoma data
the left panel shows heatmap of subset of the gene expression training data
the rows are ordered by the magnitude of the univariate cox score shown in the middle vertical column
the top and bottom genes are shown
the supervised principal component uses the top genes chosen by fold cv
it is represented by the bar at the top of the heatmap and is used to order the columns of the expression matrix
in addition each row is multiplied by the sign of the cox score
the middle panel on the right shows the survival curves on the test data when we create low and high group by splitting this supervised pc at zero training data mean
the curves are well separated as indicated by the value for the log rank test
the top panel does the same using the top scoring gene on the training data
the curves are somewhat separated but not significantly
the bottom panel uses the first principal component on all the genes and the separation is also poor
each of the top genes can be interpreted as noisy surrogates for latent underlying cell type characteristic and supervised principal components uses them all to estimate this latent factor
high dimensional problems algorithm supervised principal components
compute the standardized univariate regression coefficients for the outcome as function of each feature separately
for each value of the threshold from the list form reduced data matrix consisting of only those features whose univariate coefficient exceeds in absolute value and compute the first principal components of this matrix use these principal components in regression model to predict the outcome
pick and by cross validation step
for survival problems cox's proportional hazards regression model is widely used hence we use the score test from this model in step and the multivariate cox model in step
the details are not essential for understanding the basic method they may be found in bair et al
figure shows the results of supervised principal components in this example
we used cox score cutoff of yielding genes where the value was found through fold cross validation
we then computed the first principal component using just this subset of the data as well as its value for each of the test observations
we included this as quantitative predictor in cox regression model and its likelihood ratio significance was
when dichotomized using the mean score on the training data as threshold it clearly separates the patients in the test set into low and high risk groups middle right panel of figure
the top right panel of figure uses the top scoring gene dichotomized alone as predictor of survival
it is not significant on the test set
likewise the lower right panel shows the dichotomized principal component using all the training data which is also not significant
our procedure allows principal components in step
however the supervision in step encourages the principal components to align with the outcome and thus in most cases only the first or first few components tend to be useful for prediction
in the mathematical development below we consider only the first component but extensions to more than one component can be derived in similar way
connection to latent variable modeling formal connection between supervised principal components and the underlying cell type model figure can be seen through latent variable model for the data
suppose we have response variable which is related
high dimensional regression supervised principal components to an underlying latent variable by linear model
in addition we have measurements on set of features xj indexed by for pathway for which xj oj
the errors and oj are assumed to have mean zero and are independent of all other random variables in their respective models
we also have many additional features xk which are independent of
we would like to identify estimate and hence fit the prediction model
this is special case of latent structure model or single component factor analysis model mardia et al see also section
the latent factor is continuous version of the cell type conceptualized in figure
step is natural since on average the regression coefficient is nonzero only if is non zero
hence this step should select the features
step is natural if we assume that the errors oj have gaussian distribution with the same variance
in this case the principal component is the maximum likelihood estimate for the single factor model mardia et al
the regression in is an obvious final step
suppose there are total of features with features in the relevant set
then if and grow but is small relative to one can show under reasonable conditions that the leading supervised principal component is consistent for the underlying latent factor
the usual leading principal component may not be consistent since it can be contaminated by the presence of large number of noise features
finally suppose that the threshold used in step of the supervised principal component procedure yields large number of features for computation of the principal component
then for interpretational purposes as well as for practical uses we would like some way of finding reduced set of features that approximates the model
pre conditioning section is one way of doing this
high dimensional problems relationship with partial least squares supervised principal components is closely related to partial least squares regression section
bair et al found that the key to the good performance of supervised principal components was the filtering out of noisy features in step
partial least squares section downweights noisy features but does not throw them away as result large number of noisy features can contaminate the predictions
however modification of the partial least squares procedure has been proposed that has similar flavor to supervised principal components brown et al
nadler and coifman for example
we select the features as in steps and of supervised principal components but then apply pls rather than principal components to these features
for our current discussion we call this thresholded pls
thresholded pls can be viewed as noisy version of supervised principal components and hence we might not expect it to work as well in practice
assume the variables are all standardized
the first pls variate has the form hy xj ixj and can be thought of as an estimate of the latent factor in model
in contrast the supervised principal components direction satisfies hu xj ixj where is the leading singular value of xp
this follows from the definition of the leading principal component
hence thresholded pls uses weights which are the inner product of with each of the features while supervised principal components uses the features to derive self consistent estimate
since many features contribute to the estimate rather than just the single outcome we can expect to be less noisy than
in fact if there are features in the set and and go to infinity with then it can be shown using the techniques in bair et al that op op where is the true unobservable latent variable in the model
we now present simulation example to compare the methods numerically
there are samples and genes
we generated the data as follows
heatmap of the outcome left column and first genes from realization from model
the genes are in the columns and the samples are in the rows oij if xij oij if oij if or xij oij if or xij oij yi xij where oij and are independent normal random variables with mean and standard deviations and respectively
thus in the first genes there is an average difference of unit between samples and and this difference correlates with the outcome
the next genes have large average difference of units between samples and but this difference is uncorrelated with the outcome
the rest of the genes are noise
figure shows heatmap of typical realization with the outcome at the left and the first genes to the right
we generated simulations from this model and summarize the test error results in figure
the test errors of principal components and partial least squares are shown at the right of the plot both are badly affected by the noisy features in the data
supervised principal components and thresholded pls work best over wide range of the number of selected features with the former showing consistently lower test errors
while this example seems tailor made for supervised principal components its good performance seems to hold in other simulated and real datasets bair et al
pre conditioning for feature selection supervised principal components can yield lower test errors than competing methods as shown in figure
however it does not always produce sparse model involving only small number of features genes
even if the thresholding in step of the algorithm yields relatively small number
high dimensional problems thresholded pls relative root mean square test error supervised principal components
root mean squared test error one standard error for supervised principal components and thresholded pls on realizations from model
all methods use one component and the errors are relative to the noise standard deviation the bayes error is
for both methods different values for the filtering threshold were tried and the number of features retained is shown on the horizontal axis
the extreme right points correspond to regular principal components and partial least squares using all the genes of features it may be that some of the omitted features have sizable inner products with the supervised principal component and could act as good surrogate
in addition highly correlated features will tend to be chosen together and there may be great deal of redundancy in the set of selected features
the lasso sections and on the other hand produces sparse model from the data
how do the test errors of the two methods compare on the simulated example of the last section
figure shows the test errors for one realization from model for the lasso supervised principal components and the pre conditioned lasso described below
we see that supervised principal components orange curve reaches its lowest error when about features are included in the model which is the correct number for the simulation
although linear model in the first features is optimal the lasso green is adversely affected by the large number of noisy features and starts overfitting when far fewer are in the model
can we get the low test error of supervised principal components along with the sparsity of the lasso
this is the goal of pre conditioning paul et al
in this approach one first computes the supervised principal component predictor for each observation in the training set with the
test errors for the lasso supervised principal components and pre conditioned lasso for one realization from model
each model is indexed by the number of non zero features
the supervised principal component path is truncated at features
the lasso self truncates at the sample size see section
in this case the pre conditioned lasso achieves the lowest error with about features threshold selected by cross validation
then we apply the lasso with as the outcome variable in place of the usual outcome yi
all features are used in the lasso fit not just those that were retained in the thresholding step in supervised principal components
the idea is that by first denoising the outcome variable the lasso should not be as adversely affected by the large number of noise features
figure shows that pre conditioning purple curve has been successful here yielding much lower test error than the usual lasso and as low in this case as for supervised principal components
it also can achieve this using less features
the usual lasso applied to the raw outcome starts to overfit more quickly than the pre conditioned version
overfitting is not problem since the outcome variable has been denoised
we usually select the tuning parameter for the pre conditioned lasso on more subjective grounds like parsimony
pre conditioning can be applied in variety of settings using initial estimates other than supervised principal components and post processors other than the lasso
more details may be found in paul et al
feature assessment and the multiple testing problem in the first part of this chapter we discuss prediction models in the setting
here we consider the more basic problem of assessing the signif
high dimensional problems icance of each of the features
consider the protein mass spectrometry example of section
in that problem the scientist might not be interested in predicting whether given patient has prostate cancer
rather the goal might be to identify proteins whose abundance differs between normal and cancer samples in order to enhance understanding of the disease and suggest targets for drug development
thus our goal is to assess the significance of individual features
this assessment is usually done without the use of multivariate predictive model like those in the first part of this chapter
the feature assessment problem moves our focus from prediction to the traditional statistical topic of multiple hypothesis testing
for the remainder of this chapter we will use instead of to denote the number of features since we will frequently be referring to values
subset of the genes from microarray study of radiation sensitivity
there are total of samples in the normal group and in the radiation sensitive group we only show three samples from each group
normal radiation sensitive gene
consider for example the microarray data in table taken from study on the sensitivity of cancer patients to ionizing radiation treatment rieger et al
each row consists of the expression of genes in patient samples samples were from patients with normal reaction and from patients who had severe reaction to radiation
the measurements were made on oligo nucleotide microarrays
the object of the experiment was to find genes whose expression was different in the radiation sensitive group of patients
there are genes altogether the table shows the data for some of the genes and samples for illustration
to identify informative genes we construct two sample statistic for each gene tj sej where kj xij
here are the indices of the samples in group where is the normal group and is the sensitive group
the quantity sej is the pooled within group standard error for gene
radiation sensitivity microarray example
histogram of the statistics comparing the radiation sensitive versus insensitive groups
overlaid in blue is the histogram of the statistics from permutations of the sample labels
sej xij xij histogram of the statistics is shown in orange in figure ranging in value from to
if the tj values were normally distributed we could consider any value greater than two in absolute value to be significantly large
this would correspond to significance level of about
here there are genes with tj
however with genes we would expect many large values to occur by chance even if the grouping is unrelated to any gene
for example if the genes were independent which they are surely not the number of falsely significant genes would have binomial distribution with mean and standard deviation the actual is way out of range
how do we assess the results for all genes
this is called the multiple testing problem
we can start as above by computing value for each gene
this can be done using the theoretical distribution probabilities which assumes the features are normally distributed
an attractive alternative approach is to use the permutation distribution since it avoids about the distribution of the data
we compute in principle assumptions all permutations of the sample labels and for each permutation compute the statistics tkj
then the value for gene is
high dimensional problems pj tkj tj
of course is large number around and so we can't enumerate all of the possible permutations
instead we take random sample of the possible permutations here we took random sample of permutations
to exploit the fact that the genes are similar measured on the same scale we can instead pool the results for all genes in computing the values
xx pj tkj tj
mk this also gives more granular values than does since there many more values in the pooled null distribution than there are in each individual null distribution
using this set of values we would like to test the hypotheses treatment has no effect on gene versus treatment has an effect on gene for all
we reject at level if pj
this test has type error equal to that is the probability of falsely rejecting is
now with many tests to consider it is not clear what we should use as an overall measure of error
let aj be the event that is falsely rejected by definition pr aj
the family wise error rate fwer is the probability of at least one false rejection and is commonly used overall measure of error
in detail if aj is the event of at least one false rejection then the fwer is pr
generally pr for large and depends on the correlation between the tests
if the tests are independent each with type error rate then the family wise error rate of the collection of tests is
on the other hand if the tests have positive dependence that is pr aj ak pr aj then the fwer will be less than
positive dependence between tests often occurs in practice in particular in genomic studies
one of the simplest approaches to multiple testing is the bonferroni method
it makes each individual test more stringent in order to make the fwer equal to at most we reject if pj
it is easy to show that the resulting fwer is exercise
the bonferroni method can be useful if is relatively small but for large it is too conservative that is it calls too few genes significant
in our example if we test at level say then we must use the threshold
none of the genes had value this small
feature assessment and the multiple testing problem there are variations to this approach that adjust the individual values to achieve an fwer of at most with some approaches avoiding the assumption of independence see dudoit et al
the false discovery rate different approach to multiple testing does not try to control the fwer but focuses instead on the proportion of falsely significant genes
as we will see this approach has strong practical appeal
table summarizes the theoretical outcomes of hypothesis tests
note that the family wise error rate is pr
here we instead focus table
possible outcomes from hypothesis tests
note that is the number of false positive tests the type error rate is
the type ii error rate is and the power is
called called not significant significant total true false total on the false discovery rate fdr
in the microarray setting this is the expected proportion of genes that are incorrectly called significant among the genes that are called significant
the expectation is taken over the population from which the data are generated
benjamini and hochberg first proposed the notion of false discovery rate and gave testing procedure algorithm whose fdr is bounded by user defined level
the benjamini hochberg bh procedure is based on values these can be obtained from an asymptotic approximation to the test statistic gaussian or permutation distribution as is done here
if the hypotheses are independent benjamini and hochberg show that regardless of how many null hypotheses are true and regardless of the distribution of the values when the null hypothesis is false this procedure has the property fdr
for illustration we chose
figure shows plot of the ordered values and the line with slope
high dimensional problems algorithm benjamini hochberg bh method
fix the false discovery rate and let denote the ordered values
define max
reject all hypotheses for which pj the bh rejection threshold
microarray example continued
shown is plot of the ordered values and the line for the benjamini hochberg method
the largest for which the value falls below the line gives the bh threshold
here this occurs at indicated by the vertical line
thus the bh method calls significant the genes in red with smallest values
feature assessment and the multiple testing problem algorithm the plug in estimate of the false discovery rate
create permutations of the data producing statistics tkj for features and permutations
for range of values of the cut point let xm xk robs tj tkj
robs

estimate the fdr by fdr starting at the left and moving right the bh method finds the last time that the values fall below the line
this occurs at so we reject the genes with smallest values
note that the cutoff occurs at the th smallest value and the th largest of the values tj is thus we reject the genes with tj
from our brief description it is not clear how the bh procedure works that is why the corresponding fdr is at most the value used for
indeed the proof of this fact is quite complicated benjamini and hochberg
more direct way to proceed is plug in approach
rather than starting with value for we fix cut point for our statistics say the value that appeared above
the number of observed values tj equal or greater than is
the total number of permutation values tkj equal or greater than is for an average of per permutation
thus direct estimate of the false discovery rate is fdr
note that is approximately equal to the value of used above the difference is due to discreteness
this procedure is summarized in algorithm
to recap the plug in estimate of fdr of algorithm is equivalent to the bh procedure of algorithm using the permutation values
this correspondence between the bh method and the plug in estimate is not coincidence
exercise shows that they are equivalent in general
note that this procedure makes no reference to values at all but rather works directly with the test statistics
the plug in estimate is based on the approximation is consistent estimate of fdr storey storey et and in general fdr actually estimates al
note that the numerator
high dimensional problems since the permutation distribution uses rather null hypotheses
hence if an estimate of is available better estimate of fdr can be exercise shows way to estimate obtained from fdr
the most conservative upwardly biased estimate of fdr uses
equivalently an estimate of can be used to improve the bh method through relation
the reader might be surprised that we chose value as large as for the fdr bound
we must remember that the fdr is not the same as type error for which is the customary choice
for the scientist the false discovery rate is the expected proportion of false positive genes among the list of genes that the statistician tells him are significant
microarray experiments with fdrs as high as might still be useful especially if they are exploratory in nature
asymmetric cutpoints and the sam procedure in the testing methods described above we used the absolute value of the test statistic tj and hence applied the same cut points to both positive and negative values of the statistic
in some experiments it might happen that most or all of the differentially expressed genes change in the positive direction or all in the negative direction
for this situation it is advantageous to derive separate cut points for the two cases
the significance analysis of microarrays sam approach offers way of doing this
the basis of the sam method is shown in figure
on the vertical axis we have plotted the ordered test statistics while the horizontal axis shows the expected order statistics from the pk permutations of the data tk where tk tk tk are the ordered test statistics from permutation
two lines are drawn parallel to the line units away
starting at the origin and moving to the right we find the first place that the genes leave the band
this defines the upper cutpoint chi and all genes beyond that point are called significant marked red
similarly we find the lower cutpoint clow for genes in the bottom left corner
thus each value of the tuning parameter defines upper and lower cutpoints and the plug in estimate fdr for each of these cutpoints is estimated as before
typically range of values of and associated fdr values are computed from which particular pair are chosen on subjective grounds
the advantage of the sam approach lies in the possible asymmetry of the cutpoints
in the example of figure with we obtain significant genes they are all in the upper right
the data points in the bottom left never leave the band and hence clow
hence for this value of no genes are called significant on the left negative side
we do not impose symmetry on the cutpoints as was done in section as there is no reason to assume similar behavior at the two ends
sam plot for the radiation sensitivity microarray data
on the vertical axis we have plotted the ordered test statistics while the horizontal axis shows the expected order statistics of the test statistics from permutations of the data
two lines are drawn parallel to the line units away from it
starting at the origin and moving to the right we find the first place that the genes leave the band
this defines the upper cut point chi and all genes beyond that point are called significant marked in red
similarly we define lower cutpoint clow
for the particular value of in the plot no genes are called significant in the bottom left
high dimensional problems there is some similarity between this approach and the asymmetry possible with likelihood ratio tests
suppose we have log likelihood tj under the null hypothesis of no effect and log likelihood tj under the alternative
then likelihood ratio test amounts to rejecting the null hypothesis if tj tj for some
depending on the likelihoods and particularly their relative values this can result in different threshold for tj than for tj
the sam procedure rejects the null hypothesis if again the threshold for each depends on the corresponding value of the null value
bayesian interpretation of the fdr there is an interesting bayesian view of the fdr developed in storey and efron and tibshirani
first we need to define the positive false discovery rate pfdr as af af af pfdr
af the additional term positive refers to the fact that we are only interested in estimating an error rate where positive findings have occurred
it is this slightly modified version of the fdr that has clean bayesian interpretation
note that the usual fdr expression is not defined if pr
let be rejection region for single test in the example above we used
suppose that identical simple hypothesis tests are performed with the statistics tm and rejection region
we define random variable zj which equals if the jth null hypothesis is true and otherwise
we assume that each pair tj zj are random variables with tj zj zj zj for some distributions and
this says that each test statistic tj comes from one of two distributions if the null hypothesis is true and otherwise
letting pr zj marginally we have tj
then it can be shown efron et al storey that
bibliographic notes pfdr pr zj tj
hence under the mixture model the pfdr is the posterior probability that the null hypothesis it true given that test statistic falls in the rejection region for the test that is given that we reject the null hypothesis exercise
the false discovery rate provides measure of accuracy for tests based on an entire rejection region such as tj
but if the fdr of such test is say then gene with say tj will be more significant than gene with tj
thus it is of interest to derive local gene specific version of the fdr
the value storey of test statistic tj is defined to be the smallest fdr over all rejection regions that reject tj
that is for symmetric rejection regions the value for tj is defined to be the fdr for the rejection region
thus the value for tj will be smaller than that for tj reflecting the fact that tj is more significant than tj
the local false discovery rate efron and tibshirani at is defined to be pr zj tj
this is the positive fdr for an infinitesimal rejection region surrounding the value tj
bibliographic notes many references were given at specific points in this chapter we give some additional ones here
dudoit et al give an overview and comparison of discrimination methods for gene expression data
levina does some mathematical analysis comparing diagonal lda to full lda as with
she shows that with reasonable assumptions diagonal lda has lower asymptotic error rate than full lda
tibshirani et al and tibshirani et al proposed the nearest shrunken centroid classifier
zhu and hastie study regularized logistic regression
highdimensional regression and the lasso are very active areas of research and many references are given in section
the fused lasso was proposed by tibshirani et al while zou and hastie introduced the elastic net
supervised principal components is discussed in bair and tibshirani and bair et al
for an introduction to the analysis of censored survival data see kalbfleisch and prentice
microarray technology has led to flurry of statistical research see for example the books by speed parmigiani et al
simon et al and lee
the false discovery rate was proposed by benjamini and hochberg and studied and generalized in subsequent papers by these authors and
high dimensional problems many others
partial list of papers on fdr may be found on yoav benjamini's homepage
some more recent papers include efron and tibshirani storey genovese and wasserman storey and tibshirani and benjamini and yekutieli
dudoit et al review methods for identifying differentially expressed genes in microarray studies
exercises ex
for coefficient estimate let be the normalized version
show that as bb the normalized ridge regression estimates converge to the renormalized partial least squares one component estimates
nearest shrunken centroids and the lasso
consider naive bayes gaussian model for classification in which the features are assumed to be independent within each class
with observations and ck equal to the set of indices of the nk observations in class we observe xij j jk for ck with pk jk
set sj the pooled within class variance for feature and consider the lasso style minimization problem fc xp xij j jk xk jk fd min bb nk
j jk ck sj fe show that the solution is equivalent to the nearest shrunken centroid estimator with set to zero and mk equal to nk instead of nk as before
show that the fitted coefficients pfor the regularized multiclass logistic regression problem satisfy kj
what about the
discuss issues with these constant parameters and how they can be resolved
derive the computational formula for ridge regression
hint use the first derivative of the penalized sum of squares criterion to show that if bb then xt for some irn
prove the theorem in section by decomposing and the rows of into their projections into the column space of and its complement in irp
show how the theorem in section can be applied to regularized discriminant analysis section and equation
exercises ex
consider linear regression problem where and assume the rank of is
let the svd of udvt rvt where is nonsingular and is with orthonormal columns show that there are infinitely many least squares solutions all with zero residuals show that the ridge regression estimate for can be written bb rt bb rt show that when bb the solution vd ut has residuals all equal to zero and is unique in that it has the smallest euclidean norm amongst all zero residual solutions
data piling
exercise shows that the two class lda solution can be obtained by linear regression of binary response vector consisting of and
the prediction for any is up to scale and shift the lda score
suppose now that consider the linear regression model fit to binary response
using exercise show that there are infinitely many directions defined by in irp onto which the data project to exactly two points one for each class
these are known as data piling directions ahn and marron show that the distance between the projected points is and hence these directions define separating hyperplanes with that margin argue that there is single maximal data piling direction for which this distance is largest and is defined by vd ut where udvt is the svd of
compare the data piling direction of exercise to the direction of the optimal separating hyperplane section qualitatively
which makes the widest margin and why
use small simulation to demonstrate the difference
when linear discriminant analysis see section is degenerate because the within class covariance matrix is singular
one version of regularized discriminant analysis replaces by ridged version bb leading to regularized discriminant function bb xt bb
show that lim bb bb corresponds to the maximal data piling direction defined in exercise
suppose you have sample of pairs xi yi with yi binary and xi ir
suppose also that the two classes are separable for each
high dimensional problems pair with yi and yi xi xi for some
you wish to fit linear logistic regression model logitpr by maximum likelihood
show that is undefined
suppose we wish to select the ridge parameter bb by fold crossvalidation in situation for any linear model
we wish to use the computational shortcuts described in section
show that we need only to reduce the matrix to the matrix once and can use it in all the cross validation runs
suppose our predictors are presented as an innerproduct matrix xxt and we wish to fit the equivalent of linear logistic regression model in the original features with quadratic regularization
our predictions are also to be made using inner products new is presented as xx
let ud ut be the eigen decomposition of
show that the predictions are given by where ud and is the ridged logistic regression estimate with input matrix ud
argue that the same approach can be used for any appropriate kernel matrix
distance weighted nn classification
consider the nearestneighbor method section in two class classification problem
let be the shortest distance to training observation in class and likewise the shortest distance for class
let be the number of samples in class the number in class and show that log can be viewed as nonparametric discriminant function corresponding to nn classification
hint show that can be viewed as nonparametric estimate of the density in class at how would you modify this function to introduce class prior probabilities and different from the sample priors and
how would you generalize this approach for nn classification
kernel pca
in section we show how to compute the principal component variables from an uncentered inner product matrix
we compute the eigen decomposition ud ut with and then ud
suppose we have the inner product
exercises vector containing the inner products between new point and each of the xi in our training set
show that the centered projections of onto the principal component directions are given by ut
bonferroni method for multiple comparisons
suppose we are in multiple testing scenario with null hypotheses and corresponding values pj
let be the event that at least one null hypothesis is falsely rejected and let aj be the event that the jth null hypothesis is falsely rejected
suppose that we use the bonferroni method rejecting the jth null hypothesis if pj show that pr
hint pr aj aj pr aj pr aj pr aj aj if the hypotheses are independent then pr qm pr ac pr ac
use this to show that pr in this case
equivalence between benjamini hochberg and plug in methods in the notation of algorithm show that for rejection threshold proportion of at most of the permuted values tkj exceed where is the lth largest value among the tj
hence show that the plug in fdr estimate fdr is less than or equal to show that the cut point produces test with estimated fdr greater than
use result to show that type error of pfdr type error of power of storey
consider the data in table of section available from the book website using symmetric two sided rejection region based on the statistic compute the plug in estimate of the fdr for various values of the cut point carry out the bh procedure for various fdr levels and show the equivalence of your results with those from part
high dimensional problems let be the quartiles of the statistics from the permuted datasets
let tj and set min
multiply the fdr estimates from by and examine the results give motivation for the estimate in part
storey ex
proof of result
write
pfdr pr use the fact that given is binomial random variable with trials and probability of success pr to complete the proof
this is page printer opaque this references abu mostafa
hints neural computation
ackley
hinton and sejnowski
learning algorithm for boltzmann machines trends in cognitive sciences
adam qu davis
ward
clements
cazares
semmes
schellhammer
yasui feng and wright
serum protein fingerprinting coupled with pattern matching algorithm distinguishes prostate cancer from benign prostate hyperplasia and healthy mean cancer research
agrawal mannila srikant toivonen and verkamo
fast discovery of association rules advances in knowledge discovery and data mining aaai mit press cambridge ma
agresti
an introduction to categorical data analysis wiley new york
agresti
categorical data analysis nd ed wiley new york
ahn and marron
the direction of maximal data piling in high dimensional space technical report statistics department university of north carolina chapel hill
akaike
information theory and an extension of the maximum likelihood principle second international symposium on information theory pp
references allen
the relationship between variable selection and data augmentation and method of prediction technometrics
ambroise and mclachlan
selection bias in gene extraction on the basis of microarray gene expression data proceedings of the national academy of sciences
amit and geman
shape quantization and recognition with randomized trees neural computation
anderson and rosenfeld eds
neurocomputing foundations of research mit press cambridge ma
anderson
an introduction to multivariate statistical analysis rd ed wiley new york
bach and jordan
kernel independent component analysis journal of machine learning research
bair and tibshirani
semi supervised methods to predict patient survival from gene expression data plos biology
bair hastie paul and tibshirani
prediction by supervised principal components journal of the american statistical association
bakin
adaptive regression and model selection in data mining problems technical report phd thesis australian national university canberra
banerjee ghaoui
and d'aspremont
model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data journal of machine learning research
barron
universal approximation bounds for superpositions of sigmoid function ieee transactions on information theory
bartlett and traskin
adaboost is consistent in
scho lkopf
platt and
hoffman eds advances in neural information processing systems mit press cambridge ma pp
becker cleveland and shyu
the visual design and control of trellis display journal of computational and graphical statistics
bell and sejnowski
an information maximization approach to blind separation and blind deconvolution neural computation
references bellman
adaptive control processes princeton university press
benjamini and hochberg
controlling the false discovery rate practical and powerful approach to multiple testing journal of the royal statistical society series
benjamini and yekutieli
false discovery rate controlling confidence intervals for selected parameters journal of the american statistical association
bickel and levina
some theory for fisher's linear discriminant function naive bayes and some alternatives when there are many more variables than observations bernoulli
bickel
ritov and tsybakov
simultaneous analysis of lasso and dantzig selector annals of statistics to appear
bishop
neural networks for pattern recognition clarendon press oxford
bishop
pattern recognition and machine learning springer new york
bishop fienberg and holland
discrete multivariate analysis mit press cambridge ma
boyd and vandenberghe
convex optimization cambridge university press
breiman
the little bootstrap and other methods for dimensionality selection in regression fixed prediction error journal of the american statistical association
breiman
bagging predictors machine learning
breiman
stacked regressions machine learning
breiman
arcing classifiers with discussion annals of statistics
breiman
prediction games and arcing algorithms neural computation
breiman
random forests machine learning
breiman and friedman
predicting multivariate responses in multiple linear regression with discussion journal of the royal statistical society series
references breiman and ihaka
nonlinear discriminant analysis via scaling and ace technical report university of california berkeley
breiman and spector
submodel selection and evaluation in regression the random case international statistical review
breiman friedman olshen and stone
classification and regression trees wadsworth new york
bremaud
markov chains gibbs fields monte carlo simulation and queues springer new york
brown spiegelman and denham
chemometrics and spectral frequency selection transactions of the royal society of london series
bruce and gao
applied wavelet analysis with plus springer new york
bu hlmann and hothorn
boosting algorithms regularization prediction and model fitting with discussion statistical science
buja hastie and tibshirani
linear smoothers and additive models with discussion annals of statistics
buja swayne littman hofmann and chen
data vizualization with multidimensional scaling journal of computational and graphical statistics to appear
bunea tsybakov and wegkamp
sparsity oracle inequalities for the lasso electronic journal of statistics
burges
tutorial on support vector machines for pattern recognition knowledge discovery and data mining
butte tamayo slonim golub and kohane
discovering functional relationships between rna expression and chemotherapeutic susceptibility using relevance networks proceedings of the national academy of sciences pp
candes
compressive sampling proceedings of the international congress of mathematicians european mathematical society madrid spain
candes and tao
the dantzig selector statistical estimation when is much larger than annals of statistics
references chambers and hastie
statistical models in wadsworth brooks cole pacific grove ca
chaudhuri drton and richardson
estimation of covariance matrix with zeros biometrika
chen and buja
local multidimensional scaling for nonlinear dimension reduction graph drawing and proximity analysis journal of the american statistical association
chen
donoho and saunders
atomic decomposition by basis pursuit siam journal on scientific computing
cherkassky and ma
comparison of model selection for regression neural computation
cherkassky and mulier
learning from data nd edition wiley new york
chui
an introduction to wavelets academic press london
clifford
markov random fields in statistics in
grimmett and
welsh eds disorder in physical systems
volume in honour of john
hammersley clarendon press oxford pp
comon
independent component analysis new concept signal processing
cook and swayne
interactive and dynamic graphics for data analysis with and ggobi springer new york
with contributions from
buja
temple lang
hofmann
wickham and
cook
use and misuse of the receiver operating characteristic curve in risk prediction circulation
copas
regression prediction and shrinkage with discussion journal of the royal statistical society series methodological
cover and hart
nearest neighbor pattern classification ieee transactions on information theory it
cover and thomas
elements of information theory wiley new york
cox and hinkley
theoretical statistics chapman and hall london
references cox and wermuth
multivariate dependencies models analysis and interpretation chapman and hall london
cressie
statistics for spatial data revised edition wileyinterscience new york
csiszar and tusna dy
information geometry and alternating minimization procedures statistics decisions supplement issue
cutler and breiman
archetypal analysis technometrics
dasarathy
nearest neighbor pattern classification techniques ieee computer society press los alamitos ca
daubechies
ten lectures in wavelets society for industrial and applied mathematics philadelphia pa
daubechies defrise and de mol
an iterative thresholding algorithm for linear inverse problems with sparsity constraint communications on pure and applied mathematics de boor
practical guide to splines springer new york
dempster
covariance selection biometrics
dempster laird and rubin
maximum likelihood from incomplete data via the em algorithm with discussion journal of the royal statistical society series
devijver and kittler
pattern recognition statistical approach prentice hall englewood cliffs
dietterich
ensemble methods in machine learning lecture notes in computer science
dietterich
an experimental comparison of three methods for constructing ensembles of decision trees bagging boosting and randomization machine learning
dietterich and bakiri
solving multiclass learning problems via error correcting output codes journal of artificial intelligence research
donath
and hoffman
lower bounds for the partitioning of graphs ibm journal of research and development pp
donoho
compressed sensing ieee transactions on information theory
references donoho
for most large underdetermined systems of equations the minimal norm solution is the sparsest solution communications on pure and applied mathematics
donoho and elad
optimally sparse representation from overcomplete dictionaries via norm minimization proceedings of the national academy of sciences
donoho and johnstone
ideal spatial adaptation by wavelet shrinkage biometrika
donoho and stodden
when does non negative matrix factorization give correct decomposition into parts in
thrun
saul and
scho lkopf eds advances in neural information processing systems mit press cambridge ma
duan and li
slicing regression link free regression method annals of statistics
duchamp and stuetzle
extremal properties of principal curves in the plane annals of statistics
duda hart and stork
pattern classification nd edition wiley new york
dudoit fridlyand and speed
comparison of discrimination methods for the classification of tumors using gene expression data journal of the american statistical association
dudoit yang callow and speed
statistical methods for identifying differentially expressed genes in replicated cdna microarray experiments statistica sinica pp
edwards
introduction to graphical modelling nd edition springer new york
efron
the efficiency of logistic regression compared to normal discriminant analysis journal of the american statistical association
efron
bootstrap methods another look at the jackknife annals of statistics
efron
estimating the error rate of prediction rule some improvements on cross validation journal of the american statistical association
efron
how biased is the apparent error rate of prediction rule journal of the american statistical association
references efron and tibshirani
statistical analysis in the computer age science
efron and tibshirani
an introduction to the bootstrap chapman and hall london
efron and tibshirani
using specially designed exponential families for density estimation annals of statistics
efron and tibshirani
improvements on cross validation the bootstrap method journal of the american statistical association
efron and tibshirani
microarrays empirical bayes methods and false discovery rates genetic epidemiology
efron hastie and tibshirani
discussion of dantzig selector by candes and tao annals of statistics
efron hastie johnstone and tibshirani
least angle regression with discussion annals of statistics
efron tibshirani storey and tusher
empirical bayes analysis of microarray experiment journal of the american statistical association
evgeniou pontil and poggio
regularization networks and support vector machines advances in computational mathematics
fan and fan
high dimensional classification using features annealed independence rules annals of statistics to appear
fan and gijbels
local polynomial modelling and its applications chapman and hall london
fan and li
variable selection via nonconcave penalized likelihood and its oracle properties journal of the american statistical association
fiedler
algebraic connectivity of graphs czechoslovak mathematics journal
fienberg
the analysis of cross classified categorical data mit press cambridge
fisher
the use of multiple measurements in taxonomic problems eugen
references fisher
on grouping for maximum homogeniety journal of the american statistical association
fix and hodges
discriminatory analysis nonparametric discrimination consistency properties technical report
air force school of aviation medicine randolph field tx
flury
principal points biometrika
forgy
cluster analysis of multivariate data efficiency vs interpretability of classifications biometrics
frank and friedman
statistical view of some chemometrics regression tools with discussion technometrics
freund
boosting weak learning algorithm by majority information and computation
freund and schapire
experiments with new boosting algorithm machine learning proceedings of the thirteenth international conference morgan kauffman san francisco pp
freund and schapire
game theory on line prediction and boosting proceedings of the ninth annual conference on computational learning theory desenzano del garda italy pp
freund and schapire
decision theoretic generalization of online learning and an application to boosting journal of computer and system sciences
friedman
exploratory projection pursuit journal of the american statistical association
friedman
regularized discriminant analysis journal of the american statistical association
friedman
multivariate adaptive regression splines with discussion annals of statistics
friedman
flexible metric nearest neighbor classification technical report stanford university
friedman
an overview of predictive learning and function approximation in
cherkassky
friedman and
wechsler eds from statistics to neural networks vol of nato isi series springer new york
friedman
another approach to polychotomous classification technical report stanford university
references friedman
on bias variance loss and the curse of dimensionality journal of data mining and knowledge discovery
friedman
stochastic gradient boosting technical report stanford university
friedman
greedy function approximation gradient boosting machine annals of statistics
friedman and fisher
bump hunting in high dimensional data statistics and computing
friedman and hall
on bagging and nonlinear estimation journal of statistical planning and inference
friedman and popescu
importance sampled learning ensembles technical report stanford university department of statistics
friedman and popescu
predictive learning via rule ensembles annals of applied statistics to appear
friedman and silverman
flexible parsimonious smoothing and additive modelling with discussion technometrics
friedman and stuetzle
projection pursuit regression journal of the american statistical association
friedman and tukey
projection pursuit algorithm for exploratory data analysis ieee transactions on computers series
friedman baskett and shustek
an algorithm for finding nearest neighbors ieee transactions on computers
friedman bentley and finkel
an algorthm for finding best matches in logarithmic expected time acm transactions on mathematical software
friedman hastie and tibshirani
additive logistic regression statistical view of boosting with discussion annals of statistics
friedman hastie and tibshirani
response to mease and wyner evidence contrary to the statistical view of boosting journal of machine learning research
friedman hastie and tibshirani
sparse inverse covariance estimation with the graphical lasso biostatistics
references friedman hastie and tibshirani
regularization paths for generalized linear models via coordinate descent journal of statistical software
friedman hastie hoefling and tibshirani
pathwise coordinate optimization annals of applied statistics
friedman hastie rosset tibshirani and zhu
discussion of three boosting papers by jiang lugosi and vayatis and zhang annals of statistics
friedman stuetzle and schroeder
projection pursuit density estimation journal of the american statistical association
fu
penalized regressions the bridge vs the lasso journal of computational and graphical statistics
furnival and wilson
regression by leaps and bounds technometrics
gelfand and smith
sampling based approaches to calculating marginal densities journal of the american statistical association
gelman carlin stern and rubin
bayesian data analysis crc press boca raton fl
geman and geman
stochastic relaxation gibbs distributions and the bayesian restoration of images ieee transactions on pattern analysis and machine intelligence
genkin lewis and madigan
large scale bayesian logistic regression for text categorization technometrics
genovese and wasserman
stochastic process approach to false discovery rates annals of statistics
gersho and gray
vector quantization and signal compression kluwer academic publishers boston ma
girosi jones and poggio
regularization theory and neural network architectures neural computation
golub and van loan
matrix computations johns hopkins university press baltimore
golub heath and wahba
generalized cross validation as method for choosing good ridge parameter technometrics
references golub slonim tamayo huard gaasenbeek mesirov coller loh downing caligiuri bloomfield and lander
molecular classification of cancer class discovery and class prediction by gene expression monitoring science
goodall
procrustes methods in the statistical analysis of shape journal of the royal statistical society series
gordon
classification nd edition chapman and hall crc press london
green and silverman
nonparametric regression and generalized linear models roughness penalty approach chapman and hall london
greenacre
theory and applications of correspondence analysis academic press new york
greenshtein and ritov
persistence in high dimensional linear predictor selection and the virtue of overparametrization bernoulli
guo hastie and tibshirani
regularized linear discriminant analysis and its application in microarrays biostatistics
guyon gunn nikravesh and zadeh eds
feature extraction foundations and applications springer new york
guyon weston barnhill and vapnik
gene selection for cancer classification using support vector machines machine learning
hall
the bootstrap and edgeworth expansion springer new york
hammersley
and clifford
markov field on finite graphs and lattices unpublished
hand
discrimination and classification wiley chichester
hanley and mcneil
the meaning and use of the area under receiver operating characteristic roc curve radiology
hart
the condensed nearest neighbor rule ieee transactions on information theory
hartigan
clustering algorithms wiley new york
references hartigan
and wong
algorithm as means clustering algorithm as applied statistics
hastie
principal curves and surfaces phd thesis stanford university
hastie and herman
an analysis of gestational age neonatal size and neonatal death using nonparametric logistic regression journal of clinical epidemiology
hastie and simard
models and metrics for handwritten digit recognition statistical science
hastie and stuetzle
principal curves journal of the american statistical association
hastie and tibshirani
nonparametric logistic and proportional odds regression applied statistics
hastie and tibshirani
generalized additive models chapman and hall london
hastie and tibshirani
discriminant adaptive nearestneighbor classification ieee pattern recognition and machine intelligence
hastie and tibshirani
discriminant analysis by gaussian mixtures journal of the royal statistical society series
hastie and tibshirani
classification by pairwise coupling annals of statistics
hastie and tibshirani
independent components analysis through product density estimation in
becker and
obermayer eds advances in neural information processing systems mit press cambridge ma pp
hastie and tibshirani
efficient quadratic regularization for expression arrays biostatistics
hastie and zhu
discussion of support vector machines with applications by javier moguerza and alberto munoz statistical science
hastie botha and schnitzler
regression with an ordered categorical response statistics in medicine
references hastie buja and tibshirani
penalized discriminant analysis annals of statistics
hastie kishon clark and fan
model for signature verification technical report at bell laboratories http www stat stanford edu hastie papers signature pdf
hastie rosset tibshirani and zhu
the entire regularization path for the support vector machine journal of machine learning research
hastie taylor tibshirani and walther
forward stagewise regression and the monotone lasso electronic journal of statistics
hastie tibshirani and buja
flexible discriminant analysis by optimal scoring journal of the american statistical association
hastie tibshirani and buja
flexible discriminant and mixture models in
kay and
titterington eds statistics and artificial neural networks oxford university press
hastie tibshirani and friedman
note on comparison of model selection for regression by cherkassky and ma neural computation
hathaway
another interpretation of the em algorithm for mixture distributions statistics probability letters
hebb
the organization of behavior wiley new york
hertz krogh and palmer
introduction to the theory of neural computation addison wesley redwood city ca
hinton
connectionist learning procedures artificial intelligence
hinton
training products of experts by minimizing contrastive divergence neural computation
hinton osindero and teh
fast learning algorithm for deep belief nets neural computation
ho
random decision forests in
kavavaugh and
storms eds proc
third international conference on document analysis and recognition vol
ieee computer society press new york pp
references hoefling and tibshirani
estimation of sparse markov networks using modified logistic regression and the lasso submitted
hoerl
and kennard
ridge regression biased estimation for nonorthogonal problems technometrics
hothorn and bu hlmann
model based boosting in high dimensions bioinformatics
huber
robust estimation of location parameter annals of mathematical statistics
huber
projection pursuit annals of statistics
hunter and lange
tutorial on mm algorithms the american statistician
hyva rinen and oja
independent component analysis algorithms and applications neural networks
hyva rinen karhunen and oja
independent component analysis wiley new york
izenman
reduced rank regression for the multivariate linear model journal of multivariate analysis
jacobs jordan nowlan and hinton
adaptive mixtures of local experts neural computation
jain and dubes
algorithms for clustering data prenticehall englewood cliffs
james and hastie
the error coding method and picts journal of computational and graphical statistics
jancey
multidimensional group analysis australian journal of botany
jensen
lauritzen and olesen
bayesian updating in recursive graphical models by local computation computational statistics quarterly
jiang
process consistency for adaboost annals of statistics
jirous ek and pr euc il
on the effective implementation of the iterative proportional fitting procedure computational statistics and data analysis
johnson
study of the nips feature selection challenge submitted
references joliffe
trendafilov
and uddin
modified principal component technique based on the lasso journal of computational and graphical statistics
jones
simple lemma on greedy approximation in hilbert space and convergence rates for projection pursuit regression and neural network training annals of statistics
jordan
graphical models statistical science special issue on bayesian statistics
jordan and jacobs
hierachical mixtures of experts and the em algorithm neural computation
kalbfleisch and prentice
the statistical analysis of failure time data wiley new york
kaufman and rousseeuw
finding groups in data an introduction to cluster analysis wiley new york
kearns and vazirani
an introduction to computational learning theory mit press cambridge ma
kittler hatef duin and matas
on combining classifiers ieee transaction on pattern analysis and machine intelligence
kleinberg
stochastic discrimination annals of mathematical artificial intelligence
kleinberg
an overtraining resistant stochastic modeling method for pattern recognition annals of statistics
knight and fu
asymptotics for lasso type estimators annals of statistics
koh kim and boyd
an interior point method for large scale regularized logistic regression journal of machine learning research
kohavi
study of cross validation and bootstrap for accuracy estimation and model selection international joint conference on artificial intelligence ijcai morgan kaufmann pp
kohonen
self organization and associative memory rd edition springer berlin
kohonen
the self organizing map proceedings of the ieee
references kohonen kaski lagus saloja rvi paatero and saarela
self organization of massive document collection ieee transactions on neural networks
special issue on neural networks for data mining and knowledge discovery
koller and friedman
structured probabilistic models stanford bookstore custom publishing
unpublished draft
kressel
pairwise classification and support vector machines in
scho lkopf
burges and
smola eds advances in kernel methods support vector learning mit press cambridge ma pp
lambert
zero inflated poisson regression with an application to defects in manufacturing technometrics
lange
optimization springer new york
lauritzen
graphical models oxford university press
lauritzen and spiegelhalter
local computations with probabilities on graphical structures and their application to expert systems
royal statistical society
lawson and hansen
solving least squares problems prentice hall englewood cliffs nj
le cun
generalization and network design strategies technical report crg tr department of computer science univ of toronto
le cun boser denker henderson howard hubbard and jackel
handwritten digit recognition with backpropogation network in
touretzky ed advances in neural information processing systems vol
morgan kaufman denver co pp
le cun bottou bengio and haffner
gradient based learning applied to document recognition proceedings of the ieee
leathwick elith francis hastie and taylor
variation in demersal fish species richness in the oceans surrounding new zealand an analysis using boosted regression trees marine ecology progress series
leathwick rowe richardson elith and hastie
using multivariate adaptive regression splines to predict the distributions of new zealand's freshwater diadromous fish freshwater biology
references leblanc and tibshirani
combining estimates in regression and classification journal of the american statistical association
lecun bottou bengio and haffner
gradient based learning applied to document recognition proceedings of the ieee
lee and seung
learning the parts of objects by non negative matrix factorization nature
lee and seung
algorithms for non negative matrix factorization advances in neural information processing systems nips vol
morgan kaufman denver pp
lee
analysis of microarray gene expression data kluwer academic publishers
lee ganapathi and koller
efficient structure learning of markov networks using regularization in
scho lkopf
platt and
hoffman eds advances in neural information processing systems mit press cambridge ma pp
leslie eskin cohen weston and noble
mismatch string kernels for discriminative protein classification bioinformatics
levina
statistical issues in texture analysis phd thesis department of statistics university of california berkeley
lin mcculloch turnbull slate and clark
latent class mixed model for analyzing biomarker trajectories in longitudinal data with irregularly scheduled observations statistics in medicine
lin and zhang
component selection and smoothing in smoothing spline analysis of variance models annals of statistics
little and rubin
statistical analysis with missing data nd edition wiley new york
lloyd
least squares quantization in pcm technical report bell laboratories
published in in ieee transactions on information theory
loader
local regression and likelihood springer new york
references loh and vanichsetakul
tree structured classification via generalized discriminant analysis journal of the american statistical association
lugosi and vayatis
on the bayes risk consistency of regularized boosting methods annals of statistics
macnaughton smith williams dale and mockett
dissimilarity analysis new technique of hierarchical subdivision nature
mackay
practical bayesian framework for backpropagation neural networks neural computation
macqueen
some methods for classification and analysis of multivariate observations proceedings of the fifth berkeley symposium on mathematical statistics and probability eds

lecam and
neyman university of california press pp
madigan and raftery
model selection and accounting for model uncertainty using occam's window journal of the american statistical association
mardia kent and bibby
multivariate analysis academic press
mason baxter bartlett and frean
boosting algorithms as gradient descent
massart plastria and kaufman
non hierarchical clustering with masloc the journal of the pattern recognition society
mccullagh and nelder
generalized linear models chapman and hall london
mcculloch and pitts
logical calculus of the ideas imminent in nervous activity bulletin of mathematical biophysics
reprinted in anderson and rosenfeld pp
mclachlan
discriminant analysis and statistical pattern recognition wiley new york
mease and wyner
evidence contrary to the statistical view of boosting with discussion journal of machine learning research
meinshausen
lasso with relaxation computational statistics and data analysis
references meinshausen and bu hlmann
high dimensional graphs and variable selection with the lasso annals of statistics
meir and ra tsch
an introduction to boosting and leveraging in
mendelson and
smola eds lecture notes in computer science advanced lectures in machine learning springer new york
michie spiegelhalter and taylor eds
machine learning neural and statistical classification ellis horwood series in artificial intelligence ellis horwood
morgan
and sonquist
problems in the analysis of survey data and proposal journal of the american statistical association
murray gill and wright
practical optimization academic press
myles and hand
the multiclass metric problem in nearest neighbor classification pattern recognition
nadler and coifman
an exact asymptotic formula for the error in cls and in pls the importance of dimensional reduction in multivariate calibration journal of chemometrics
neal
bayesian learning for neural networks springer new york
neal and hinton
view of the em algorithm that justifies incremental sparse and other variants in learning in graphical models
jordan ed dordrecht kluwer academic publishers boston ma pp
neal and zhang
high dimensional classification with bayesian neural networks and dirichlet diffusion trees in
guyon
gunn
nikravesh and
zadeh eds feature extraction foundations and applications springer new york pp
onton and makeig
information based modeling of eventrelated brain dynamics in neuper and klimesch eds progress in brain research vol
elsevier pp
osborne presnell and turlach
new approach to variable selection in least squares problems ima journal of numerical analysis
osborne presnell and turlach
on the lasso and its dual journal of computational and graphical statistics
references pace
and barry
sparse spatial autoregressions statistics and probability letters
page brin motwani and winograd
the pagerank citation ranking bringing order to the web technical report stanford digital library technologies project http citeseer ist psu edu page pagerank html
park
and hastie
regularization path algorithm for generalized linear models journal of the royal statistical society series
parker
learning logic technical report tr cambridge ma mit center for research in computational economics and management science
parmigiani garett
irizarry
and zeger
eds
the analysis of gene expression data springer new york
paul bair hastie and tibshirani
pre conditioning for feature selection and regression in high dimensional problems annals of statistics
pearl
on evidential reasoning in hierarchy of hypotheses artificial intelligence
pearl
probabilistic reasoning in intelligent systems networks of plausible inference morgan kaufmann san francisco ca
pearl
causality models reasoning and inference cambridge university press
peterson and anderson
mean field theory learning algorithm for neural networks complex systems
petricoin
ardekani
hitt
levine
fusaro steinberg
mills
simone fishman
kohn and liotta
use of proteomic patterns in serum to identify ovarian cancer lancet
platt
fast training of support vector machines using sequential minimal optimization in advances in kernel methods support vector learning
scho lkopf and
burges and
smola eds mit press cambridge ma pp
quinlan
programs for machine learning morgan kaufmann san mateo
quinlan
www rulequest com
references ramaswamy tamayo rifkin mukherjee yeang angelo ladd reich latulippe mesirov poggio gerald loda lander and golub
multiclass cancer diagnosis using tumor gene expression signature pnas
ramsay and silverman
functional data analysis springer new york
rao
linear statistical inference and its applications wiley new york
ra tsch and warmuth
maximizing the margin with boosting proceedings of the th annual conference on computational learning theory pp
ravikumar liu lafferty and wasserman
spam sparse additive models in
platt
koller
singer and
roweis eds advances in neural information processing systems mit press cambridge ma pp
ridgeway
the state of boosting computing science and statistics
rieger hong tusher tang tibshirani and chu
toxicity from radiation therapy associated with abnormal transcriptional responses to dna damage proceedings of the national academy of sciences
ripley
pattern recognition and neural networks cambridge university press
rissanen
universal prior for integers and estimation by minimum description length annals of statistics
robbins and munro
stochastic approximation method annals of mathematical statistics
roosen and hastie
automatic smoothing spline projection pursuit journal of computational and graphical statistics
rosenblatt
the perceptron probabilistic model for information storage and organization in the brain psychological review
rosenblatt
principles of neurodynamics perceptrons and the theory of brain mechanisms spartan washington
references rosenwald wright chan
connors
campo fisher
gascoyne
muller hermelink
smeland
and staudt
the use of molecular profiling to predict survival after chemotherapy for diffuse large cell lymphoma the new england journal of medicine
rosset and zhu
piecewise linear regularized solution paths annals of statistics
rosset zhu and hastie
boosting as regularized path to maximum margin classifier journal of machine learning research
rosset zhu and hastie
margin maximizing loss functions in
thrun
saul and
scho lkopf eds advances in neural information processing systems mit press cambridge ma
rousseauw du plessis benade jordaan kotze jooste and ferreira
coronary risk factor screening in three rural communities south african medical journal
roweis
and saul
locally linear embedding science
rumelhart hinton and williams
learning internal representations by error propagation in
rumelhart and
mcclelland eds parallel distributed processing explorations in the microstructure of cognition the mit press cambridge ma pp
sachs perez pe'er lauffenburger and nolan
causal protein signaling networks derived from multiparameter singlecell data science
schapire
the strength of weak learnability machine learning
schapire
the boosting approach to machine learning an overview in
denison
hansen
holmes
mallick and
yu eds msri workshop on nonlinear estimation and classification springer new york
schapire and singer
improved boosting algorithms using confidence rated predictions machine learning
schapire freund bartlett and lee
boosting the margin new explanation for the effectiveness of voting methods annals of statistics
references scho lkopf smola and mu ller
kernel principal component analysis in
scho lkopf
burges and
smola eds advances in kernel methods support vector learning mit press cambridge ma usa pp
schwarz
estimating the dimension of model annals of statistics
scott
multivariate density estimation theory practice and visualization wiley new york
seber
multivariate observations wiley new york
segal
machine learning benchmarks and random forest regression technical report escholarship repository university of california http repositories edlib org cbmb bench rf regn
shao
bootstrap model selection journal of the american statistical association
shenoy and shafer
an axiomatic framework for bayesian and belief function propagation aaai workshop on uncertainty in ai north holland pp
short and fukunaga
the optimal distance measure for nearest neighbor classification ieee transactions on information theory
silverman
density estimation for statistics and data analysis chapman and hall london
silvey
statistical inference chapman and hall london
simard cun
and denker
efficient pattern recognition using new transformation distance advances in neural information processing systems morgan kaufman san mateo ca pp
simon
korn
mcshane
radmacher
wright and zhao
design and analysis of dna microarray investigations springer new york
sjo strand rostrup ryberg larsen studholme baezner ferro fazekas pantoni inzitari and waldemar
sparse decomposition and modeling of anatomical shape variation ieee transactions on medical imaging
speed and kiiveri
gaussian markov distributions over finite graphs annals of statistics
references speed ed
statistical analysis of gene expression microarray data chapman and hall london
spiegelhalter best gilks and inskip
hepatitis case study in mcmc methods in
gilks
richardson and
spegelhalter eds markov chain monte carlo in practice interdisciplinary statistics chapman and hall london pp
spielman
and teng
spectral partitioning works planar graphs and finite element meshes ieee symposium on foundations of computer science pp
stamey kabalin mcneal johnstone freiha redwine and yang
prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate ii radical prostatectomy treated patients journal of urology
stone hansen kooperberg and truong
polynomial splines and their tensor products with discussion annals of statistics
stone
cross validatory choice and assessment of statistical predictions journal of the royal statistical society series
stone
an asymptotic equivalence of choice of model by crossvalidation and akaike's criterion journal of the royal statistical society series
stone and brooks
continuum regression cross validated sequentially constructed prediction embracing ordinary least squares partial least squares and principal components regression corr journal of the royal statistical society series
storey
direct approach to false discovery rates journal of the royal statistical society
storey
the positive false discovery rate bayesian interpretation and the value annals of statistics
storey and tibshirani
statistical significance for genomewide studies proceedings of the national academy of sciences
storey taylor and siegmund
strong control conservative point estimation and simultaneous conservative consistency of false discovery rates unified approach journal of the royal statistical society series
references surowiecki
the wisdom of crowds why the many are smarter than the few and how collective wisdom shapes business economics societies and nations little brown
swayne cook and buja
xgobi interactive dynamic graphics in the window system with link to asa proceedings of section on statistical graphics pp
tanner and wong
the calculation of posterior distributions by data augmentation with discussion journal of the american statistical association
tarpey and flury
self consistency fundamental concept in statistics statistical science
tenenbaum
de silva and langford
global geometric framework for nonlinear dimensionality reduction science
tibshirani
regression shrinkage and selection via the lasso journal of the royal statistical society series
tibshirani and hastie
margin trees for high dimensional classification journal of machine learning research
tibshirani and knight
model search and inference by bootstrap bumping journal of computational and graphical statistics
tibshirani and wang
spatial smoothing and hot spot detection for cgh data using the fused lasso biostatistics
tibshirani hastie narasimhan and chu
diagnosis of multiple cancer types by shrunken centroids of gene expression proceedings of the national academy of sciences
tibshirani hastie narasimhan and chu
class prediction by nearest shrunken centroids with applications to dna microarrays statistical science
tibshirani saunders rosset zhu and knight
sparsity and smoothness via the fused lasso journal of the royal statistical society series
tibshirani walther and hastie
estimating the number of clusters in dataset via the gap statistic journal of the royal statistical society series
tropp
greed is good algorithmic results for sparse approximation ieee transactions on information theory
references tropp
just relax convex programming methods for identifying sparse signals in noise ieee transactions on information theory
valiant
theory of the learnable communications of the acm van der merwe and zidek
multivariate regression analysis and canonical variates the canadian journal of statistics
vapnik
the nature of statistical learning theory springer new york
vapnik
statistical learning theory wiley new york
vidakovic
statistical modeling by wavelets wiley new york von luxburg
tutorial on spectral clustering statistics and computing
wahba
spline bases regularization and generalized crossvalidation for solving approximation problems with large quantities of noisy data proceedings of the international conference on approximation theory in honour of george lorenz academic press austin texas pp
wahba
spline models for observational data siam philadelphia
wahba lin and zhang
gacv for support vector machines in
smola
bartlett
scho lkopf and
schuurmans eds advances in large margin classifiers mit press cambridge ma pp
wainwright
sharp thresholds for noisy and high dimensional recovery of sparsity using constrained quadratic programming technical report department of statistics university of california berkeley
wainwright
ravikumar and lafferty
highdimensional graphical model selection using regularized logistic regression in
scho lkopf
platt and
hoffman eds advances in neural information processing systems mit press cambridge ma pp
wasserman
all of statistics concise course in statistical inference springer new york
weisberg
applied linear regression wiley new york
references werbos
beyond regression phd thesis harvard university
weston and watkins
multiclass support vector machines in
verleysen ed proceedings of esann
facto press brussels
whittaker
graphical models in applied multivariate statistics wiley chichester
wickerhauser
adapted wavelet analysis from theory to software
peters ltd natick ma
widrow and hoff
adaptive switching circuits ire wescon convention record vol
pp reprinted in andersen and rosenfeld
wold
soft modelling by latent variables the nonlinear iterative partial least squares nipals approach perspectives in probability and statistics in honor of
bartlett pp
wolpert
stacked generalization neural networks
wu and lange
the mm alternative to em unpublished
wu and lange
coordinate descent procedures for lasso penalized regression annals of applied statistics
yee and wild
vector generalized additive models journal of the royal statistical society series
yuan and lin
model selection and estimation in regression with grouped variables journal of the royal statistical society series
zhang
model selection via multifold cross validation annals of statistics
zhang and yu
boosting with early stopping convergence and consistency annals of statistics
zhao and yu
on model selection consistency of lasso journal of machine learning research
zhao rocha and yu
the composite absolute penalties for grouped and hierarchichal variable selection annals of statistics to appear
zhu and hastie
classification of gene microarrays by penalized logistic regression biostatistics
zhu zou rosset and hastie
multiclass adaboost unpublished
references zou
the adaptive lasso and its oracle properties journal of the american statistical association
zou and hastie
regularization and variable selection via the elastic net journal of the royal statistical society series
zou hastie and tibshirani
sparse principal component analysis journal of computational and graphical statistics
zou hastie and tibshirani
on the degrees of freedom of the lasso annals of statistics
this is page printer opaque this author index abu mostafa
baskett
ackley
baxter
adam
becker
agrawal
bell
agresti
bellman
ahn
benade
akaike
bengio
allen
benjamini
ambroise
bentley
amit
best
anderson
bibby
anderson
angelo
bickel
ardekani
bickel
bishop
bach
bishop
baezner
bloomfield
bair
boser
bakin
botha
bakiri
bottou
banerjee
boyd
barnhill
breiman
barron
barry
bartlett
author index bremaud
dale
brin
dasarathy
brooks
d'aspremont
brown
daubechies
bruce
davis
bu hlmann de boor
buja
de mol de silva
bunea
defrise
burges
dempster
butte
denham
denker
caligiuri
devijver
callow
dietterich
campo
candes
donath
carlin
donoho
cazares
chambers
downing
chan
drton
chaudhuri du plessis
chen
duan
chen
dubes
cherkassky
duchamp
chu
duda
chui
dudoit
clark
duin
clark
clements
edwards
cleveland
efron
clifford
cohen
coifman
elad
coller
elith
comon
eskin
connors
evgeniou
cook
cook
fan
copas
fan
cover
fazekas
cox
feng
cressie
ferreira
csiszar
ferro
cun
fiedler
cutler
fienberg
author index finkel
girosi
fisher
golub
fisher
golub
fisher
goodall
fisher
gordon
fishman
gray
fix
green
flury
greenacre
forgy
greenshtein
francis
guo
frank
guyon
frean
freiha
haffner
freund
hall
fridlyand
hammersley
friedman
hand
hanley
hansen
hansen
hart
hartigan
hastie
friedman
fu
fukunaga
furnival
fusaro
gaasenbeek
ganapathi
gao
gascoyne
gelfand
gelman
hatef
geman
hathaway
geman
heath
genkin
hebb
genovese
henderson
gerald
herman
gersho
hertz
ghaoui
hinkley
gijbels
hinton
gilks
gill
hitt
author index ho
kaufman
hochberg
kearns
hodges
kennard
hoefling
kent
hoerl
hoff
kiiveri
hoffman
kim
hofmann
kishon
holland
kittler
hong
kleinberg
hothorn
knight
howard
koh
huard
kohane
hubbard
kohavi
huber
kohn
hunter
kohonen
hyva rinen
koller
ihaka
kooperberg
inskip
korn
inzitari
kotze
izenman
kressel
krogh
jackel
jacobs
ladd
jain
lafferty
james
lafferty
jancey
lagus
jensen
laird
jiang
lambert
jirous ek
lander
johnson
lange
johnstone
langford
larsen
joliffe
latulippe
jones
lauffenburger
jones
lauritzen
jooste
lawson
jordaan
le cun
jordan
leathwick
leblanc
kabalin
lecun
kalbfleisch
lee
karhunen
lee
kaski
lee
author index lee
mockett
leslie
morgan
levina
motwani
levine
mukherjee
lewis
mulier
li
muller hermelink
li
mu ller
lin
munro
lin
murray
liotta
myles
little
littman
nadler
liu
narasimhan
lloyd
neal
loader
loda
nelder
lugosi
noble
loh
nolan
loh
nowlan
oja
ma
olesen
macnaughton smith
olshen
mackay
macqueen
onton
madigan
osborne
makeig
osindero
mannila
mardia
paatero
pace
marron
page
mason
palmer
massart
pantoni
matas
park
mccullagh
parker
mcculloch
paul
mcculloch
pearl
mclachlan
pe'er
mcneal
perez
mcneil
peterson mcshane
petricoin
mease
pitts
meinshausen
plastria
meir
platt
mesirov
poggio
mills
author index pontil
ryberg
popescu
prentice
saarela
presnell
sachs
pr euc il
saloja rvi
saul
qu
saunders
quinlan
schapire
radmacher
schellhammer
raftery
schnitzler
ramaswamy
scho lkopf
ramsay
schroeder
rao
schwarz
ra tsch
scott
ravikumar
seber
redwine
segal
reich
sejnowski
richardson
semmes
richardson
seung
ridgeway
shafer
rieger
shao
rifkin
shenoy
ripley
short
shustek
shyu
siegmund
rissanen
silverman
ritov
robbins
silvey
rocha
simard
roosen
simon
rosenblatt
simone
rosenwald
singer
rosset
sjo strand
slate
slonim
smeland
rostrup
smith
rousseauw
smola
rousseeuw
sonquist
rowe
spector
roweis
speed
rubin
spiegelhalter
rumelhart
spiegelman
author index spielman
tukey
srikant
turlach
stamey
turnbull
staudt
tusher
steinberg
tusna dy
stern
stodden
uddin
stone
valiant
stone van der merwe
storey
van loan
stork
vandenberghe
studholme
vanichsetakul
stuetzle
vapnik
surowiecki
swayne
vayatis
vazirani
tamayo
verkamo
tang
vidakovic
tanner von luxburg
tao
tarpey
wahba
taylor
taylor
wainwright
teh
wainwright
tenenbaum
waldemar
teng
walther
thomas
wang
tibshirani
ward
warmuth
wasserman
watkins
wegkamp
weisberg
werbos
wermuth
weston
whittaker
toivonen
wickerhauser
traskin
widrow
trendafilov
wild
tropp
williams
truong
williams
tsybakov
wilson
author index winograd
wold
wolpert
wong
wong
wright
wright
wu
wyner
yang
yang
yasui
yeang
yee
yekutieli
yu
yuan
zhang
zhang
zhang
zhang
zhao
zhao
zhu
zidek
zou
this is page printer opaque this index regularization see lasso demographics document activation function flow cytometry adaboost galaxy adaptive lasso heart attack adaptive methods lymphoma adaptive nearest neighbor meth marketing ods microarray adaptive wavelet filtering nested spheres additive model new zealand fish adjusted response nuclear magnetic resonance affine set affine invariant average ozone aic see akaike information cri prostate cancer terion protein mass spectrometry akaike information criterion aic satellite image skin of the orange analysis of deviance spam applications abstracts vowel aorta waveform bone zip code california housing archetypal analysis association rules countries
index automatic relevance determination implementations margin maximization automatic selection of smoothing numerical optimization parameters partial dependence plots regularization path spline shrinkage back propagation stochastic gradient boosting backfitting tree size backward variable importance selection bootstrap stepwise selection backward pass relationship to bayesian method bagging basis expansions and regulariza relationship to maximum liketion lihood method basis functions bottom up clustering bump hunting see patient rule batch learning induction method baum welch algorithm bumping bayes classifier factor canonical variates methods cart see classification and rerate gression trees bayesian categorical predictors bayesian information criterion bic censored data classical multidimensional scaling benjamini hochberg method best subset selection classification between class covariance matrix classification and regression trees bias cart bias variance decomposition clique clustering bias variance tradeoff means bic see bayesian information cri agglomerative terion hierarchical boltzmann machines codebook bonferroni method combinatorial algorithms boosting combining models as lasso regression committee exponential loss and adaboost comparison of learning methods gradient boosting complete data
index complexity parameter discriminant computational shortcuts adaptive nearest neighbor clasquadratic penalty sifier condensing procedure analysis conditional likelihood coordinates confusion matrix functions conjugate gradients dissimilarity measure consensus dummy variables convolutional networks coordinate descent early stopping cosso effective degrees of freedom cost complexity pruning covariance graph cp statistic effective number of parameters cross entropy cross validation cubic smoothing spline eigenvalues of smoother matrix cubic spline curse of dimensionality elastic net em algorithm dantzig selector as maximization maximization data augmentation procedure daubechies symmlet wavelets for two component gaussian mixture de correlation encoder decision boundary ensemble decision trees decoder see encoder ensemble learning decomposable models entropy degrees of freedom equivalent kernel in an additive model error rate in ridge regression error correcting codes of tree estimates of in sample prediction of smoother matrices error expectation maximization algorithm delta rule see em algorithm demmler reinsch basis for splines extra sample error density estimation false discovery rate deviance diagonal linear discriminant anal feature ysis extraction dimension reduction selection for nearest neighbors feed forward neural networks discrete variables
index fisher's linear discriminant global markov property gradient boosting flexible discriminant analysis gradient descent graph laplacian forward graphical lasso selection grouped lasso stagewise stagewise additive modeling haar basis function hammersley clifford theorem stepwise hard thresholding forward pass algorithm hat matrix fourier transform helix frequentist methods hessian matrix function approximation hidden nodes fused lasso hidden units hierarchical clustering gap statistic hierarchical mixtures of experts gating networks gauss markov theorem high dimensional problems gauss newton method hints gaussian normal distribution hyperplane see separating hygaussian graphical model perplane gaussian mixtures gaussian radial basis functions ica see independent components analysis gbm see gradient boosting importance sampling gbm package see gradient boost in sample prediction error ing incomplete data gcv see generalized cross validation independent components analysis gem generalized em generalization independent variables error indicator response matrix performance inference generalized additive model information fisher generalized association rules observed information theory generalized cross validation inner product generalized linear discriminant anal inputs ysis instability of trees generalized linear models intercept gibbs sampler invariance manifold for mixtures invariant metric gini index inverse wavelet transform
index irls see iteratively reweighted fused least squares latent irreducible error factor ising model variable isomap learning isometric feature mapping learning rate iterative proportional scaling learning vector quantization iteratively reweighted least squares least angle regression irls least squares jensen's inequality leave one out cross validation join tree lenet junction tree likelihood function linear basis expansion means clustering linear combination splits medoid clustering linear discriminant function nearest neighbor classifiers karhunen loeve transformation prin linear methods cipal components for classification for regression karush kuhn tucker conditions linear models and least squares kernel linear regression of an indicator classification matrix density classification linear separability density estimation linear smoother function link function logistic regression lle see local linear embedding principal component local false discovery rate string local likelihood trick local linear embedding kernel methods local methods in high dimensions knot local minima kriging local polynomial regression kruskal shephard scaling local regression kullback leibler distance localization in time frequency loess local regression lagrange multipliers log linear model landmark log odds ratio logit laplacian logistic sigmoid function laplacian distribution logistic regression lar see least angle regression logit log odds ratio lasso loss function loss matrix
index lossless compression mixture of experts lossy compression mixtures and the em algorithm lvq see learning vector quan tization mm algorithm mode seekers mahalanobis distance model averaging and stacking majority vote model combination majorization model complexity majorize minimize algorithm model selection map maximum aposteriori es modified regression timate monte carlo method margin mother wavelet market basket analysis multidimensional scaling markov chain monte carlo mcmc multidimensional splines methods multiedit algorithm markov graph multilayer perceptron markov networks multinomial distribution mars see multivariate adaptive multiple additive regression trees regression splines mart mart see multiple additive re multiple hypothesis testing gression trees maximum likelihood estimation multiple minima multiple outcome shrinkage and mcmc see markov chain monte selection carlo methods multiple outputs mdl see minimum description multiple regression from simple unilength variate regression mean field approximation multiresolution analysis mean squared error multivariate adaptive regression memory based method splines mars metropolis hastings algorithm multivariate nonparametric regresminimum description length mdl sion minorization nadaraya watson estimate minorize maximize algorithm naive bayes classifier misclassification error natural cubic splines missing data nearest centroids missing predictor values nearest neighbor methods mixing proportions mixture discriminant analysis nearest shrunken centroids mixture modeling network diagram neural networks
index newton's method newton raphson perceptron procedure piecewise polynomials and splines non negative matrix factorization posterior nonparametric logistic regression distribution probability normal gaussian distribution power method pre conditioning normal equations prediction accuracy numerical optimization prediction error predictive distribution object dissimilarity prim see patient rule induction online algorithm method optimal scoring principal components optimal separating hyperplane regression optimism of the training error rate sparse supervised ordered categorical ordinal pre principal curves and surfaces dictor ordered features principal points orthogonal predictors prior distribution overfitting procrustes average pagerank distance pairwise distance projection pursuit pairwise markov property regression parametric bootstrap prototype classifier partial dependence plots prototype methods partial least squares proximity matrices partition function pruning parzen window pasting qr decomposition path algorithm quadratic approximations and inpatient rule induction method prim ference quadratic discriminant function peeling penalization see regularization radial basis function rbf netpenalized discriminant analysis work radial basis functions penalized polynomial regression radial kernel penalized regression random forest penalty matrix algorithm
index bias separating hyperplanes comparison to boosting example separator out of bag oob shape average overfit shrinkage methods proximity plot sigmoid variable importance significance analysis of microarvariance rays rao score test similarity measure see dissimirayleigh quotient larity measure receiver operating characteristic single index model roc curve singular value decomposition reduced rank linear discriminant analysis singular values regression singular vectors regression spline sliced inverse regression regularization smoother regularized discriminant analysis matrix smoothing parameter relevance network representer of evaluation smoothing spline reproducing kernel hilbert space soft clustering soft thresholding reproducing property softmax function responsibilities som see self organizing map ridge regression sparse risk factor additive model robust fitting graph rosenblatt's perceptron learning specificity of test algorithm spectral clustering rug plot spline rulefit additive cubic sam see significance anal cubic smoothing ysis of microarrays interaction sammon mapping regression scad smoothing scaling of the inputs thin plate schwarz's criterion squared error loss score equations srm see structural risk minimizaself consistency property tion self organizing map som stacking stacked generalization sensitivity of test starting values separating hyperplane statistical decision theory
index statistical model unsupervised learning as supersteepest descent vised learning stepwise selection stochastic approximation validation set stochastic search bumping vapnik chervonenkis vc dimen sion stress function variable importance plot structural risk minimization srm variable types and terminology variance subset selection between supervised learning within supervised principal components variance reduction varying coefficient models support vector classifier vc dimension see vapnik chervonmulticlass enkis dimension support vector machine vector quantization voronoi regions sure shrinkage method survival analysis wald test survival curve wavelet svd see singular value decom basis functions position smoothing symmlet basis transform weak learner tangent distance weakest link pruning tanh activation function webpages target variables website for book tensor product basis weight decay test error weight elimination test set weights in neural network thin plate spline within class covariance matrix thinning strategy trace of matrix training epoch training error training set tree for regression tree based methods trees for classification trellis display undirected graph universal approximator unsupervised learning
